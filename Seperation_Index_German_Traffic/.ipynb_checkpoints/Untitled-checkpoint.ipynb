{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named keras",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-fd846a12612e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named keras"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, BatchNormalization, Dropout\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras import backend as k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoadSignClassifier:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    @staticmethod\n",
    "    def createCNN(width, height, depth, classes):\n",
    "        model = Sequential()\n",
    "        input_shape = (height, width, depth)\n",
    "        model.add(Conv2D(filters=8, kernel_size=(5, 5), input_shape=input_shape, activation=\"relu\"))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        \n",
    "        model.add(Conv2D(filters=16, kernel_size=(3, 3), activation=\"relu\"))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Conv2D(filters=16, kernel_size=(3, 3), activation=\"relu\"))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    " \n",
    "        model.add(Conv2D(filters=32, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Conv2D(filters=32, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        model.add(Flatten())\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(512, activation=\"relu\"))\n",
    "        model.add(Dense(classes, activation=\"softmax\"))\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def load_data(dataset, csv):\n",
    "    images = []\n",
    "    classes = []\n",
    "    rows = pd.read_csv(dataset + csv)\n",
    "    rows = rows.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    for i, row in rows.iterrows():\n",
    "        img_class = row[\"ClassId\"]\n",
    "        img_path = row[\"Path\"]\n",
    "        image = os.path.join(dataset, img_path)\n",
    "        image = cv2.imread(image)\n",
    "        image_rs = cv2.resize(image, (30, 30), 3)\n",
    "\n",
    "        r, g, b = cv2.split(image_rs)\n",
    "\n",
    "        img_r = cv2.equalizeHist(r)\n",
    "        img_g = cv2.equalizeHist(g)\n",
    "        img_b = cv2.equalizeHist(b)\n",
    "\n",
    "        new_image = cv2.merge((img_r, img_g, img_b))\n",
    "\n",
    "        if i % 500 == 0:\n",
    "            print(f\"loaded: {i}\")\n",
    "        images.append(new_image)\n",
    "        classes.append(img_class)\n",
    "\n",
    "    x = np.array(images)\n",
    "    y = np.array(classes)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = r\"/home/sspc/Desktop/gtsrb-german-traffic-sign\"\n",
    "test_data = r\"/home/sspc/Desktop/gtsrb-german-traffic-sign\"\n",
    "(train_X, train_Y) = load_data(train_data, \"/Train.csv\")\n",
    "(test_X, test_Y) = load_data(test_data, \"/Test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"UPDATE: Normalizing data\")\n",
    "x_train = train_X.astype(\"float64\") / 255.0\n",
    "x_test = test_X.astype(\"float64\") / 255.0\n",
    "print(\"UPDATE: One-Hot Encoding data\")\n",
    "num_labels = len(np.unique(train_Y))\n",
    "y_train = to_categorical(train_Y)\n",
    "y_test = to_categorical(test_Y)\n",
    "\n",
    "y_test_one_hot = np.argmax(y_test,1)\n",
    "y_train_one_hot = np.argmax(y_train,1)\n",
    "\n",
    "class_totals = y_train.sum(axis=0)\n",
    "class_weight = class_totals.max() / class_totals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "epochs = 100\n",
    "batch_size = 64\n",
    "optimizer = Adam(lr=learning_rate, decay=learning_rate / epochs)\n",
    "model = RoadSignClassifier.createCNN(width=30, height=30, depth=3, classes=43)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "model_fit = model.fit(\n",
    "    x=x_train,\n",
    "    y=y_train,\n",
    "    batch_size=batch_size, \n",
    "    epochs=epochs,\n",
    "    validation_split=0.2,\n",
    "    class_weight=class_weight,\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def submodel(self,number_layer,flatten=True):\n",
    "    if flatten:\n",
    "        out = Flatten()(self.model.layers[number_layer].output)\n",
    "    else:\n",
    "        out = self.model.layers[number_layer].output\n",
    "    print('name: ',self.model.layers[number_layer].name)\n",
    "    new_model = keras.Model(inputs=[self.model.input],\n",
    "                      output=out)\n",
    "    # out = keras.layers.MaxPool2D((3, 3))(base_model.output)\n",
    "    # out = Dense(fc1_size, activation='sigmoid')(out)\n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% \n"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_SI(featuremap, label, mode='dontCare'):\n",
    "    tf.reset_default_graph()\n",
    "    # featuremap = tf.convert_to_tensor(featuremap)\n",
    "    def cond(count_true, i ,size_loop):\n",
    "        return tf.less(i, size_loop)\n",
    "\n",
    "    def body(count_true, i, size_loop):\n",
    "        norm = tf.subtract(square, 2 * tf.tensordot(array, array[i, :], axes=1))\n",
    "\n",
    "        delta = tf.get_variable(\"delta\", [number], dtype=tf.float32, initializer=tf.constant_initializer(0))\n",
    "        delta = tf.scatter_update(delta, i - 1, 0)\n",
    "        delta = tf.scatter_update(delta, i, np.inf)\n",
    "        norm = tf.math.add(norm, delta)\n",
    "\n",
    "        min_index_norm = tf.argmin(norm)\n",
    "        equal = tf.equal(label[min_index_norm], label[i])\n",
    "        count_true = tf.cond(equal, lambda: tf.add(count_true, 1), lambda: count_true)\n",
    "\n",
    "        return count_true,tf.add(i, 1), size_loop\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        [number, size] = featuremap.shape\n",
    "        array_plhdr = tf.placeholder(dtype=tf.float32, shape=[number, size])\n",
    "        array = tf.get_variable('array', [number, size])\n",
    "        label = tf.convert_to_tensor(label)\n",
    "\n",
    "        square = tf.math.reduce_sum(tf.math.square(array), axis=1)\n",
    "\n",
    "        size_loop = tf.constant(number)\n",
    "        i = tf.constant(0)\n",
    "        count_true = tf.constant(0)\n",
    "        count_true, i, _= tf.while_loop(cond, body, [count_true,i, size_loop])  # ,parallel_iterations=100\n",
    "\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        sess.run(array.assign(array_plhdr), {array_plhdr: featuremap})\n",
    "        count,_ = sess.run([count_true,i])\n",
    "\n",
    "        print(count)\n",
    "        return count,number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"predict(train)\")\n",
    "predicted_x = model.predict(x_train, normalize=False)\n",
    "residuals = np.argmax(predicted_x, 1) != np.argmax(y_train, 1)\n",
    "loss = sum(residuals)/len(residuals)\n",
    "print(\"loss (train): \",loss)\n",
    "#\n",
    "equal = np.argmax(predicted_x, 1) == np.argmax(y_train, 1)\n",
    "acc = sum(equal) / len(equal)\n",
    "print('accuracy (train): ', acc)\n",
    "\n",
    "print(\"predict(test)\")\n",
    "predicted_x = model.predict(x_test, normalize=False)\n",
    "residuals = np.argmax(predicted_x, 1) != np.argmax(y_test, 1)\n",
    "loss = sum(residuals)/len(residuals)\n",
    "print(\"loss (test): \",loss)\n",
    "#\n",
    "equal = np.argmax(predicted_x, 1) == np.argmax(y_test, 1)\n",
    "acc = sum(equal) / len(equal)\n",
    "print('accuracy (test): ', acc)\n",
    "\n",
    "output_file_train = open('train.txt', 'a+', 1)\n",
    "output_file_test = open('test.txt', 'a+', 1)\n",
    "model.summary()\n",
    "for i in range(40, 100):\n",
    "    # tf.reset_default_graph()\n",
    "    print(\"_____________________________________________________\")\n",
    "    model = RoadSignClassifier.createCNN(width=30, height=30, depth=3, classes=43)\n",
    "    if i >= 53:\n",
    "        sub_model = model.submodel(i, False)\n",
    "    else:\n",
    "        sub_model = model.submodel(i)\n",
    "    #\n",
    "    x_train_out = sub_model.predict(x_train)\n",
    "    x_test_out = sub_model.predict(x_test)\n",
    "    print('predict: ', i, x_train_out.shape, x_test_out.shape)\n",
    "\n",
    "    print('calculate SI test')\n",
    "    # prev = time.time()\n",
    "    result2, number2 = calculate_SI(x_test_out, y_test_one_hot)\n",
    "    output_file_test.write(\"%i %f %i %f\\n\" % (i, result2, number2, float(result2 / number2)))\n",
    "    # new = time.time()\n",
    "    print(i, result2, number2, float(result2 / number2))\n",
    "    # prev = new\n",
    "    # K.clear_session() #after call calculate_SI must clear session\n",
    "    prev = 0\n",
    "    print('calculate SI train')\n",
    "    result1, number1 = calculate_SI(x_train_out, y_train_one_hot)\n",
    "    output_file_train.write(\"%i %f %i %f\\n\" % (i, result1, number1, float(result1 / number1)))\n",
    "    print(i, result1, number1, float(result1 / number1), time.time() - prev)\n",
    "    x_train_out = None\n",
    "    x_test_out = None\n",
    "    sub_model = None\n",
    "    k.clear_session()  # after call calculate_SI must clear session\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
