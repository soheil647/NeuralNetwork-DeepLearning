{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "cgan.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "VldEtyAfbeoy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxEJPxzpbeo3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.datasets import cifar10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmEdyB0MgIdk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "c2d2fe73-7ce0-4131-bb04-d42c5f07f1d1"
      },
      "source": [
        "!nvcc --version\n",
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2019 NVIDIA Corporation\n",
            "Built on Sun_Jul_28_19:07:16_PDT_2019\n",
            "Cuda compilation tools, release 10.1, V10.1.243\n",
            "Tue Aug  4 19:44:51 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 450.57       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   60C    P8    10W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y01b4W0xGiZW",
        "colab_type": "text"
      },
      "source": [
        "## Load Cifar10 Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHpKmXlDbeo8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "623179f9-4ee9-438d-a493-73c4f722bb78"
      },
      "source": [
        "(x1, y1), (x2, y2) = cifar10.load_data()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 4s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4upHsJ0GmYA",
        "colab_type": "text"
      },
      "source": [
        "## Combine test and train data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0VC4ygcbepB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = np.concatenate((x1, x2), axis=0)\n",
        "y = np.concatenate((y1, y2), axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fnwirR8bepF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "del x1, y1, x2, y2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BG2duhtrbepJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import keras\n",
        "from keras.preprocessing import image\n",
        "from keras.models import Model\n",
        "from keras.layers import *\n",
        "from keras.optimizers import Adam, RMSprop\n",
        "from keras.layers.advanced_activations import LeakyReLU"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2bs1fLAGp9M",
        "colab_type": "text"
      },
      "source": [
        "Batchsize of 50 10 class, noise dim is 2048"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7UX3tzTbepN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img_size = 32\n",
        "noise_size = 2048\n",
        "batch_size = 50\n",
        "classes = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKH7hlLeGvEs",
        "colab_type": "text"
      },
      "source": [
        "## Define Generator model\n",
        "in generator we have we have noise as input layer and embed layer and classes of noise size while flatening them</br>\n",
        "then we have a 2048 fully connected layer for getting noise (noise dim is 2048) </br>\n",
        "then there are 4 Conv transpose or deConv layers to generate image at output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mimvzva8bepR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generator():\n",
        "    \n",
        "    noise = Input(shape=(noise_size, ))\n",
        "    label = Input(shape=(1, ))\n",
        "    \n",
        "    label_embedding = Flatten()(Embedding(classes, noise_size)(label))\n",
        "    \n",
        "    model_input = multiply([noise, label_embedding])\n",
        "    \n",
        "    x = Dense(2048)(model_input)\n",
        "    \n",
        "    x = Reshape((2, 2, 512))(x)\n",
        "    x = BatchNormalization(momentum=0.9)(x)\n",
        "    x = LeakyReLU(0.1)(x)\n",
        "    \n",
        "    x = Conv2DTranspose(256, (5, 5), padding='same', strides=2)(x)\n",
        "    x = BatchNormalization(momentum=0.9)(x)\n",
        "    x = LeakyReLU(0.1)(x)\n",
        "    \n",
        "    x = Conv2DTranspose(128, (5, 5), padding='same', strides=2)(x)\n",
        "    x = BatchNormalization(momentum=0.9)(x)\n",
        "    x = LeakyReLU(0.1)(x)\n",
        "    \n",
        "    x = Conv2DTranspose(64, (5, 5), padding='same', strides=2)(x)\n",
        "    x = BatchNormalization(momentum=0.9)(x)\n",
        "    x = LeakyReLU(0.1)(x)\n",
        "    \n",
        "    x = Conv2DTranspose(3, (5, 5), padding='same', strides=2)(x)\n",
        "    img = Activation('tanh')(x)\n",
        "    \n",
        "    return Model([noise, label], img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1BX3MmvHOaW",
        "colab_type": "text"
      },
      "source": [
        "## Define discriminator Network\n",
        "here we get image that created by generator as input at 3 conv layer and 1 label input while flatening to poduce label output in CGAN </br>\n",
        "then 1 fully connected layer after dropout to produce real or fakeness"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HmVHTxoDbepU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def discriminator():\n",
        "    \n",
        "    img = Input(shape=(img_size, img_size, 3))\n",
        "    \n",
        "    x = GaussianNoise(0.1)(img)\n",
        "    \n",
        "    x = Conv2D(64, (3, 3), padding='same', strides = 2)(x)\n",
        "    x = BatchNormalization(momentum=0.9)(x)\n",
        "    x = LeakyReLU(0.2)(x)\n",
        "        \n",
        "    x = Conv2D(128, (3, 3), padding='same', strides = 2)(x)\n",
        "    x = BatchNormalization(momentum=0.9)(x)\n",
        "    x = LeakyReLU(0.2)(x)\n",
        "        \n",
        "    x = Conv2D(256, (3, 3), padding='same', strides = 2)(x)\n",
        "    x = BatchNormalization(momentum=0.9)(x)\n",
        "    x = LeakyReLU(0.2)(x)\n",
        "        \n",
        "    x = Conv2D(512, (3, 3), padding='same', strides = 2)(x)\n",
        "    x = BatchNormalization(momentum=0.9)(x)\n",
        "    x = LeakyReLU(0.2)(x)\n",
        "    \n",
        "    label = Input(shape=(1, ))\n",
        "    label_embedding = Flatten()(Embedding(classes, noise_size)(label))\n",
        "    \n",
        "    flat_img = Flatten()(x)\n",
        "    \n",
        "    model_input = multiply([flat_img, label_embedding])\n",
        "\n",
        "    nn = Dropout(0.3)(model_input)\n",
        "    \n",
        "    validity = Dense(1, activation='sigmoid')(nn)\n",
        "    \n",
        "    return Model([img, label], validity)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FeF8RYZHlUa",
        "colab_type": "text"
      },
      "source": [
        "## DEfine CGAN model\n",
        "we combine generator and discriminator model with ADAM optimizer  to produce CGAN network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOe148YTbepY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "d_model = discriminator()\n",
        "d_model.compile(loss=['binary_crossentropy'], optimizer=Adam(lr=0.0002, beta_1=0.5))\n",
        "d_model.trainable = False\n",
        "g_model = generator()\n",
        "\n",
        "noise = keras.Input(shape=(noise_size, ))\n",
        "label = keras.Input(shape=(1, ))\n",
        "img = g_model([noise, label])\n",
        "\n",
        "valid = d_model([img, label])\n",
        "\n",
        "combined = Model([noise, label], valid)\n",
        "combined.compile(loss=['binary_crossentropy'], optimizer=Adam(lr=0.001, beta_1=0.5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wu55RuoaIJG1",
        "colab_type": "text"
      },
      "source": [
        "## Train Network\n",
        "here we train network, for eacch batch of input and for each image in that we first produce a real and fake image and giving it to discriminator to train then by adding noise from batchsize we generate image and train generator on thatand for every 50 we print losses"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WqxUld9Kbepb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(epochs):\n",
        "    d_loss_history = []\n",
        "    g_loss_history  = [] \n",
        "    for epoch in range(epochs):\n",
        "        \n",
        "        random = np.random.randint(0, 11)\n",
        "        \n",
        "        for index in range(int(x.shape[0]/batch_size)):\n",
        "                     \n",
        "            valid = np.ones((batch_size, 1)) - (np.random.random()*0.1)\n",
        "            fake = np.zeros((batch_size, 1)) + (np.random.random()*0.1)\n",
        "            \n",
        "            x_train = x[index*batch_size : (index+1)*batch_size]\n",
        "            y_train = y[index*batch_size : (index+1)*batch_size]\n",
        "            x_train = (x_train - 127.5)/127.5\n",
        "            \n",
        "            if index % 100 == random:\n",
        "                valid = np.zeros((batch_size, 1)) + (np.random.random()*0.1)\n",
        "                fake = np.ones((batch_size, 1)) - (np.random.random()*0.1)\n",
        "            \n",
        "            noise = np.random.randn(batch_size, noise_size)\n",
        "            gen_img = g_model.predict([noise, y_train])\n",
        "                        \n",
        "            d_loss_real = d_model.train_on_batch([x_train, y_train], valid)\n",
        "            d_loss_fake = d_model.train_on_batch([gen_img, y_train], fake)\n",
        "            d_loss = 0.5*(np.add(d_loss_real, d_loss_fake))\n",
        "\n",
        "            sample_label = np.random.randint(0, 10, batch_size).reshape(-1, 1)\n",
        "\n",
        "            valid = np.ones((batch_size, 1))\n",
        "            \n",
        "            g_loss = combined.train_on_batch([noise, sample_label], valid)\n",
        "\n",
        "            if index % (batch_size) == 0:\n",
        "                print(index)\n",
        "                print(\"%d [D loss: %f] [G loss: %f]\" % (epoch, d_loss, g_loss))\n",
        "                d_loss_history.append(d_loss)\n",
        "                g_loss_history.append(g_loss)\n",
        "                sample_images(epoch)\n",
        "        \n",
        "        name = './weights/combined_' + str(epoch) + '.h5'\n",
        "        combined.save_weights(name)\n",
        "        \n",
        "        time.sleep(30)\n",
        "    return g_loss_history, d_loss_history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8d150rLaigCd",
        "colab_type": "text"
      },
      "source": [
        "## Sample images to Draw digures\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQijcU1mbepf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample_images(epoch):\n",
        "    r = 2\n",
        "    c = 5\n",
        "    noise = np.random.randn(10, noise_size)\n",
        "    sample_label = np.arange(0, 10).reshape(-1, 1)\n",
        "            \n",
        "    gen_img = g_model.predict([noise, sample_label])\n",
        "        \n",
        "    fig, axs = plt.subplots(r, c)\n",
        "    cnt = 0\n",
        "    for i in range(r):\n",
        "        for j in range(c):\n",
        "            img = image.array_to_img(gen_img[cnt])\n",
        "            axs[i,j].imshow(img)\n",
        "            axs[i,j].set_title(\"Class: %d\" % sample_label[cnt])\n",
        "            axs[i,j].axis('off')\n",
        "            cnt += 1\n",
        "    fig.savefig(\"images/%d.png\" % epoch)\n",
        "    plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpQjwah-bepj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b23b768a-03b3-4d9c-afa1-9f56782b3a4f"
      },
      "source": [
        "g_loss_history, d_loss_history = train(20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "0 [D loss: 0.697392] [G loss: 0.692948]\n",
            "50\n",
            "0 [D loss: 0.578080] [G loss: 0.686301]\n",
            "100\n",
            "0 [D loss: 0.226013] [G loss: 0.190735]\n",
            "150\n",
            "0 [D loss: 0.178096] [G loss: 0.077708]\n",
            "200\n",
            "0 [D loss: 0.210440] [G loss: 0.046639]\n",
            "250\n",
            "0 [D loss: 0.140536] [G loss: 0.024208]\n",
            "300\n",
            "0 [D loss: 0.282827] [G loss: 0.018577]\n",
            "350\n",
            "0 [D loss: 0.209517] [G loss: 0.036820]\n",
            "400\n",
            "0 [D loss: 0.180378] [G loss: 0.022412]\n",
            "450\n",
            "0 [D loss: 0.288181] [G loss: 0.033512]\n",
            "500\n",
            "0 [D loss: 0.138622] [G loss: 0.021673]\n",
            "550\n",
            "0 [D loss: 0.104300] [G loss: 0.033136]\n",
            "600\n",
            "0 [D loss: 0.100721] [G loss: 0.023546]\n",
            "650\n",
            "0 [D loss: 0.085697] [G loss: 0.026715]\n",
            "700\n",
            "0 [D loss: 0.253280] [G loss: 0.024928]\n",
            "750\n",
            "0 [D loss: 0.198241] [G loss: 0.057576]\n",
            "800\n",
            "0 [D loss: 0.304871] [G loss: 0.028156]\n",
            "850\n",
            "0 [D loss: 0.175288] [G loss: 0.043680]\n",
            "900\n",
            "0 [D loss: 0.137001] [G loss: 0.112556]\n",
            "950\n",
            "0 [D loss: 0.287453] [G loss: 0.064270]\n",
            "1000\n",
            "0 [D loss: 0.158131] [G loss: 0.106609]\n",
            "1050\n",
            "0 [D loss: 0.166633] [G loss: 0.088714]\n",
            "1100\n",
            "0 [D loss: 0.256249] [G loss: 0.076070]\n",
            "1150\n",
            "0 [D loss: 0.079724] [G loss: 0.056943]\n",
            "0\n",
            "1 [D loss: 0.216322] [G loss: 0.026236]\n",
            "50\n",
            "1 [D loss: 0.274854] [G loss: 0.107654]\n",
            "100\n",
            "1 [D loss: 0.068331] [G loss: 0.050730]\n",
            "150\n",
            "1 [D loss: 0.292786] [G loss: 0.092296]\n",
            "200\n",
            "1 [D loss: 0.198444] [G loss: 0.063481]\n",
            "250\n",
            "1 [D loss: 0.246426] [G loss: 0.112280]\n",
            "300\n",
            "1 [D loss: 0.217436] [G loss: 0.080093]\n",
            "350\n",
            "1 [D loss: 0.084080] [G loss: 0.096839]\n",
            "400\n",
            "1 [D loss: 0.271742] [G loss: 0.074771]\n",
            "450\n",
            "1 [D loss: 0.166813] [G loss: 0.089293]\n",
            "500\n",
            "1 [D loss: 0.101659] [G loss: 0.063248]\n",
            "550\n",
            "1 [D loss: 0.295117] [G loss: 0.082552]\n",
            "600\n",
            "1 [D loss: 0.207105] [G loss: 0.067120]\n",
            "650\n",
            "1 [D loss: 0.161811] [G loss: 0.101867]\n",
            "700\n",
            "1 [D loss: 0.233736] [G loss: 0.060495]\n",
            "750\n",
            "1 [D loss: 0.168074] [G loss: 0.092970]\n",
            "800\n",
            "1 [D loss: 0.212939] [G loss: 0.066071]\n",
            "850\n",
            "1 [D loss: 0.215496] [G loss: 0.104390]\n",
            "900\n",
            "1 [D loss: 0.227858] [G loss: 0.084589]\n",
            "950\n",
            "1 [D loss: 0.264017] [G loss: 0.088319]\n",
            "1000\n",
            "1 [D loss: 0.196176] [G loss: 0.064040]\n",
            "1050\n",
            "1 [D loss: 0.129192] [G loss: 0.101000]\n",
            "1100\n",
            "1 [D loss: 0.102419] [G loss: 0.076111]\n",
            "1150\n",
            "1 [D loss: 0.178312] [G loss: 0.087308]\n",
            "0\n",
            "2 [D loss: 0.099283] [G loss: 0.076820]\n",
            "50\n",
            "2 [D loss: 0.144644] [G loss: 0.099611]\n",
            "100\n",
            "2 [D loss: 0.275136] [G loss: 0.096425]\n",
            "150\n",
            "2 [D loss: 0.128951] [G loss: 0.103521]\n",
            "200\n",
            "2 [D loss: 0.207231] [G loss: 0.079538]\n",
            "250\n",
            "2 [D loss: 0.169906] [G loss: 0.099829]\n",
            "300\n",
            "2 [D loss: 0.311930] [G loss: 0.083220]\n",
            "350\n",
            "2 [D loss: 0.170777] [G loss: 0.123573]\n",
            "400\n",
            "2 [D loss: 0.199633] [G loss: 0.104870]\n",
            "450\n",
            "2 [D loss: 0.268279] [G loss: 0.121743]\n",
            "500\n",
            "2 [D loss: 0.227942] [G loss: 0.098958]\n",
            "550\n",
            "2 [D loss: 0.102098] [G loss: 0.133645]\n",
            "600\n",
            "2 [D loss: 0.213252] [G loss: 0.106693]\n",
            "650\n",
            "2 [D loss: 0.187146] [G loss: 0.118328]\n",
            "700\n",
            "2 [D loss: 0.258035] [G loss: 0.100717]\n",
            "750\n",
            "2 [D loss: 0.164598] [G loss: 0.139952]\n",
            "800\n",
            "2 [D loss: 0.105239] [G loss: 0.106464]\n",
            "850\n",
            "2 [D loss: 0.283669] [G loss: 0.158731]\n",
            "900\n",
            "2 [D loss: 0.091114] [G loss: 0.117764]\n",
            "950\n",
            "2 [D loss: 0.146066] [G loss: 0.134540]\n",
            "1000\n",
            "2 [D loss: 0.239570] [G loss: 0.124403]\n",
            "1050\n",
            "2 [D loss: 0.184283] [G loss: 0.088168]\n",
            "1100\n",
            "2 [D loss: 0.257781] [G loss: 0.061051]\n",
            "1150\n",
            "2 [D loss: 0.133823] [G loss: 0.070755]\n",
            "0\n",
            "3 [D loss: 0.069373] [G loss: 0.060128]\n",
            "50\n",
            "3 [D loss: 0.255051] [G loss: 0.160741]\n",
            "100\n",
            "3 [D loss: 0.231004] [G loss: 0.123960]\n",
            "150\n",
            "3 [D loss: 0.105344] [G loss: 0.101198]\n",
            "200\n",
            "3 [D loss: 0.252808] [G loss: 0.067409]\n",
            "250\n",
            "3 [D loss: 0.168673] [G loss: 0.099283]\n",
            "300\n",
            "3 [D loss: 0.158409] [G loss: 0.100071]\n",
            "350\n",
            "3 [D loss: 0.197008] [G loss: 0.145333]\n",
            "400\n",
            "3 [D loss: 0.239419] [G loss: 0.128028]\n",
            "450\n",
            "3 [D loss: 0.256156] [G loss: 0.092719]\n",
            "500\n",
            "3 [D loss: 0.234937] [G loss: 0.057991]\n",
            "550\n",
            "3 [D loss: 0.126222] [G loss: 0.080261]\n",
            "600\n",
            "3 [D loss: 0.259354] [G loss: 0.061333]\n",
            "650\n",
            "3 [D loss: 0.157479] [G loss: 0.111832]\n",
            "700\n",
            "3 [D loss: 0.121215] [G loss: 0.067342]\n",
            "750\n",
            "3 [D loss: 0.147583] [G loss: 0.108625]\n",
            "800\n",
            "3 [D loss: 0.294676] [G loss: 0.082955]\n",
            "850\n",
            "3 [D loss: 0.178743] [G loss: 0.113158]\n",
            "900\n",
            "3 [D loss: 0.163654] [G loss: 0.070686]\n",
            "950\n",
            "3 [D loss: 0.158219] [G loss: 0.294109]\n",
            "1000\n",
            "3 [D loss: 0.169957] [G loss: 0.209113]\n",
            "1050\n",
            "3 [D loss: 0.175391] [G loss: 0.277362]\n",
            "1100\n",
            "3 [D loss: 0.230106] [G loss: 0.198043]\n",
            "1150\n",
            "3 [D loss: 0.188927] [G loss: 0.207868]\n",
            "0\n",
            "4 [D loss: 0.169440] [G loss: 0.169449]\n",
            "50\n",
            "4 [D loss: 0.215244] [G loss: 0.195897]\n",
            "100\n",
            "4 [D loss: 0.226840] [G loss: 0.173293]\n",
            "150\n",
            "4 [D loss: 0.282534] [G loss: 0.176623]\n",
            "200\n",
            "4 [D loss: 0.146025] [G loss: 0.143797]\n",
            "250\n",
            "4 [D loss: 0.195365] [G loss: 0.174238]\n",
            "300\n",
            "4 [D loss: 0.169612] [G loss: 0.164625]\n",
            "350\n",
            "4 [D loss: 0.271332] [G loss: 0.178431]\n",
            "400\n",
            "4 [D loss: 0.281306] [G loss: 0.140465]\n",
            "450\n",
            "4 [D loss: 0.198937] [G loss: 0.177702]\n",
            "500\n",
            "4 [D loss: 0.126153] [G loss: 0.171246]\n",
            "550\n",
            "4 [D loss: 0.159235] [G loss: 0.202318]\n",
            "600\n",
            "4 [D loss: 0.112143] [G loss: 0.147751]\n",
            "650\n",
            "4 [D loss: 0.141424] [G loss: 0.195726]\n",
            "700\n",
            "4 [D loss: 0.164105] [G loss: 0.218769]\n",
            "750\n",
            "4 [D loss: 0.147935] [G loss: 0.251068]\n",
            "800\n",
            "4 [D loss: 0.219108] [G loss: 0.186221]\n",
            "850\n",
            "4 [D loss: 0.231855] [G loss: 0.207073]\n",
            "900\n",
            "4 [D loss: 0.200471] [G loss: 0.178848]\n",
            "950\n",
            "4 [D loss: 0.223057] [G loss: 0.184155]\n",
            "1000\n",
            "4 [D loss: 0.180103] [G loss: 0.181751]\n",
            "1050\n",
            "4 [D loss: 0.190551] [G loss: 0.192132]\n",
            "1100\n",
            "4 [D loss: 0.141488] [G loss: 0.156514]\n",
            "1150\n",
            "4 [D loss: 0.189999] [G loss: 0.178793]\n",
            "0\n",
            "5 [D loss: 0.159270] [G loss: 0.151479]\n",
            "50\n",
            "5 [D loss: 0.264704] [G loss: 0.195736]\n",
            "100\n",
            "5 [D loss: 0.216251] [G loss: 0.172701]\n",
            "150\n",
            "5 [D loss: 0.181613] [G loss: 0.163898]\n",
            "200\n",
            "5 [D loss: 0.174020] [G loss: 0.141412]\n",
            "250\n",
            "5 [D loss: 0.127598] [G loss: 0.184295]\n",
            "300\n",
            "5 [D loss: 0.182821] [G loss: 0.187792]\n",
            "350\n",
            "5 [D loss: 0.174880] [G loss: 0.187678]\n",
            "400\n",
            "5 [D loss: 0.300668] [G loss: 0.173854]\n",
            "450\n",
            "5 [D loss: 0.266584] [G loss: 0.196002]\n",
            "500\n",
            "5 [D loss: 0.134989] [G loss: 0.172157]\n",
            "550\n",
            "5 [D loss: 0.210565] [G loss: 0.218577]\n",
            "600\n",
            "5 [D loss: 0.101614] [G loss: 0.176896]\n",
            "650\n",
            "5 [D loss: 0.166394] [G loss: 0.185807]\n",
            "700\n",
            "5 [D loss: 0.211123] [G loss: 0.171713]\n",
            "750\n",
            "5 [D loss: 0.315836] [G loss: 0.187137]\n",
            "800\n",
            "5 [D loss: 0.116089] [G loss: 0.191093]\n",
            "850\n",
            "5 [D loss: 0.144468] [G loss: 0.183795]\n",
            "900\n",
            "5 [D loss: 0.182720] [G loss: 0.152510]\n",
            "950\n",
            "5 [D loss: 0.155070] [G loss: 0.168801]\n",
            "1000\n",
            "5 [D loss: 0.190128] [G loss: 0.160002]\n",
            "1050\n",
            "5 [D loss: 0.142665] [G loss: 0.166984]\n",
            "1100\n",
            "5 [D loss: 0.214766] [G loss: 0.141954]\n",
            "1150\n",
            "5 [D loss: 0.146662] [G loss: 0.151952]\n",
            "0\n",
            "6 [D loss: 0.203429] [G loss: 0.131292]\n",
            "50\n",
            "6 [D loss: 0.242350] [G loss: 0.131187]\n",
            "100\n",
            "6 [D loss: 0.103720] [G loss: 0.104391]\n",
            "150\n",
            "6 [D loss: 0.271729] [G loss: 0.173826]\n",
            "200\n",
            "6 [D loss: 0.196011] [G loss: 0.154720]\n",
            "250\n",
            "6 [D loss: 0.255955] [G loss: 0.158701]\n",
            "300\n",
            "6 [D loss: 0.275759] [G loss: 0.143746]\n",
            "350\n",
            "6 [D loss: 0.232125] [G loss: 0.154731]\n",
            "400\n",
            "6 [D loss: 0.282017] [G loss: 0.142683]\n",
            "450\n",
            "6 [D loss: 0.261040] [G loss: 0.164694]\n",
            "500\n",
            "6 [D loss: 0.269564] [G loss: 0.147145]\n",
            "550\n",
            "6 [D loss: 0.232005] [G loss: 0.166600]\n",
            "600\n",
            "6 [D loss: 0.073574] [G loss: 0.136475]\n",
            "650\n",
            "6 [D loss: 0.232518] [G loss: 0.129576]\n",
            "700\n",
            "6 [D loss: 0.149832] [G loss: 0.136854]\n",
            "750\n",
            "6 [D loss: 0.235252] [G loss: 0.155396]\n",
            "800\n",
            "6 [D loss: 0.209011] [G loss: 0.146065]\n",
            "850\n",
            "6 [D loss: 0.227582] [G loss: 0.180554]\n",
            "900\n",
            "6 [D loss: 0.156748] [G loss: 0.151334]\n",
            "950\n",
            "6 [D loss: 0.191482] [G loss: 0.180758]\n",
            "1000\n",
            "6 [D loss: 0.141740] [G loss: 0.160239]\n",
            "1050\n",
            "6 [D loss: 0.162111] [G loss: 0.169672]\n",
            "1100\n",
            "6 [D loss: 0.159649] [G loss: 0.146614]\n",
            "1150\n",
            "6 [D loss: 0.262232] [G loss: 0.156485]\n",
            "0\n",
            "7 [D loss: 0.192731] [G loss: 0.133587]\n",
            "50\n",
            "7 [D loss: 0.145627] [G loss: 0.143130]\n",
            "100\n",
            "7 [D loss: 0.153387] [G loss: 0.132561]\n",
            "150\n",
            "7 [D loss: 0.216975] [G loss: 0.163274]\n",
            "200\n",
            "7 [D loss: 0.178349] [G loss: 0.154836]\n",
            "250\n",
            "7 [D loss: 0.224984] [G loss: 0.172845]\n",
            "300\n",
            "7 [D loss: 0.164656] [G loss: 0.152294]\n",
            "350\n",
            "7 [D loss: 0.126408] [G loss: 0.165788]\n",
            "400\n",
            "7 [D loss: 0.232224] [G loss: 0.159603]\n",
            "450\n",
            "7 [D loss: 0.099237] [G loss: 0.198308]\n",
            "500\n",
            "7 [D loss: 0.130000] [G loss: 0.163817]\n",
            "550\n",
            "7 [D loss: 0.152034] [G loss: 0.152884]\n",
            "600\n",
            "7 [D loss: 0.266292] [G loss: 0.134485]\n",
            "650\n",
            "7 [D loss: 0.199362] [G loss: 0.208005]\n",
            "700\n",
            "7 [D loss: 0.237981] [G loss: 0.153828]\n",
            "750\n",
            "7 [D loss: 0.200993] [G loss: 0.149911]\n",
            "800\n",
            "7 [D loss: 0.154325] [G loss: 0.128018]\n",
            "850\n",
            "7 [D loss: 0.201885] [G loss: 0.166348]\n",
            "900\n",
            "7 [D loss: 0.212530] [G loss: 0.151863]\n",
            "950\n",
            "7 [D loss: 0.152401] [G loss: 0.181690]\n",
            "1000\n",
            "7 [D loss: 0.323539] [G loss: 0.153535]\n",
            "1050\n",
            "7 [D loss: 0.261509] [G loss: 0.201554]\n",
            "1100\n",
            "7 [D loss: 0.234039] [G loss: 0.181586]\n",
            "1150\n",
            "7 [D loss: 0.227738] [G loss: 0.169610]\n",
            "0\n",
            "8 [D loss: 0.291269] [G loss: 0.144369]\n",
            "50\n",
            "8 [D loss: 0.265119] [G loss: 0.204661]\n",
            "100\n",
            "8 [D loss: 0.301530] [G loss: 0.176233]\n",
            "150\n",
            "8 [D loss: 0.138649] [G loss: 0.183907]\n",
            "200\n",
            "8 [D loss: 0.275748] [G loss: 0.152381]\n",
            "250\n",
            "8 [D loss: 0.163708] [G loss: 0.191169]\n",
            "300\n",
            "8 [D loss: 0.249908] [G loss: 0.139337]\n",
            "350\n",
            "8 [D loss: 0.162427] [G loss: 0.147574]\n",
            "400\n",
            "8 [D loss: 0.215738] [G loss: 0.135483]\n",
            "450\n",
            "8 [D loss: 0.257692] [G loss: 0.170546]\n",
            "500\n",
            "8 [D loss: 0.141470] [G loss: 0.137002]\n",
            "550\n",
            "8 [D loss: 0.323019] [G loss: 0.173142]\n",
            "600\n",
            "8 [D loss: 0.204886] [G loss: 0.138288]\n",
            "650\n",
            "8 [D loss: 0.152212] [G loss: 0.169313]\n",
            "700\n",
            "8 [D loss: 0.086780] [G loss: 0.160580]\n",
            "750\n",
            "8 [D loss: 0.253849] [G loss: 0.149326]\n",
            "800\n",
            "8 [D loss: 0.301029] [G loss: 0.147249]\n",
            "850\n",
            "8 [D loss: 0.290272] [G loss: 0.178711]\n",
            "900\n",
            "8 [D loss: 0.221185] [G loss: 0.156662]\n",
            "950\n",
            "8 [D loss: 0.253953] [G loss: 0.180118]\n",
            "1000\n",
            "8 [D loss: 0.185247] [G loss: 0.151712]\n",
            "1050\n",
            "8 [D loss: 0.162587] [G loss: 0.186172]\n",
            "1100\n",
            "8 [D loss: 0.069513] [G loss: 0.170000]\n",
            "1150\n",
            "8 [D loss: 0.218905] [G loss: 0.176794]\n",
            "0\n",
            "9 [D loss: 0.207098] [G loss: 0.159680]\n",
            "50\n",
            "9 [D loss: 0.120906] [G loss: 0.160002]\n",
            "100\n",
            "9 [D loss: 0.197993] [G loss: 0.139952]\n",
            "150\n",
            "9 [D loss: 0.282893] [G loss: 0.181212]\n",
            "200\n",
            "9 [D loss: 0.299401] [G loss: 0.160423]\n",
            "250\n",
            "9 [D loss: 0.274821] [G loss: 0.194955]\n",
            "300\n",
            "9 [D loss: 0.227850] [G loss: 0.192507]\n",
            "350\n",
            "9 [D loss: 0.201909] [G loss: 0.178954]\n",
            "400\n",
            "9 [D loss: 0.211784] [G loss: 0.165678]\n",
            "450\n",
            "9 [D loss: 0.249460] [G loss: 0.181393]\n",
            "500\n",
            "9 [D loss: 0.200924] [G loss: 0.156943]\n",
            "550\n",
            "9 [D loss: 0.225069] [G loss: 0.152089]\n",
            "600\n",
            "9 [D loss: 0.235441] [G loss: 0.136653]\n",
            "650\n",
            "9 [D loss: 0.302944] [G loss: 0.150957]\n",
            "700\n",
            "9 [D loss: 0.192818] [G loss: 0.156898]\n",
            "750\n",
            "9 [D loss: 0.176944] [G loss: 0.191049]\n",
            "800\n",
            "9 [D loss: 0.195345] [G loss: 0.154718]\n",
            "850\n",
            "9 [D loss: 0.264893] [G loss: 0.168975]\n",
            "900\n",
            "9 [D loss: 0.262540] [G loss: 0.177022]\n",
            "950\n",
            "9 [D loss: 0.177055] [G loss: 0.216449]\n",
            "1000\n",
            "9 [D loss: 0.213973] [G loss: 0.186794]\n",
            "1050\n",
            "9 [D loss: 0.153194] [G loss: 0.191534]\n",
            "1100\n",
            "9 [D loss: 0.276076] [G loss: 0.177703]\n",
            "1150\n",
            "9 [D loss: 0.229978] [G loss: 0.192456]\n",
            "0\n",
            "10 [D loss: 0.252880] [G loss: 0.179696]\n",
            "50\n",
            "10 [D loss: 0.170424] [G loss: 0.211158]\n",
            "100\n",
            "10 [D loss: 0.164136] [G loss: 0.184553]\n",
            "150\n",
            "10 [D loss: 0.138064] [G loss: 0.191908]\n",
            "200\n",
            "10 [D loss: 0.170935] [G loss: 0.176811]\n",
            "250\n",
            "10 [D loss: 0.152930] [G loss: 0.197433]\n",
            "300\n",
            "10 [D loss: 0.223398] [G loss: 0.933462]\n",
            "350\n",
            "10 [D loss: 0.228902] [G loss: 0.062432]\n",
            "400\n",
            "10 [D loss: 0.096368] [G loss: 0.208982]\n",
            "450\n",
            "10 [D loss: 0.135800] [G loss: 0.153600]\n",
            "500\n",
            "10 [D loss: 0.146614] [G loss: 0.118310]\n",
            "550\n",
            "10 [D loss: 0.235250] [G loss: 0.217657]\n",
            "600\n",
            "10 [D loss: 0.193076] [G loss: 0.127072]\n",
            "650\n",
            "10 [D loss: 0.123592] [G loss: 0.157786]\n",
            "700\n",
            "10 [D loss: 0.121986] [G loss: 0.117703]\n",
            "750\n",
            "10 [D loss: 0.207061] [G loss: 0.178592]\n",
            "800\n",
            "10 [D loss: 0.224479] [G loss: 0.106804]\n",
            "850\n",
            "10 [D loss: 0.270441] [G loss: 0.135033]\n",
            "900\n",
            "10 [D loss: 0.168965] [G loss: 0.105729]\n",
            "950\n",
            "10 [D loss: 0.173454] [G loss: 0.195261]\n",
            "1000\n",
            "10 [D loss: 0.236934] [G loss: 0.161550]\n",
            "1050\n",
            "10 [D loss: 0.267677] [G loss: 0.154505]\n",
            "1100\n",
            "10 [D loss: 0.190430] [G loss: 0.102007]\n",
            "1150\n",
            "10 [D loss: 0.088787] [G loss: 0.127801]\n",
            "0\n",
            "11 [D loss: 0.164577] [G loss: 0.119770]\n",
            "50\n",
            "11 [D loss: 0.162848] [G loss: 0.156082]\n",
            "100\n",
            "11 [D loss: 0.162014] [G loss: 0.132637]\n",
            "150\n",
            "11 [D loss: 0.281051] [G loss: 0.158935]\n",
            "200\n",
            "11 [D loss: 0.237402] [G loss: 0.154798]\n",
            "250\n",
            "11 [D loss: 0.251962] [G loss: 0.162997]\n",
            "300\n",
            "11 [D loss: 0.196900] [G loss: 0.163764]\n",
            "350\n",
            "11 [D loss: 0.128170] [G loss: 0.192331]\n",
            "400\n",
            "11 [D loss: 0.111102] [G loss: 0.172125]\n",
            "450\n",
            "11 [D loss: 0.168235] [G loss: 0.202080]\n",
            "500\n",
            "11 [D loss: 0.113345] [G loss: 0.179901]\n",
            "550\n",
            "11 [D loss: 0.259413] [G loss: 0.214779]\n",
            "600\n",
            "11 [D loss: 0.078292] [G loss: 0.193282]\n",
            "650\n",
            "11 [D loss: 0.118596] [G loss: 0.220028]\n",
            "700\n",
            "11 [D loss: 0.181137] [G loss: 0.231508]\n",
            "750\n",
            "11 [D loss: 0.165664] [G loss: 0.192184]\n",
            "800\n",
            "11 [D loss: 0.089795] [G loss: 0.159822]\n",
            "850\n",
            "11 [D loss: 0.160849] [G loss: 0.171488]\n",
            "900\n",
            "11 [D loss: 0.227240] [G loss: 0.159412]\n",
            "950\n",
            "11 [D loss: 0.200299] [G loss: 0.167537]\n",
            "1000\n",
            "11 [D loss: 0.286404] [G loss: 0.151711]\n",
            "1050\n",
            "11 [D loss: 0.084918] [G loss: 0.159022]\n",
            "1100\n",
            "11 [D loss: 0.201770] [G loss: 0.147655]\n",
            "1150\n",
            "11 [D loss: 0.292677] [G loss: 0.163287]\n",
            "0\n",
            "12 [D loss: 0.152102] [G loss: 0.150921]\n",
            "50\n",
            "12 [D loss: 0.281839] [G loss: 0.164625]\n",
            "100\n",
            "12 [D loss: 0.211508] [G loss: 0.153902]\n",
            "150\n",
            "12 [D loss: 0.270045] [G loss: 0.164947]\n",
            "200\n",
            "12 [D loss: 0.175894] [G loss: 0.155942]\n",
            "250\n",
            "12 [D loss: 0.220147] [G loss: 0.183836]\n",
            "300\n",
            "12 [D loss: 0.233018] [G loss: 0.165869]\n",
            "350\n",
            "12 [D loss: 0.191441] [G loss: 0.164443]\n",
            "400\n",
            "12 [D loss: 0.123806] [G loss: 0.143846]\n",
            "450\n",
            "12 [D loss: 0.267333] [G loss: 0.180829]\n",
            "500\n",
            "12 [D loss: 0.257029] [G loss: 0.162125]\n",
            "550\n",
            "12 [D loss: 0.237971] [G loss: 0.187126]\n",
            "600\n",
            "12 [D loss: 0.252788] [G loss: 0.159016]\n",
            "650\n",
            "12 [D loss: 0.231750] [G loss: 0.178874]\n",
            "700\n",
            "12 [D loss: 0.179013] [G loss: 0.178755]\n",
            "750\n",
            "12 [D loss: 0.150365] [G loss: 0.198088]\n",
            "800\n",
            "12 [D loss: 0.272349] [G loss: 0.175463]\n",
            "850\n",
            "12 [D loss: 0.212167] [G loss: 0.191705]\n",
            "900\n",
            "12 [D loss: 0.145254] [G loss: 0.172891]\n",
            "950\n",
            "12 [D loss: 0.202336] [G loss: 0.191873]\n",
            "1000\n",
            "12 [D loss: 0.174909] [G loss: 0.180151]\n",
            "1050\n",
            "12 [D loss: 0.127885] [G loss: 0.234621]\n",
            "1100\n",
            "12 [D loss: 0.195823] [G loss: 0.203531]\n",
            "1150\n",
            "12 [D loss: 0.109029] [G loss: 0.213117]\n",
            "0\n",
            "13 [D loss: 0.160278] [G loss: 0.198267]\n",
            "50\n",
            "13 [D loss: 0.217921] [G loss: 0.220605]\n",
            "100\n",
            "13 [D loss: 0.176255] [G loss: 0.213967]\n",
            "150\n",
            "13 [D loss: 0.172665] [G loss: 0.229398]\n",
            "200\n",
            "13 [D loss: 0.205506] [G loss: 0.192429]\n",
            "250\n",
            "13 [D loss: 0.218549] [G loss: 0.242778]\n",
            "300\n",
            "13 [D loss: 0.235897] [G loss: 0.208409]\n",
            "350\n",
            "13 [D loss: 0.170001] [G loss: 0.244137]\n",
            "400\n",
            "13 [D loss: 0.219495] [G loss: 0.235804]\n",
            "450\n",
            "13 [D loss: 0.218334] [G loss: 0.319656]\n",
            "500\n",
            "13 [D loss: 0.244706] [G loss: 0.271957]\n",
            "550\n",
            "13 [D loss: 0.168991] [G loss: 0.249859]\n",
            "600\n",
            "13 [D loss: 0.222280] [G loss: 0.217494]\n",
            "650\n",
            "13 [D loss: 0.213548] [G loss: 0.228508]\n",
            "700\n",
            "13 [D loss: 0.256053] [G loss: 0.237279]\n",
            "750\n",
            "13 [D loss: 0.221198] [G loss: 0.264634]\n",
            "800\n",
            "13 [D loss: 0.335247] [G loss: 0.235006]\n",
            "850\n",
            "13 [D loss: 0.078096] [G loss: 0.224233]\n",
            "900\n",
            "13 [D loss: 0.192419] [G loss: 0.201023]\n",
            "950\n",
            "13 [D loss: 0.154876] [G loss: 0.191628]\n",
            "1000\n",
            "13 [D loss: 0.232813] [G loss: 0.178888]\n",
            "1050\n",
            "13 [D loss: 0.227939] [G loss: 0.186682]\n",
            "1100\n",
            "13 [D loss: 0.117660] [G loss: 0.183538]\n",
            "1150\n",
            "13 [D loss: 0.230385] [G loss: 0.211319]\n",
            "0\n",
            "14 [D loss: 0.085634] [G loss: 0.183449]\n",
            "50\n",
            "14 [D loss: 0.246095] [G loss: 0.186409]\n",
            "100\n",
            "14 [D loss: 0.241276] [G loss: 0.163089]\n",
            "150\n",
            "14 [D loss: 0.133564] [G loss: 0.185689]\n",
            "200\n",
            "14 [D loss: 0.248843] [G loss: 0.167549]\n",
            "250\n",
            "14 [D loss: 0.271011] [G loss: 0.208271]\n",
            "300\n",
            "14 [D loss: 0.178294] [G loss: 0.185082]\n",
            "350\n",
            "14 [D loss: 0.202441] [G loss: 0.226196]\n",
            "400\n",
            "14 [D loss: 0.195249] [G loss: 0.196688]\n",
            "450\n",
            "14 [D loss: 0.168404] [G loss: 0.265451]\n",
            "500\n",
            "14 [D loss: 0.206551] [G loss: 0.231545]\n",
            "550\n",
            "14 [D loss: 0.244505] [G loss: 0.242427]\n",
            "600\n",
            "14 [D loss: 0.280486] [G loss: 0.211848]\n",
            "650\n",
            "14 [D loss: 0.242640] [G loss: 0.234585]\n",
            "700\n",
            "14 [D loss: 0.089036] [G loss: 0.221032]\n",
            "750\n",
            "14 [D loss: 0.275923] [G loss: 0.228020]\n",
            "800\n",
            "14 [D loss: 0.259290] [G loss: 0.220304]\n",
            "850\n",
            "14 [D loss: 0.235582] [G loss: 0.235644]\n",
            "900\n",
            "14 [D loss: 0.218603] [G loss: 0.204060]\n",
            "950\n",
            "14 [D loss: 0.227799] [G loss: 0.203280]\n",
            "1000\n",
            "14 [D loss: 0.242578] [G loss: 0.190854]\n",
            "1050\n",
            "14 [D loss: 0.101066] [G loss: 0.196363]\n",
            "1100\n",
            "14 [D loss: 0.270030] [G loss: 0.191503]\n",
            "1150\n",
            "14 [D loss: 0.106703] [G loss: 0.204387]\n",
            "0\n",
            "15 [D loss: 0.220609] [G loss: 0.163700]\n",
            "50\n",
            "15 [D loss: 0.185417] [G loss: 0.189993]\n",
            "100\n",
            "15 [D loss: 0.280770] [G loss: 0.173894]\n",
            "150\n",
            "15 [D loss: 0.092061] [G loss: 0.171093]\n",
            "200\n",
            "15 [D loss: 0.274819] [G loss: 0.163195]\n",
            "250\n",
            "15 [D loss: 0.175667] [G loss: 0.194596]\n",
            "300\n",
            "15 [D loss: 0.095636] [G loss: 0.175301]\n",
            "350\n",
            "15 [D loss: 0.201951] [G loss: 0.170342]\n",
            "400\n",
            "15 [D loss: 0.105951] [G loss: 0.143158]\n",
            "450\n",
            "15 [D loss: 0.170846] [G loss: 0.180914]\n",
            "500\n",
            "15 [D loss: 0.253075] [G loss: 0.165459]\n",
            "550\n",
            "15 [D loss: 0.130341] [G loss: 0.188075]\n",
            "600\n",
            "15 [D loss: 0.317138] [G loss: 0.170792]\n",
            "650\n",
            "15 [D loss: 0.244234] [G loss: 0.167499]\n",
            "700\n",
            "15 [D loss: 0.301518] [G loss: 0.146623]\n",
            "750\n",
            "15 [D loss: 0.155492] [G loss: 0.143969]\n",
            "800\n",
            "15 [D loss: 0.204756] [G loss: 0.130212]\n",
            "850\n",
            "15 [D loss: 0.213567] [G loss: 0.135815]\n",
            "900\n",
            "15 [D loss: 0.118871] [G loss: 0.116364]\n",
            "950\n",
            "15 [D loss: 0.134496] [G loss: 0.125672]\n",
            "1000\n",
            "15 [D loss: 0.272490] [G loss: 0.109879]\n",
            "1050\n",
            "15 [D loss: 0.206540] [G loss: 0.164489]\n",
            "1100\n",
            "15 [D loss: 0.175482] [G loss: 0.146338]\n",
            "1150\n",
            "15 [D loss: 0.173586] [G loss: 0.151151]\n",
            "0\n",
            "16 [D loss: 0.337330] [G loss: 0.133238]\n",
            "50\n",
            "16 [D loss: 0.312704] [G loss: 0.144123]\n",
            "100\n",
            "16 [D loss: 0.149512] [G loss: 0.128777]\n",
            "150\n",
            "16 [D loss: 0.218605] [G loss: 0.180082]\n",
            "200\n",
            "16 [D loss: 0.214031] [G loss: 0.140756]\n",
            "250\n",
            "16 [D loss: 0.118172] [G loss: 0.166138]\n",
            "300\n",
            "16 [D loss: 0.130764] [G loss: 0.156537]\n",
            "350\n",
            "16 [D loss: 0.219220] [G loss: 0.165640]\n",
            "400\n",
            "16 [D loss: 0.208187] [G loss: 0.136547]\n",
            "450\n",
            "16 [D loss: 0.150220] [G loss: 0.155301]\n",
            "500\n",
            "16 [D loss: 0.195873] [G loss: 0.136237]\n",
            "550\n",
            "16 [D loss: 0.150051] [G loss: 0.213826]\n",
            "600\n",
            "16 [D loss: 0.142789] [G loss: 0.143377]\n",
            "650\n",
            "16 [D loss: 0.289994] [G loss: 0.229342]\n",
            "700\n",
            "16 [D loss: 0.211485] [G loss: 0.222521]\n",
            "750\n",
            "16 [D loss: 0.098187] [G loss: 0.219608]\n",
            "800\n",
            "16 [D loss: 0.158135] [G loss: 0.201002]\n",
            "850\n",
            "16 [D loss: 0.254660] [G loss: 0.242195]\n",
            "900\n",
            "16 [D loss: 0.202890] [G loss: 0.209343]\n",
            "950\n",
            "16 [D loss: 0.168475] [G loss: 0.230743]\n",
            "1000\n",
            "16 [D loss: 0.179237] [G loss: 0.221799]\n",
            "1050\n",
            "16 [D loss: 0.232485] [G loss: 0.239041]\n",
            "1100\n",
            "16 [D loss: 0.293954] [G loss: 0.238992]\n",
            "1150\n",
            "16 [D loss: 0.207500] [G loss: 0.274944]\n",
            "0\n",
            "17 [D loss: 0.136740] [G loss: 0.236803]\n",
            "50\n",
            "17 [D loss: 0.086215] [G loss: 0.235791]\n",
            "100\n",
            "17 [D loss: 0.195959] [G loss: 0.210979]\n",
            "150\n",
            "17 [D loss: 0.145099] [G loss: 0.240245]\n",
            "200\n",
            "17 [D loss: 0.220151] [G loss: 0.230105]\n",
            "250\n",
            "17 [D loss: 0.198388] [G loss: 0.239322]\n",
            "300\n",
            "17 [D loss: 0.190844] [G loss: 0.219203]\n",
            "350\n",
            "17 [D loss: 0.221636] [G loss: 0.212240]\n",
            "400\n",
            "17 [D loss: 0.096146] [G loss: 0.191557]\n",
            "450\n",
            "17 [D loss: 0.122728] [G loss: 0.220118]\n",
            "500\n",
            "17 [D loss: 0.145634] [G loss: 0.178611]\n",
            "550\n",
            "17 [D loss: 0.128048] [G loss: 0.198268]\n",
            "600\n",
            "17 [D loss: 0.190903] [G loss: 0.178501]\n",
            "650\n",
            "17 [D loss: 0.284136] [G loss: 0.229136]\n",
            "700\n",
            "17 [D loss: 0.214689] [G loss: 0.221745]\n",
            "750\n",
            "17 [D loss: 0.265306] [G loss: 0.251337]\n",
            "800\n",
            "17 [D loss: 0.255306] [G loss: 0.235952]\n",
            "850\n",
            "17 [D loss: 0.211332] [G loss: 0.242759]\n",
            "900\n",
            "17 [D loss: 0.155666] [G loss: 0.227875]\n",
            "950\n",
            "17 [D loss: 0.163369] [G loss: 0.243078]\n",
            "1000\n",
            "17 [D loss: 0.141873] [G loss: 0.208563]\n",
            "1050\n",
            "17 [D loss: 0.326639] [G loss: 0.228016]\n",
            "1100\n",
            "17 [D loss: 0.187556] [G loss: 0.219133]\n",
            "1150\n",
            "17 [D loss: 0.211492] [G loss: 0.243327]\n",
            "0\n",
            "18 [D loss: 0.135715] [G loss: 0.209449]\n",
            "50\n",
            "18 [D loss: 0.225797] [G loss: 0.199341]\n",
            "100\n",
            "18 [D loss: 0.213839] [G loss: 0.190710]\n",
            "150\n",
            "18 [D loss: 0.128605] [G loss: 0.224257]\n",
            "200\n",
            "18 [D loss: 0.205343] [G loss: 0.188623]\n",
            "250\n",
            "18 [D loss: 0.130197] [G loss: 0.192513]\n",
            "300\n",
            "18 [D loss: 0.187640] [G loss: 0.179600]\n",
            "350\n",
            "18 [D loss: 0.179940] [G loss: 0.195318]\n",
            "400\n",
            "18 [D loss: 0.124690] [G loss: 0.165983]\n",
            "450\n",
            "18 [D loss: 0.203488] [G loss: 0.193997]\n",
            "500\n",
            "18 [D loss: 0.147574] [G loss: 0.170848]\n",
            "550\n",
            "18 [D loss: 0.117309] [G loss: 0.155339]\n",
            "600\n",
            "18 [D loss: 0.178921] [G loss: 0.131739]\n",
            "650\n",
            "18 [D loss: 0.142616] [G loss: 0.187106]\n",
            "700\n",
            "18 [D loss: 0.169761] [G loss: 0.150291]\n",
            "750\n",
            "18 [D loss: 0.143511] [G loss: 0.204039]\n",
            "800\n",
            "18 [D loss: 0.178682] [G loss: 0.188147]\n",
            "850\n",
            "18 [D loss: 0.219771] [G loss: 0.217684]\n",
            "900\n",
            "18 [D loss: 0.216232] [G loss: 0.175396]\n",
            "950\n",
            "18 [D loss: 0.199531] [G loss: 0.187902]\n",
            "1000\n",
            "18 [D loss: 0.209658] [G loss: 0.162676]\n",
            "1050\n",
            "18 [D loss: 0.187067] [G loss: 0.192953]\n",
            "1100\n",
            "18 [D loss: 0.274536] [G loss: 0.184332]\n",
            "1150\n",
            "18 [D loss: 0.203486] [G loss: 0.198235]\n",
            "0\n",
            "19 [D loss: 0.192871] [G loss: 0.158187]\n",
            "50\n",
            "19 [D loss: 0.199921] [G loss: 0.141525]\n",
            "100\n",
            "19 [D loss: 0.298934] [G loss: 0.116442]\n",
            "150\n",
            "19 [D loss: 0.070012] [G loss: 0.153617]\n",
            "200\n",
            "19 [D loss: 0.106877] [G loss: 0.142109]\n",
            "250\n",
            "19 [D loss: 0.194083] [G loss: 0.168400]\n",
            "300\n",
            "19 [D loss: 0.250030] [G loss: 0.133939]\n",
            "350\n",
            "19 [D loss: 0.208330] [G loss: 0.161984]\n",
            "400\n",
            "19 [D loss: 0.213309] [G loss: 0.160913]\n",
            "450\n",
            "19 [D loss: 0.132888] [G loss: 0.162868]\n",
            "500\n",
            "19 [D loss: 0.233865] [G loss: 0.135176]\n",
            "550\n",
            "19 [D loss: 0.226602] [G loss: 0.181583]\n",
            "600\n",
            "19 [D loss: 0.093068] [G loss: 0.155127]\n",
            "650\n",
            "19 [D loss: 0.143731] [G loss: 0.208823]\n",
            "700\n",
            "19 [D loss: 0.237999] [G loss: 0.210790]\n",
            "750\n",
            "19 [D loss: 0.193634] [G loss: 0.185639]\n",
            "800\n",
            "19 [D loss: 0.142211] [G loss: 0.177150]\n",
            "850\n",
            "19 [D loss: 0.255760] [G loss: 0.211905]\n",
            "900\n",
            "19 [D loss: 0.157140] [G loss: 0.164304]\n",
            "950\n",
            "19 [D loss: 0.252881] [G loss: 0.212964]\n",
            "1000\n",
            "19 [D loss: 0.182263] [G loss: 0.216082]\n",
            "1050\n",
            "19 [D loss: 0.192193] [G loss: 0.246175]\n",
            "1100\n",
            "19 [D loss: 0.256186] [G loss: 0.231262]\n",
            "1150\n",
            "19 [D loss: 0.259279] [G loss: 0.244610]\n",
            "0\n",
            "20 [D loss: 0.089178] [G loss: 0.216731]\n",
            "50\n",
            "20 [D loss: 0.136326] [G loss: 0.239613]\n",
            "100\n",
            "20 [D loss: 0.286630] [G loss: 0.212652]\n",
            "150\n",
            "20 [D loss: 0.195966] [G loss: 0.230770]\n",
            "200\n",
            "20 [D loss: 0.158520] [G loss: 0.195255]\n",
            "250\n",
            "20 [D loss: 0.220742] [G loss: 0.208741]\n",
            "300\n",
            "20 [D loss: 0.162648] [G loss: 0.190316]\n",
            "350\n",
            "20 [D loss: 0.240979] [G loss: 0.217100]\n",
            "400\n",
            "20 [D loss: 0.095361] [G loss: 0.202510]\n",
            "450\n",
            "20 [D loss: 0.216598] [G loss: 0.210503]\n",
            "500\n",
            "20 [D loss: 0.144065] [G loss: 0.188063]\n",
            "550\n",
            "20 [D loss: 0.149858] [G loss: 0.207144]\n",
            "600\n",
            "20 [D loss: 0.206083] [G loss: 0.192653]\n",
            "650\n",
            "20 [D loss: 0.161790] [G loss: 0.189162]\n",
            "700\n",
            "20 [D loss: 0.173194] [G loss: 0.182902]\n",
            "750\n",
            "20 [D loss: 0.213840] [G loss: 0.200260]\n",
            "800\n",
            "20 [D loss: 0.290428] [G loss: 0.188041]\n",
            "850\n",
            "20 [D loss: 0.120828] [G loss: 0.208704]\n",
            "900\n",
            "20 [D loss: 0.129852] [G loss: 0.186319]\n",
            "950\n",
            "20 [D loss: 0.152993] [G loss: 0.188398]\n",
            "1000\n",
            "20 [D loss: 0.251486] [G loss: 0.185345]\n",
            "1050\n",
            "20 [D loss: 0.194053] [G loss: 0.174497]\n",
            "1100\n",
            "20 [D loss: 0.136534] [G loss: 0.165396]\n",
            "1150\n",
            "20 [D loss: 0.213420] [G loss: 0.190103]\n",
            "0\n",
            "21 [D loss: 0.152986] [G loss: 0.175236]\n",
            "50\n",
            "21 [D loss: 0.149016] [G loss: 0.190854]\n",
            "100\n",
            "21 [D loss: 0.221005] [G loss: 0.180862]\n",
            "150\n",
            "21 [D loss: 0.110959] [G loss: 0.177232]\n",
            "200\n",
            "21 [D loss: 0.206597] [G loss: 0.165119]\n",
            "250\n",
            "21 [D loss: 0.179350] [G loss: 0.173153]\n",
            "300\n",
            "21 [D loss: 0.227232] [G loss: 0.153340]\n",
            "350\n",
            "21 [D loss: 0.260704] [G loss: 0.162626]\n",
            "400\n",
            "21 [D loss: 0.299712] [G loss: 0.148812]\n",
            "450\n",
            "21 [D loss: 0.285448] [G loss: 0.168511]\n",
            "500\n",
            "21 [D loss: 0.196692] [G loss: 0.158064]\n",
            "550\n",
            "21 [D loss: 0.161766] [G loss: 0.190823]\n",
            "600\n",
            "21 [D loss: 0.263748] [G loss: 0.179685]\n",
            "650\n",
            "21 [D loss: 0.277319] [G loss: 0.204789]\n",
            "700\n",
            "21 [D loss: 0.082747] [G loss: 0.180026]\n",
            "750\n",
            "21 [D loss: 0.124591] [G loss: 0.171657]\n",
            "800\n",
            "21 [D loss: 0.131880] [G loss: 0.150481]\n",
            "850\n",
            "21 [D loss: 0.235571] [G loss: 0.174835]\n",
            "900\n",
            "21 [D loss: 0.198733] [G loss: 0.157084]\n",
            "950\n",
            "21 [D loss: 0.196261] [G loss: 0.177862]\n",
            "1000\n",
            "21 [D loss: 0.268548] [G loss: 0.161733]\n",
            "1050\n",
            "21 [D loss: 0.188984] [G loss: 0.184873]\n",
            "1100\n",
            "21 [D loss: 0.182991] [G loss: 0.166609]\n",
            "1150\n",
            "21 [D loss: 0.164677] [G loss: 0.191367]\n",
            "0\n",
            "22 [D loss: 0.152386] [G loss: 0.155568]\n",
            "50\n",
            "22 [D loss: 0.168059] [G loss: 0.164252]\n",
            "100\n",
            "22 [D loss: 0.096298] [G loss: 0.148720]\n",
            "150\n",
            "22 [D loss: 0.108441] [G loss: 0.150575]\n",
            "200\n",
            "22 [D loss: 0.187189] [G loss: 0.138312]\n",
            "250\n",
            "22 [D loss: 0.245433] [G loss: 0.160126]\n",
            "300\n",
            "22 [D loss: 0.169686] [G loss: 0.148485]\n",
            "350\n",
            "22 [D loss: 0.277230] [G loss: 0.159731]\n",
            "400\n",
            "22 [D loss: 0.143459] [G loss: 0.132664]\n",
            "450\n",
            "22 [D loss: 0.255848] [G loss: 0.147229]\n",
            "500\n",
            "22 [D loss: 0.210151] [G loss: 0.145842]\n",
            "550\n",
            "22 [D loss: 0.188295] [G loss: 0.168949]\n",
            "600\n",
            "22 [D loss: 0.269342] [G loss: 0.149853]\n",
            "650\n",
            "22 [D loss: 0.155826] [G loss: 0.150212]\n",
            "700\n",
            "22 [D loss: 0.261781] [G loss: 0.146904]\n",
            "750\n",
            "22 [D loss: 0.185214] [G loss: 0.181043]\n",
            "800\n",
            "22 [D loss: 0.103911] [G loss: 0.132456]\n",
            "850\n",
            "22 [D loss: 0.198469] [G loss: 0.175799]\n",
            "900\n",
            "22 [D loss: 0.119521] [G loss: 0.146671]\n",
            "950\n",
            "22 [D loss: 0.198089] [G loss: 0.155857]\n",
            "1000\n",
            "22 [D loss: 0.198139] [G loss: 0.134460]\n",
            "1050\n",
            "22 [D loss: 0.128874] [G loss: 0.195211]\n",
            "1100\n",
            "22 [D loss: 0.162250] [G loss: 0.184507]\n",
            "1150\n",
            "22 [D loss: 0.147093] [G loss: 0.179188]\n",
            "0\n",
            "23 [D loss: 0.227414] [G loss: 0.168787]\n",
            "50\n",
            "23 [D loss: 0.201068] [G loss: 0.162305]\n",
            "100\n",
            "23 [D loss: 0.200804] [G loss: 0.149993]\n",
            "150\n",
            "23 [D loss: 0.183879] [G loss: 0.156388]\n",
            "200\n",
            "23 [D loss: 0.189144] [G loss: 0.142481]\n",
            "250\n",
            "23 [D loss: 0.205214] [G loss: 0.226854]\n",
            "300\n",
            "23 [D loss: 0.177889] [G loss: 0.205310]\n",
            "350\n",
            "23 [D loss: 0.230008] [G loss: 0.225899]\n",
            "400\n",
            "23 [D loss: 0.228998] [G loss: 0.202716]\n",
            "450\n",
            "23 [D loss: 0.249560] [G loss: 0.229822]\n",
            "500\n",
            "23 [D loss: 0.236670] [G loss: 0.218780]\n",
            "550\n",
            "23 [D loss: 0.273562] [G loss: 0.255252]\n",
            "600\n",
            "23 [D loss: 0.290252] [G loss: 0.234548]\n",
            "650\n",
            "23 [D loss: 0.285911] [G loss: 0.226780]\n",
            "700\n",
            "23 [D loss: 0.124935] [G loss: 0.212596]\n",
            "750\n",
            "23 [D loss: 0.197043] [G loss: 0.209831]\n",
            "800\n",
            "23 [D loss: 0.070660] [G loss: 0.192722]\n",
            "850\n",
            "23 [D loss: 0.236324] [G loss: 0.188331]\n",
            "900\n",
            "23 [D loss: 0.210033] [G loss: 0.146758]\n",
            "950\n",
            "23 [D loss: 0.236528] [G loss: 0.197295]\n",
            "1000\n",
            "23 [D loss: 0.136811] [G loss: 0.187655]\n",
            "1050\n",
            "23 [D loss: 0.214880] [G loss: 0.194910]\n",
            "1100\n",
            "23 [D loss: 0.124367] [G loss: 0.174520]\n",
            "1150\n",
            "23 [D loss: 0.201533] [G loss: 0.206232]\n",
            "0\n",
            "24 [D loss: 2.775788] [G loss: 0.229697]\n",
            "50\n",
            "24 [D loss: 0.261543] [G loss: 0.185896]\n",
            "100\n",
            "24 [D loss: 2.760740] [G loss: 0.198987]\n",
            "150\n",
            "24 [D loss: 0.212159] [G loss: 0.200615]\n",
            "200\n",
            "24 [D loss: 2.900198] [G loss: 0.219581]\n",
            "250\n",
            "24 [D loss: 0.202480] [G loss: 0.181700]\n",
            "300\n",
            "24 [D loss: 2.679020] [G loss: 0.203295]\n",
            "350\n",
            "24 [D loss: 0.104414] [G loss: 0.184116]\n",
            "400\n",
            "24 [D loss: 2.863099] [G loss: 0.183725]\n",
            "450\n",
            "24 [D loss: 0.136395] [G loss: 0.183809]\n",
            "500\n",
            "24 [D loss: 2.753108] [G loss: 0.211450]\n",
            "550\n",
            "24 [D loss: 0.273964] [G loss: 0.180217]\n",
            "600\n",
            "24 [D loss: 2.671253] [G loss: 0.192407]\n",
            "650\n",
            "24 [D loss: 0.096489] [G loss: 0.178751]\n",
            "700\n",
            "24 [D loss: 2.872969] [G loss: 0.189609]\n",
            "750\n",
            "24 [D loss: 0.256064] [G loss: 0.185845]\n",
            "800\n",
            "24 [D loss: 2.773418] [G loss: 0.212206]\n",
            "850\n",
            "24 [D loss: 0.229465] [G loss: 0.180745]\n",
            "900\n",
            "24 [D loss: 2.723321] [G loss: 0.178551]\n",
            "950\n",
            "24 [D loss: 0.148102] [G loss: 0.159410]\n",
            "1000\n",
            "24 [D loss: 2.811509] [G loss: 0.173918]\n",
            "1050\n",
            "24 [D loss: 0.178350] [G loss: 0.166763]\n",
            "1100\n",
            "24 [D loss: 2.845423] [G loss: 0.192104]\n",
            "1150\n",
            "24 [D loss: 0.228208] [G loss: 0.173278]\n",
            "0\n",
            "25 [D loss: 0.185429] [G loss: 0.152523]\n",
            "50\n",
            "25 [D loss: 0.113874] [G loss: 0.166641]\n",
            "100\n",
            "25 [D loss: 0.245202] [G loss: 0.167617]\n",
            "150\n",
            "25 [D loss: 0.300043] [G loss: 0.187722]\n",
            "200\n",
            "25 [D loss: 0.172286] [G loss: 0.174532]\n",
            "250\n",
            "25 [D loss: 0.224151] [G loss: 0.185384]\n",
            "300\n",
            "25 [D loss: 0.184691] [G loss: 0.159660]\n",
            "350\n",
            "25 [D loss: 0.250971] [G loss: 0.192981]\n",
            "400\n",
            "25 [D loss: 0.249360] [G loss: 0.168122]\n",
            "450\n",
            "25 [D loss: 0.179411] [G loss: 0.198273]\n",
            "500\n",
            "25 [D loss: 0.252652] [G loss: 0.179770]\n",
            "550\n",
            "25 [D loss: 0.163093] [G loss: 0.167970]\n",
            "600\n",
            "25 [D loss: 0.230900] [G loss: 0.153217]\n",
            "650\n",
            "25 [D loss: 0.249815] [G loss: 0.184342]\n",
            "700\n",
            "25 [D loss: 0.245465] [G loss: 0.163088]\n",
            "750\n",
            "25 [D loss: 0.286651] [G loss: 0.179589]\n",
            "800\n",
            "25 [D loss: 0.224257] [G loss: 0.167216]\n",
            "850\n",
            "25 [D loss: 0.242654] [G loss: 0.198605]\n",
            "900\n",
            "25 [D loss: 0.174930] [G loss: 0.171191]\n",
            "950\n",
            "25 [D loss: 0.176852] [G loss: 0.178416]\n",
            "1000\n",
            "25 [D loss: 0.218357] [G loss: 0.146641]\n",
            "1050\n",
            "25 [D loss: 0.275501] [G loss: 0.161961]\n",
            "1100\n",
            "25 [D loss: 0.125062] [G loss: 0.145233]\n",
            "1150\n",
            "25 [D loss: 0.177484] [G loss: 0.171860]\n",
            "0\n",
            "26 [D loss: 0.315279] [G loss: 0.134417]\n",
            "50\n",
            "26 [D loss: 0.243924] [G loss: 0.100696]\n",
            "100\n",
            "26 [D loss: 0.177200] [G loss: 0.112534]\n",
            "150\n",
            "26 [D loss: 0.163499] [G loss: 0.113369]\n",
            "200\n",
            "26 [D loss: 0.228257] [G loss: 0.101062]\n",
            "250\n",
            "26 [D loss: 0.085290] [G loss: 0.110536]\n",
            "300\n",
            "26 [D loss: 0.158849] [G loss: 0.104427]\n",
            "350\n",
            "26 [D loss: 0.231831] [G loss: 0.117324]\n",
            "400\n",
            "26 [D loss: 0.120718] [G loss: 0.112676]\n",
            "450\n",
            "26 [D loss: 0.241849] [G loss: 0.130899]\n",
            "500\n",
            "26 [D loss: 0.114892] [G loss: 0.120977]\n",
            "550\n",
            "26 [D loss: 0.199804] [G loss: 0.146337]\n",
            "600\n",
            "26 [D loss: 0.210878] [G loss: 0.137278]\n",
            "650\n",
            "26 [D loss: 0.240286] [G loss: 0.155314]\n",
            "700\n",
            "26 [D loss: 0.284542] [G loss: 0.149262]\n",
            "750\n",
            "26 [D loss: 0.194046] [G loss: 0.167681]\n",
            "800\n",
            "26 [D loss: 0.237735] [G loss: 0.157920]\n",
            "850\n",
            "26 [D loss: 0.122040] [G loss: 0.166980]\n",
            "900\n",
            "26 [D loss: 0.248041] [G loss: 0.157090]\n",
            "950\n",
            "26 [D loss: 0.205264] [G loss: 0.168530]\n",
            "1000\n",
            "26 [D loss: 0.224912] [G loss: 0.163554]\n",
            "1050\n",
            "26 [D loss: 0.161659] [G loss: 0.165931]\n",
            "1100\n",
            "26 [D loss: 0.185355] [G loss: 0.150999]\n",
            "1150\n",
            "26 [D loss: 0.227310] [G loss: 0.179017]\n",
            "0\n",
            "27 [D loss: 0.169062] [G loss: 0.162313]\n",
            "50\n",
            "27 [D loss: 0.143595] [G loss: 0.171639]\n",
            "100\n",
            "27 [D loss: 0.192251] [G loss: 0.158862]\n",
            "150\n",
            "27 [D loss: 0.319906] [G loss: 0.166289]\n",
            "200\n",
            "27 [D loss: 0.151047] [G loss: 0.156810]\n",
            "250\n",
            "27 [D loss: 0.280957] [G loss: 0.178136]\n",
            "300\n",
            "27 [D loss: 0.153699] [G loss: 0.176593]\n",
            "350\n",
            "27 [D loss: 0.152199] [G loss: 0.185457]\n",
            "400\n",
            "27 [D loss: 0.265204] [G loss: 0.174084]\n",
            "450\n",
            "27 [D loss: 0.113147] [G loss: 0.194437]\n",
            "500\n",
            "27 [D loss: 0.188439] [G loss: 0.180516]\n",
            "550\n",
            "27 [D loss: 0.251848] [G loss: 0.210291]\n",
            "600\n",
            "27 [D loss: 0.317698] [G loss: 0.195799]\n",
            "650\n",
            "27 [D loss: 0.092256] [G loss: 0.195235]\n",
            "700\n",
            "27 [D loss: 0.196329] [G loss: 0.189983]\n",
            "750\n",
            "27 [D loss: 0.198650] [G loss: 0.207444]\n",
            "800\n",
            "27 [D loss: 0.245951] [G loss: 0.194704]\n",
            "850\n",
            "27 [D loss: 0.181498] [G loss: 0.211949]\n",
            "900\n",
            "27 [D loss: 0.187272] [G loss: 0.202229]\n",
            "950\n",
            "27 [D loss: 0.217021] [G loss: 0.229941]\n",
            "1000\n",
            "27 [D loss: 0.235929] [G loss: 0.235816]\n",
            "1050\n",
            "27 [D loss: 0.150799] [G loss: 0.249259]\n",
            "1100\n",
            "27 [D loss: 0.230509] [G loss: 0.242102]\n",
            "1150\n",
            "27 [D loss: 0.173109] [G loss: 0.288620]\n",
            "0\n",
            "28 [D loss: 0.154285] [G loss: 0.264139]\n",
            "50\n",
            "28 [D loss: 0.244227] [G loss: 0.273804]\n",
            "100\n",
            "28 [D loss: 0.231303] [G loss: 0.267680]\n",
            "150\n",
            "28 [D loss: 0.232852] [G loss: 0.278041]\n",
            "200\n",
            "28 [D loss: 0.227991] [G loss: 0.270805]\n",
            "250\n",
            "28 [D loss: 0.148690] [G loss: 0.286012]\n",
            "300\n",
            "28 [D loss: 0.155625] [G loss: 0.269955]\n",
            "350\n",
            "28 [D loss: 0.270164] [G loss: 0.299833]\n",
            "400\n",
            "28 [D loss: 0.151507] [G loss: 0.290331]\n",
            "450\n",
            "28 [D loss: 0.154421] [G loss: 0.315780]\n",
            "500\n",
            "28 [D loss: 0.166193] [G loss: 0.286890]\n",
            "550\n",
            "28 [D loss: 0.210646] [G loss: 0.312816]\n",
            "600\n",
            "28 [D loss: 0.094981] [G loss: 0.286199]\n",
            "650\n",
            "28 [D loss: 0.301649] [G loss: 0.284393]\n",
            "700\n",
            "28 [D loss: 0.205833] [G loss: 0.279175]\n",
            "750\n",
            "28 [D loss: 0.215122] [G loss: 0.300445]\n",
            "800\n",
            "28 [D loss: 0.217994] [G loss: 0.279146]\n",
            "850\n",
            "28 [D loss: 0.137503] [G loss: 0.309664]\n",
            "900\n",
            "28 [D loss: 0.290058] [G loss: 0.280626]\n",
            "950\n",
            "28 [D loss: 0.069186] [G loss: 0.327446]\n",
            "1000\n",
            "28 [D loss: 0.179247] [G loss: 0.317005]\n",
            "1050\n",
            "28 [D loss: 0.151944] [G loss: 0.355213]\n",
            "1100\n",
            "28 [D loss: 0.183665] [G loss: 0.328550]\n",
            "1150\n",
            "28 [D loss: 0.135646] [G loss: 0.371482]\n",
            "0\n",
            "29 [D loss: 0.202793] [G loss: 0.342272]\n",
            "50\n",
            "29 [D loss: 0.184464] [G loss: 0.356465]\n",
            "100\n",
            "29 [D loss: 0.275688] [G loss: 0.327380]\n",
            "150\n",
            "29 [D loss: 0.202312] [G loss: 0.340482]\n",
            "200\n",
            "29 [D loss: 0.316015] [G loss: 0.308384]\n",
            "250\n",
            "29 [D loss: 0.102516] [G loss: 0.323037]\n",
            "300\n",
            "29 [D loss: 0.218734] [G loss: 0.297004]\n",
            "350\n",
            "29 [D loss: 0.193148] [G loss: 0.307910]\n",
            "400\n",
            "29 [D loss: 0.259684] [G loss: 0.286226]\n",
            "450\n",
            "29 [D loss: 0.287401] [G loss: 0.311220]\n",
            "500\n",
            "29 [D loss: 0.237713] [G loss: 0.299594]\n",
            "550\n",
            "29 [D loss: 0.252999] [G loss: 0.318208]\n",
            "600\n",
            "29 [D loss: 0.131994] [G loss: 0.280700]\n",
            "650\n",
            "29 [D loss: 0.281961] [G loss: 0.296991]\n",
            "700\n",
            "29 [D loss: 0.265534] [G loss: 0.276218]\n",
            "750\n",
            "29 [D loss: 0.229925] [G loss: 0.278674]\n",
            "800\n",
            "29 [D loss: 0.177808] [G loss: 0.266269]\n",
            "850\n",
            "29 [D loss: 0.229668] [G loss: 0.291971]\n",
            "900\n",
            "29 [D loss: 0.080500] [G loss: 0.274213]\n",
            "950\n",
            "29 [D loss: 0.104626] [G loss: 0.275039]\n",
            "1000\n",
            "29 [D loss: 0.208060] [G loss: 0.260949]\n",
            "1050\n",
            "29 [D loss: 0.175604] [G loss: 0.281111]\n",
            "1100\n",
            "29 [D loss: 0.094163] [G loss: 0.262707]\n",
            "1150\n",
            "29 [D loss: 0.158085] [G loss: 0.250851]\n",
            "0\n",
            "30 [D loss: 0.183273] [G loss: 0.236843]\n",
            "50\n",
            "30 [D loss: 0.179224] [G loss: 0.252326]\n",
            "100\n",
            "30 [D loss: 0.255303] [G loss: 0.231975]\n",
            "150\n",
            "30 [D loss: 0.167118] [G loss: 0.241187]\n",
            "200\n",
            "30 [D loss: 0.102076] [G loss: 0.221221]\n",
            "250\n",
            "30 [D loss: 0.242814] [G loss: 0.240889]\n",
            "300\n",
            "30 [D loss: 0.247733] [G loss: 0.228846]\n",
            "350\n",
            "30 [D loss: 0.196489] [G loss: 0.251478]\n",
            "400\n",
            "30 [D loss: 0.249482] [G loss: 0.221711]\n",
            "450\n",
            "30 [D loss: 0.206630] [G loss: 0.231480]\n",
            "500\n",
            "30 [D loss: 0.137957] [G loss: 0.210274]\n",
            "550\n",
            "30 [D loss: 0.184893] [G loss: 0.240556]\n",
            "600\n",
            "30 [D loss: 0.175438] [G loss: 0.229446]\n",
            "650\n",
            "30 [D loss: 0.266640] [G loss: 0.233000]\n",
            "700\n",
            "30 [D loss: 0.182276] [G loss: 0.218549]\n",
            "750\n",
            "30 [D loss: 0.146102] [G loss: 0.216681]\n",
            "800\n",
            "30 [D loss: 0.206074] [G loss: 0.213449]\n",
            "850\n",
            "30 [D loss: 0.179576] [G loss: 0.237922]\n",
            "900\n",
            "30 [D loss: 0.327931] [G loss: 0.209947]\n",
            "950\n",
            "30 [D loss: 0.153039] [G loss: 0.241778]\n",
            "1000\n",
            "30 [D loss: 0.217941] [G loss: 0.236021]\n",
            "1050\n",
            "30 [D loss: 0.246172] [G loss: 0.249496]\n",
            "1100\n",
            "30 [D loss: 0.100210] [G loss: 0.222293]\n",
            "1150\n",
            "30 [D loss: 0.248131] [G loss: 0.249997]\n",
            "0\n",
            "31 [D loss: 0.177797] [G loss: 0.218072]\n",
            "50\n",
            "31 [D loss: 0.198578] [G loss: 0.251972]\n",
            "100\n",
            "31 [D loss: 0.237596] [G loss: 0.227102]\n",
            "150\n",
            "31 [D loss: 0.150474] [G loss: 0.253914]\n",
            "200\n",
            "31 [D loss: 0.110463] [G loss: 0.223775]\n",
            "250\n",
            "31 [D loss: 0.207547] [G loss: 0.235068]\n",
            "300\n",
            "31 [D loss: 0.260150] [G loss: 0.216503]\n",
            "350\n",
            "31 [D loss: 0.261825] [G loss: 0.253368]\n",
            "400\n",
            "31 [D loss: 0.187714] [G loss: 0.232065]\n",
            "450\n",
            "31 [D loss: 0.261338] [G loss: 0.219632]\n",
            "500\n",
            "31 [D loss: 0.191227] [G loss: 0.198506]\n",
            "550\n",
            "31 [D loss: 0.214336] [G loss: 0.192463]\n",
            "600\n",
            "31 [D loss: 0.107133] [G loss: 0.173121]\n",
            "650\n",
            "31 [D loss: 0.213866] [G loss: 0.187199]\n",
            "700\n",
            "31 [D loss: 0.087652] [G loss: 0.156511]\n",
            "750\n",
            "31 [D loss: 0.170985] [G loss: 0.170878]\n",
            "800\n",
            "31 [D loss: 0.232357] [G loss: 0.160290]\n",
            "850\n",
            "31 [D loss: 0.115187] [G loss: 0.185035]\n",
            "900\n",
            "31 [D loss: 0.214198] [G loss: 0.176158]\n",
            "950\n",
            "31 [D loss: 0.148533] [G loss: 0.203806]\n",
            "1000\n",
            "31 [D loss: 0.208099] [G loss: 0.185099]\n",
            "1050\n",
            "31 [D loss: 0.219162] [G loss: 0.186132]\n",
            "1100\n",
            "31 [D loss: 0.218422] [G loss: 0.166254]\n",
            "1150\n",
            "31 [D loss: 0.141206] [G loss: 0.227897]\n",
            "0\n",
            "32 [D loss: 0.114943] [G loss: 0.188843]\n",
            "50\n",
            "32 [D loss: 0.239082] [G loss: 0.185559]\n",
            "100\n",
            "32 [D loss: 0.245639] [G loss: 0.174185]\n",
            "150\n",
            "32 [D loss: 0.122975] [G loss: 0.180958]\n",
            "200\n",
            "32 [D loss: 0.122423] [G loss: 0.163313]\n",
            "250\n",
            "32 [D loss: 0.278430] [G loss: 0.173055]\n",
            "300\n",
            "32 [D loss: 0.225954] [G loss: 0.148097]\n",
            "350\n",
            "32 [D loss: 0.262917] [G loss: 0.184760]\n",
            "400\n",
            "32 [D loss: 0.305803] [G loss: 0.181471]\n",
            "450\n",
            "32 [D loss: 0.167542] [G loss: 0.178733]\n",
            "500\n",
            "32 [D loss: 0.260995] [G loss: 0.167093]\n",
            "550\n",
            "32 [D loss: 0.170203] [G loss: 0.189736]\n",
            "600\n",
            "32 [D loss: 0.105608] [G loss: 0.171725]\n",
            "650\n",
            "32 [D loss: 0.288925] [G loss: 0.167130]\n",
            "700\n",
            "32 [D loss: 0.198610] [G loss: 0.152711]\n",
            "750\n",
            "32 [D loss: 0.237635] [G loss: 0.156704]\n",
            "800\n",
            "32 [D loss: 0.266529] [G loss: 0.139665]\n",
            "850\n",
            "32 [D loss: 0.183633] [G loss: 0.171428]\n",
            "900\n",
            "32 [D loss: 0.162091] [G loss: 0.136226]\n",
            "950\n",
            "32 [D loss: 0.163196] [G loss: 0.151115]\n",
            "1000\n",
            "32 [D loss: 0.159113] [G loss: 0.144515]\n",
            "1050\n",
            "32 [D loss: 0.085034] [G loss: 0.159822]\n",
            "1100\n",
            "32 [D loss: 0.308366] [G loss: 0.154203]\n",
            "1150\n",
            "32 [D loss: 0.195988] [G loss: 0.151291]\n",
            "0\n",
            "33 [D loss: 0.198455] [G loss: 0.152471]\n",
            "50\n",
            "33 [D loss: 0.256740] [G loss: 0.164712]\n",
            "100\n",
            "33 [D loss: 0.133156] [G loss: 0.152825]\n",
            "150\n",
            "33 [D loss: 0.272312] [G loss: 0.133405]\n",
            "200\n",
            "33 [D loss: 0.128623] [G loss: 0.111552]\n",
            "250\n",
            "33 [D loss: 0.246896] [G loss: 0.121298]\n",
            "300\n",
            "33 [D loss: 0.184884] [G loss: 0.109102]\n",
            "350\n",
            "33 [D loss: 0.267053] [G loss: 0.124508]\n",
            "400\n",
            "33 [D loss: 0.159419] [G loss: 0.117243]\n",
            "450\n",
            "33 [D loss: 0.157046] [G loss: 0.136101]\n",
            "500\n",
            "33 [D loss: 0.104568] [G loss: 0.123070]\n",
            "550\n",
            "33 [D loss: 0.182133] [G loss: 0.132717]\n",
            "600\n",
            "33 [D loss: 0.228245] [G loss: 0.116803]\n",
            "650\n",
            "33 [D loss: 0.192909] [G loss: 0.139625]\n",
            "700\n",
            "33 [D loss: 0.285464] [G loss: 0.132559]\n",
            "750\n",
            "33 [D loss: 0.069587] [G loss: 0.142754]\n",
            "800\n",
            "33 [D loss: 0.279188] [G loss: 0.130170]\n",
            "850\n",
            "33 [D loss: 0.150997] [G loss: 0.156896]\n",
            "900\n",
            "33 [D loss: 0.206693] [G loss: 0.139187]\n",
            "950\n",
            "33 [D loss: 0.241117] [G loss: 0.146278]\n",
            "1000\n",
            "33 [D loss: 0.234186] [G loss: 0.146582]\n",
            "1050\n",
            "33 [D loss: 0.219398] [G loss: 0.168569]\n",
            "1100\n",
            "33 [D loss: 0.262448] [G loss: 0.138647]\n",
            "1150\n",
            "33 [D loss: 0.154409] [G loss: 0.163713]\n",
            "0\n",
            "34 [D loss: 0.107895] [G loss: 0.150659]\n",
            "50\n",
            "34 [D loss: 0.226816] [G loss: 0.146984]\n",
            "100\n",
            "34 [D loss: 0.176603] [G loss: 0.149088]\n",
            "150\n",
            "34 [D loss: 0.184022] [G loss: 0.155498]\n",
            "200\n",
            "34 [D loss: 0.203253] [G loss: 0.140484]\n",
            "250\n",
            "34 [D loss: 0.247547] [G loss: 0.154127]\n",
            "300\n",
            "34 [D loss: 0.170324] [G loss: 0.136827]\n",
            "350\n",
            "34 [D loss: 0.114341] [G loss: 0.165811]\n",
            "400\n",
            "34 [D loss: 0.106018] [G loss: 0.146011]\n",
            "450\n",
            "34 [D loss: 0.173223] [G loss: 0.153644]\n",
            "500\n",
            "34 [D loss: 0.129240] [G loss: 0.142321]\n",
            "550\n",
            "34 [D loss: 0.170507] [G loss: 0.152502]\n",
            "600\n",
            "34 [D loss: 0.199560] [G loss: 0.147422]\n",
            "650\n",
            "34 [D loss: 0.110646] [G loss: 0.137326]\n",
            "700\n",
            "34 [D loss: 0.207056] [G loss: 0.120505]\n",
            "750\n",
            "34 [D loss: 0.144194] [G loss: 0.134863]\n",
            "800\n",
            "34 [D loss: 0.211471] [G loss: 0.118203]\n",
            "850\n",
            "34 [D loss: 0.204227] [G loss: 0.114332]\n",
            "900\n",
            "34 [D loss: 0.268173] [G loss: 0.114366]\n",
            "950\n",
            "34 [D loss: 0.188522] [G loss: 0.113510]\n",
            "1000\n",
            "34 [D loss: 0.145869] [G loss: 0.101377]\n",
            "1050\n",
            "34 [D loss: 0.252485] [G loss: 0.126843]\n",
            "1100\n",
            "34 [D loss: 0.208570] [G loss: 0.113798]\n",
            "1150\n",
            "34 [D loss: 0.105077] [G loss: 0.128351]\n",
            "0\n",
            "35 [D loss: 0.109855] [G loss: 0.108448]\n",
            "50\n",
            "35 [D loss: 0.242106] [G loss: 0.114788]\n",
            "100\n",
            "35 [D loss: 0.179199] [G loss: 0.104791]\n",
            "150\n",
            "35 [D loss: 0.189992] [G loss: 0.129546]\n",
            "200\n",
            "35 [D loss: 0.180056] [G loss: 0.105597]\n",
            "250\n",
            "35 [D loss: 0.205079] [G loss: 0.126959]\n",
            "300\n",
            "35 [D loss: 0.168650] [G loss: 0.127508]\n",
            "350\n",
            "35 [D loss: 0.327903] [G loss: 0.154411]\n",
            "400\n",
            "35 [D loss: 0.256930] [G loss: 0.131370]\n",
            "450\n",
            "35 [D loss: 0.168901] [G loss: 0.127126]\n",
            "500\n",
            "35 [D loss: 0.174430] [G loss: 0.112173]\n",
            "550\n",
            "35 [D loss: 0.249527] [G loss: 0.117074]\n",
            "600\n",
            "35 [D loss: 0.193575] [G loss: 0.102683]\n",
            "650\n",
            "35 [D loss: 0.168395] [G loss: 0.113776]\n",
            "700\n",
            "35 [D loss: 0.196163] [G loss: 0.106983]\n",
            "750\n",
            "35 [D loss: 0.087934] [G loss: 0.114286]\n",
            "800\n",
            "35 [D loss: 0.344900] [G loss: 0.101654]\n",
            "850\n",
            "35 [D loss: 0.263100] [G loss: 0.126837]\n",
            "900\n",
            "35 [D loss: 0.288469] [G loss: 0.105694]\n",
            "950\n",
            "35 [D loss: 0.201178] [G loss: 0.123510]\n",
            "1000\n",
            "35 [D loss: 0.143261] [G loss: 0.112960]\n",
            "1050\n",
            "35 [D loss: 0.099361] [G loss: 0.138371]\n",
            "1100\n",
            "35 [D loss: 0.209433] [G loss: 0.129003]\n",
            "1150\n",
            "35 [D loss: 0.249370] [G loss: 0.139519]\n",
            "0\n",
            "36 [D loss: 0.253112] [G loss: 0.118456]\n",
            "50\n",
            "36 [D loss: 0.319284] [G loss: 0.112764]\n",
            "100\n",
            "36 [D loss: 0.309944] [G loss: 0.107124]\n",
            "150\n",
            "36 [D loss: 0.197996] [G loss: 0.111271]\n",
            "200\n",
            "36 [D loss: 0.187222] [G loss: 0.107822]\n",
            "250\n",
            "36 [D loss: 0.291892] [G loss: 0.116902]\n",
            "300\n",
            "36 [D loss: 0.173241] [G loss: 0.104213]\n",
            "350\n",
            "36 [D loss: 0.290566] [G loss: 0.141896]\n",
            "400\n",
            "36 [D loss: 0.157875] [G loss: 0.108305]\n",
            "450\n",
            "36 [D loss: 0.115450] [G loss: 0.143232]\n",
            "500\n",
            "36 [D loss: 0.106438] [G loss: 0.122080]\n",
            "550\n",
            "36 [D loss: 0.207471] [G loss: 0.131430]\n",
            "600\n",
            "36 [D loss: 0.306897] [G loss: 0.112039]\n",
            "650\n",
            "36 [D loss: 0.174953] [G loss: 0.111678]\n",
            "700\n",
            "36 [D loss: 0.264888] [G loss: 0.104114]\n",
            "750\n",
            "36 [D loss: 0.240847] [G loss: 0.106632]\n",
            "800\n",
            "36 [D loss: 0.296221] [G loss: 0.103987]\n",
            "850\n",
            "36 [D loss: 0.264863] [G loss: 0.111880]\n",
            "900\n",
            "36 [D loss: 0.230191] [G loss: 0.099073]\n",
            "950\n",
            "36 [D loss: 0.188970] [G loss: 0.107263]\n",
            "1000\n",
            "36 [D loss: 0.193466] [G loss: 0.090955]\n",
            "1050\n",
            "36 [D loss: 0.199900] [G loss: 0.120936]\n",
            "1100\n",
            "36 [D loss: 0.207105] [G loss: 0.109109]\n",
            "1150\n",
            "36 [D loss: 0.225174] [G loss: 0.120478]\n",
            "0\n",
            "37 [D loss: 0.127863] [G loss: 0.117287]\n",
            "50\n",
            "37 [D loss: 0.245969] [G loss: 0.146048]\n",
            "100\n",
            "37 [D loss: 0.213550] [G loss: 0.123548]\n",
            "150\n",
            "37 [D loss: 0.278223] [G loss: 0.137770]\n",
            "200\n",
            "37 [D loss: 0.172375] [G loss: 0.128766]\n",
            "250\n",
            "37 [D loss: 0.283215] [G loss: 0.137516]\n",
            "300\n",
            "37 [D loss: 0.317837] [G loss: 0.131729]\n",
            "350\n",
            "37 [D loss: 0.158929] [G loss: 0.152332]\n",
            "400\n",
            "37 [D loss: 0.234236] [G loss: 0.136067]\n",
            "450\n",
            "37 [D loss: 0.321903] [G loss: 0.143390]\n",
            "500\n",
            "37 [D loss: 0.192154] [G loss: 0.129804]\n",
            "550\n",
            "37 [D loss: 0.098671] [G loss: 0.146255]\n",
            "600\n",
            "37 [D loss: 0.170635] [G loss: 0.128266]\n",
            "650\n",
            "37 [D loss: 0.276926] [G loss: 0.143104]\n",
            "700\n",
            "37 [D loss: 0.190717] [G loss: 0.130440]\n",
            "750\n",
            "37 [D loss: 0.224955] [G loss: 0.154419]\n",
            "800\n",
            "37 [D loss: 0.085594] [G loss: 0.142896]\n",
            "850\n",
            "37 [D loss: 0.216863] [G loss: 0.167754]\n",
            "900\n",
            "37 [D loss: 0.194089] [G loss: 0.160485]\n",
            "950\n",
            "37 [D loss: 0.192687] [G loss: 0.171817]\n",
            "1000\n",
            "37 [D loss: 0.175982] [G loss: 0.148528]\n",
            "1050\n",
            "37 [D loss: 0.107345] [G loss: 0.187045]\n",
            "1100\n",
            "37 [D loss: 0.230229] [G loss: 0.181486]\n",
            "1150\n",
            "37 [D loss: 0.160334] [G loss: 0.224298]\n",
            "0\n",
            "38 [D loss: 0.180149] [G loss: 0.196756]\n",
            "50\n",
            "38 [D loss: 0.232910] [G loss: 0.222786]\n",
            "100\n",
            "38 [D loss: 0.287722] [G loss: 0.214135]\n",
            "150\n",
            "38 [D loss: 0.248264] [G loss: 0.240612]\n",
            "200\n",
            "38 [D loss: 0.109036] [G loss: 0.232232]\n",
            "250\n",
            "38 [D loss: 0.258023] [G loss: 0.238187]\n",
            "300\n",
            "38 [D loss: 0.110914] [G loss: 0.229923]\n",
            "350\n",
            "38 [D loss: 0.240936] [G loss: 0.253474]\n",
            "400\n",
            "38 [D loss: 0.090337] [G loss: 0.220254]\n",
            "450\n",
            "38 [D loss: 0.318887] [G loss: 0.237836]\n",
            "500\n",
            "38 [D loss: 0.183750] [G loss: 0.218637]\n",
            "550\n",
            "38 [D loss: 0.247050] [G loss: 0.267648]\n",
            "600\n",
            "38 [D loss: 0.138399] [G loss: 0.225073]\n",
            "650\n",
            "38 [D loss: 0.219591] [G loss: 0.233090]\n",
            "700\n",
            "38 [D loss: 0.298531] [G loss: 0.222592]\n",
            "750\n",
            "38 [D loss: 0.267215] [G loss: 0.246742]\n",
            "800\n",
            "38 [D loss: 0.271489] [G loss: 0.230883]\n",
            "850\n",
            "38 [D loss: 0.208417] [G loss: 0.245218]\n",
            "900\n",
            "38 [D loss: 0.121417] [G loss: 0.227298]\n",
            "950\n",
            "38 [D loss: 0.272829] [G loss: 0.241730]\n",
            "1000\n",
            "38 [D loss: 0.189521] [G loss: 0.214392]\n",
            "1050\n",
            "38 [D loss: 0.190164] [G loss: 0.232363]\n",
            "1100\n",
            "38 [D loss: 0.304782] [G loss: 0.217123]\n",
            "1150\n",
            "38 [D loss: 0.096035] [G loss: 0.256557]\n",
            "0\n",
            "39 [D loss: 0.245874] [G loss: 0.231897]\n",
            "50\n",
            "39 [D loss: 0.101514] [G loss: 0.250878]\n",
            "100\n",
            "39 [D loss: 0.324699] [G loss: 0.233723]\n",
            "150\n",
            "39 [D loss: 0.269731] [G loss: 0.234618]\n",
            "200\n",
            "39 [D loss: 0.128228] [G loss: 0.228995]\n",
            "250\n",
            "39 [D loss: 0.166615] [G loss: 0.228167]\n",
            "300\n",
            "39 [D loss: 0.113591] [G loss: 0.211321]\n",
            "350\n",
            "39 [D loss: 0.228981] [G loss: 0.223340]\n",
            "400\n",
            "39 [D loss: 0.280978] [G loss: 0.198545]\n",
            "450\n",
            "39 [D loss: 0.238730] [G loss: 0.223513]\n",
            "500\n",
            "39 [D loss: 0.183573] [G loss: 0.208674]\n",
            "550\n",
            "39 [D loss: 0.276260] [G loss: 0.217957]\n",
            "600\n",
            "39 [D loss: 0.255496] [G loss: 0.204048]\n",
            "650\n",
            "39 [D loss: 0.151936] [G loss: 0.202359]\n",
            "700\n",
            "39 [D loss: 0.159053] [G loss: 0.192137]\n",
            "750\n",
            "39 [D loss: 0.147983] [G loss: 0.187532]\n",
            "800\n",
            "39 [D loss: 0.259903] [G loss: 0.175596]\n",
            "850\n",
            "39 [D loss: 0.135513] [G loss: 0.184091]\n",
            "900\n",
            "39 [D loss: 0.104452] [G loss: 0.164715]\n",
            "950\n",
            "39 [D loss: 0.278650] [G loss: 0.180565]\n",
            "1000\n",
            "39 [D loss: 0.183295] [G loss: 0.168295]\n",
            "1050\n",
            "39 [D loss: 0.173570] [G loss: 0.172589]\n",
            "1100\n",
            "39 [D loss: 0.266288] [G loss: 0.142542]\n",
            "1150\n",
            "39 [D loss: 0.220522] [G loss: 0.159168]\n",
            "0\n",
            "40 [D loss: 0.145172] [G loss: 0.146906]\n",
            "50\n",
            "40 [D loss: 0.213197] [G loss: 0.159159]\n",
            "100\n",
            "40 [D loss: 0.141072] [G loss: 0.146203]\n",
            "150\n",
            "40 [D loss: 0.164840] [G loss: 0.198914]\n",
            "200\n",
            "40 [D loss: 0.149456] [G loss: 0.182983]\n",
            "250\n",
            "40 [D loss: 0.112926] [G loss: 0.220068]\n",
            "300\n",
            "40 [D loss: 0.065338] [G loss: 0.195296]\n",
            "350\n",
            "40 [D loss: 0.262631] [G loss: 0.198061]\n",
            "400\n",
            "40 [D loss: 0.154133] [G loss: 0.152417]\n",
            "450\n",
            "40 [D loss: 0.196518] [G loss: 0.155289]\n",
            "500\n",
            "40 [D loss: 0.151593] [G loss: 0.149781]\n",
            "550\n",
            "40 [D loss: 0.181332] [G loss: 0.158066]\n",
            "600\n",
            "40 [D loss: 0.130892] [G loss: 0.136558]\n",
            "650\n",
            "40 [D loss: 0.273448] [G loss: 0.148557]\n",
            "700\n",
            "40 [D loss: 0.193993] [G loss: 0.127943]\n",
            "750\n",
            "40 [D loss: 0.165453] [G loss: 0.184657]\n",
            "800\n",
            "40 [D loss: 0.195141] [G loss: 0.163551]\n",
            "850\n",
            "40 [D loss: 0.198820] [G loss: 0.180102]\n",
            "900\n",
            "40 [D loss: 0.222388] [G loss: 0.173713]\n",
            "950\n",
            "40 [D loss: 0.215089] [G loss: 0.204695]\n",
            "1000\n",
            "40 [D loss: 0.333224] [G loss: 0.182054]\n",
            "1050\n",
            "40 [D loss: 0.194076] [G loss: 0.200540]\n",
            "1100\n",
            "40 [D loss: 0.178799] [G loss: 0.190126]\n",
            "1150\n",
            "40 [D loss: 0.227231] [G loss: 0.226583]\n",
            "0\n",
            "41 [D loss: 0.274209] [G loss: 0.209208]\n",
            "50\n",
            "41 [D loss: 0.116513] [G loss: 0.203353]\n",
            "100\n",
            "41 [D loss: 0.097511] [G loss: 0.193606]\n",
            "150\n",
            "41 [D loss: 0.172152] [G loss: 0.203938]\n",
            "200\n",
            "41 [D loss: 0.210260] [G loss: 0.191304]\n",
            "250\n",
            "41 [D loss: 0.269036] [G loss: 0.213887]\n",
            "300\n",
            "41 [D loss: 0.200455] [G loss: 0.195203]\n",
            "350\n",
            "41 [D loss: 0.233302] [G loss: 0.232906]\n",
            "400\n",
            "41 [D loss: 0.286862] [G loss: 0.205520]\n",
            "450\n",
            "41 [D loss: 0.213544] [G loss: 0.203944]\n",
            "500\n",
            "41 [D loss: 0.183181] [G loss: 0.184124]\n",
            "550\n",
            "41 [D loss: 0.155193] [G loss: 0.183157]\n",
            "600\n",
            "41 [D loss: 0.137306] [G loss: 0.173390]\n",
            "650\n",
            "41 [D loss: 0.189990] [G loss: 0.182530]\n",
            "700\n",
            "41 [D loss: 0.178494] [G loss: 0.159216]\n",
            "750\n",
            "41 [D loss: 0.106772] [G loss: 0.166054]\n",
            "800\n",
            "41 [D loss: 0.325993] [G loss: 0.164735]\n",
            "850\n",
            "41 [D loss: 0.221538] [G loss: 0.179744]\n",
            "900\n",
            "41 [D loss: 0.236094] [G loss: 0.164205]\n",
            "950\n",
            "41 [D loss: 0.158648] [G loss: 0.171475]\n",
            "1000\n",
            "41 [D loss: 0.241796] [G loss: 0.154096]\n",
            "1050\n",
            "41 [D loss: 0.106377] [G loss: 0.161090]\n",
            "1100\n",
            "41 [D loss: 0.205429] [G loss: 0.160338]\n",
            "1150\n",
            "41 [D loss: 0.176682] [G loss: 0.162748]\n",
            "0\n",
            "42 [D loss: 0.164473] [G loss: 0.137708]\n",
            "50\n",
            "42 [D loss: 0.185021] [G loss: 0.161970]\n",
            "100\n",
            "42 [D loss: 0.254813] [G loss: 0.155216]\n",
            "150\n",
            "42 [D loss: 0.322597] [G loss: 0.172825]\n",
            "200\n",
            "42 [D loss: 0.271360] [G loss: 0.151661]\n",
            "250\n",
            "42 [D loss: 0.202863] [G loss: 0.158980]\n",
            "300\n",
            "42 [D loss: 0.225508] [G loss: 0.155865]\n",
            "350\n",
            "42 [D loss: 0.300261] [G loss: 0.174396]\n",
            "400\n",
            "42 [D loss: 0.119482] [G loss: 0.166689]\n",
            "450\n",
            "42 [D loss: 0.160379] [G loss: 0.175721]\n",
            "500\n",
            "42 [D loss: 0.196691] [G loss: 0.167714]\n",
            "550\n",
            "42 [D loss: 0.230505] [G loss: 0.185495]\n",
            "600\n",
            "42 [D loss: 0.259334] [G loss: 0.171288]\n",
            "650\n",
            "42 [D loss: 0.149226] [G loss: 0.188328]\n",
            "700\n",
            "42 [D loss: 0.145964] [G loss: 0.180650]\n",
            "750\n",
            "42 [D loss: 0.088926] [G loss: 0.206681]\n",
            "800\n",
            "42 [D loss: 0.173616] [G loss: 0.189898]\n",
            "850\n",
            "42 [D loss: 0.212309] [G loss: 0.188579]\n",
            "900\n",
            "42 [D loss: 0.237883] [G loss: 0.175379]\n",
            "950\n",
            "42 [D loss: 0.241336] [G loss: 0.210653]\n",
            "1000\n",
            "42 [D loss: 0.324571] [G loss: 0.199861]\n",
            "1050\n",
            "42 [D loss: 0.192705] [G loss: 0.219904]\n",
            "1100\n",
            "42 [D loss: 0.301415] [G loss: 0.204501]\n",
            "1150\n",
            "42 [D loss: 0.269565] [G loss: 0.230536]\n",
            "0\n",
            "43 [D loss: 0.143730] [G loss: 0.204932]\n",
            "50\n",
            "43 [D loss: 0.149366] [G loss: 0.232007]\n",
            "100\n",
            "43 [D loss: 0.165103] [G loss: 0.225941]\n",
            "150\n",
            "43 [D loss: 0.184972] [G loss: 0.228163]\n",
            "200\n",
            "43 [D loss: 0.057176] [G loss: 0.201210]\n",
            "250\n",
            "43 [D loss: 0.087075] [G loss: 0.214960]\n",
            "300\n",
            "43 [D loss: 0.160365] [G loss: 0.198497]\n",
            "350\n",
            "43 [D loss: 0.250113] [G loss: 0.215467]\n",
            "400\n",
            "43 [D loss: 0.192679] [G loss: 0.195361]\n",
            "450\n",
            "43 [D loss: 0.257763] [G loss: 0.217875]\n",
            "500\n",
            "43 [D loss: 0.282996] [G loss: 0.219888]\n",
            "550\n",
            "43 [D loss: 0.191819] [G loss: 0.241166]\n",
            "600\n",
            "43 [D loss: 0.216924] [G loss: 0.215704]\n",
            "650\n",
            "43 [D loss: 0.256234] [G loss: 0.232365]\n",
            "700\n",
            "43 [D loss: 0.176092] [G loss: 0.224170]\n",
            "750\n",
            "43 [D loss: 0.278877] [G loss: 0.230748]\n",
            "800\n",
            "43 [D loss: 0.141523] [G loss: 0.216070]\n",
            "850\n",
            "43 [D loss: 0.171456] [G loss: 0.239191]\n",
            "900\n",
            "43 [D loss: 0.297308] [G loss: 0.234028]\n",
            "950\n",
            "43 [D loss: 0.273080] [G loss: 0.245530]\n",
            "1000\n",
            "43 [D loss: 0.120327] [G loss: 0.227646]\n",
            "1050\n",
            "43 [D loss: 0.193209] [G loss: 0.226056]\n",
            "1100\n",
            "43 [D loss: 0.261537] [G loss: 0.225693]\n",
            "1150\n",
            "43 [D loss: 0.119459] [G loss: 0.237442]\n",
            "0\n",
            "44 [D loss: 0.141442] [G loss: 0.212541]\n",
            "50\n",
            "44 [D loss: 0.210701] [G loss: 0.217977]\n",
            "100\n",
            "44 [D loss: 0.267315] [G loss: 0.213253]\n",
            "150\n",
            "44 [D loss: 0.123011] [G loss: 0.229803]\n",
            "200\n",
            "44 [D loss: 0.263730] [G loss: 0.213684]\n",
            "250\n",
            "44 [D loss: 0.133053] [G loss: 0.223375]\n",
            "300\n",
            "44 [D loss: 0.197349] [G loss: 0.226176]\n",
            "350\n",
            "44 [D loss: 0.281396] [G loss: 0.242634]\n",
            "400\n",
            "44 [D loss: 0.284811] [G loss: 0.224237]\n",
            "450\n",
            "44 [D loss: 0.262626] [G loss: 0.241225]\n",
            "500\n",
            "44 [D loss: 0.252937] [G loss: 0.215913]\n",
            "550\n",
            "44 [D loss: 0.073939] [G loss: 0.243538]\n",
            "600\n",
            "44 [D loss: 0.174241] [G loss: 0.225567]\n",
            "650\n",
            "44 [D loss: 0.258016] [G loss: 0.230557]\n",
            "700\n",
            "44 [D loss: 0.167064] [G loss: 0.206825]\n",
            "750\n",
            "44 [D loss: 0.169407] [G loss: 0.210085]\n",
            "800\n",
            "44 [D loss: 0.152758] [G loss: 0.188428]\n",
            "850\n",
            "44 [D loss: 0.152866] [G loss: 0.205544]\n",
            "900\n",
            "44 [D loss: 0.171794] [G loss: 0.177790]\n",
            "950\n",
            "44 [D loss: 0.128884] [G loss: 0.188204]\n",
            "1000\n",
            "44 [D loss: 0.182418] [G loss: 0.185412]\n",
            "1050\n",
            "44 [D loss: 0.209245] [G loss: 0.206162]\n",
            "1100\n",
            "44 [D loss: 0.211377] [G loss: 0.179723]\n",
            "1150\n",
            "44 [D loss: 0.231572] [G loss: 0.175425]\n",
            "0\n",
            "45 [D loss: 0.125949] [G loss: 0.154529]\n",
            "50\n",
            "45 [D loss: 0.255248] [G loss: 0.148233]\n",
            "100\n",
            "45 [D loss: 0.146274] [G loss: 0.150109]\n",
            "150\n",
            "45 [D loss: 0.243532] [G loss: 0.179567]\n",
            "200\n",
            "45 [D loss: 0.117457] [G loss: 0.158309]\n",
            "250\n",
            "45 [D loss: 0.099218] [G loss: 0.145047]\n",
            "300\n",
            "45 [D loss: 0.236403] [G loss: 0.141765]\n",
            "350\n",
            "45 [D loss: 0.198952] [G loss: 0.152339]\n",
            "400\n",
            "45 [D loss: 0.202933] [G loss: 0.137904]\n",
            "450\n",
            "45 [D loss: 0.120806] [G loss: 0.159111]\n",
            "500\n",
            "45 [D loss: 0.201112] [G loss: 0.145519]\n",
            "550\n",
            "45 [D loss: 0.138373] [G loss: 0.168724]\n",
            "600\n",
            "45 [D loss: 0.194515] [G loss: 0.156744]\n",
            "650\n",
            "45 [D loss: 0.163530] [G loss: 0.177115]\n",
            "700\n",
            "45 [D loss: 0.267365] [G loss: 0.159510]\n",
            "750\n",
            "45 [D loss: 0.233181] [G loss: 0.168156]\n",
            "800\n",
            "45 [D loss: 0.192932] [G loss: 0.146823]\n",
            "850\n",
            "45 [D loss: 0.294056] [G loss: 0.152075]\n",
            "900\n",
            "45 [D loss: 0.180885] [G loss: 0.137595]\n",
            "950\n",
            "45 [D loss: 0.279570] [G loss: 0.154771]\n",
            "1000\n",
            "45 [D loss: 0.215122] [G loss: 0.158244]\n",
            "1050\n",
            "45 [D loss: 0.238000] [G loss: 0.167353]\n",
            "1100\n",
            "45 [D loss: 0.197034] [G loss: 0.147609]\n",
            "1150\n",
            "45 [D loss: 0.211753] [G loss: 0.169070]\n",
            "0\n",
            "46 [D loss: 0.238945] [G loss: 0.144085]\n",
            "50\n",
            "46 [D loss: 0.218798] [G loss: 0.186668]\n",
            "100\n",
            "46 [D loss: 0.223060] [G loss: 0.164441]\n",
            "150\n",
            "46 [D loss: 0.195756] [G loss: 0.172412]\n",
            "200\n",
            "46 [D loss: 0.184758] [G loss: 0.152876]\n",
            "250\n",
            "46 [D loss: 0.225863] [G loss: 0.159748]\n",
            "300\n",
            "46 [D loss: 0.285564] [G loss: 0.134561]\n",
            "350\n",
            "46 [D loss: 0.157814] [G loss: 0.194551]\n",
            "400\n",
            "46 [D loss: 0.211717] [G loss: 0.171503]\n",
            "450\n",
            "46 [D loss: 0.180238] [G loss: 0.191202]\n",
            "500\n",
            "46 [D loss: 0.215069] [G loss: 0.180476]\n",
            "550\n",
            "46 [D loss: 0.204873] [G loss: 0.221383]\n",
            "600\n",
            "46 [D loss: 0.268646] [G loss: 0.203960]\n",
            "650\n",
            "46 [D loss: 0.124054] [G loss: 0.217801]\n",
            "700\n",
            "46 [D loss: 0.136523] [G loss: 0.217974]\n",
            "750\n",
            "46 [D loss: 0.232914] [G loss: 0.222430]\n",
            "800\n",
            "46 [D loss: 0.292120] [G loss: 0.206010]\n",
            "850\n",
            "46 [D loss: 0.198587] [G loss: 0.238107]\n",
            "900\n",
            "46 [D loss: 0.206678] [G loss: 0.213938]\n",
            "950\n",
            "46 [D loss: 0.314521] [G loss: 0.215267]\n",
            "1000\n",
            "46 [D loss: 0.233640] [G loss: 0.189783]\n",
            "1050\n",
            "46 [D loss: 0.189500] [G loss: 0.192192]\n",
            "1100\n",
            "46 [D loss: 0.167497] [G loss: 0.183707]\n",
            "1150\n",
            "46 [D loss: 0.313120] [G loss: 0.201092]\n",
            "0\n",
            "47 [D loss: 0.235785] [G loss: 0.186416]\n",
            "50\n",
            "47 [D loss: 0.133713] [G loss: 0.175073]\n",
            "100\n",
            "47 [D loss: 0.240276] [G loss: 0.160705]\n",
            "150\n",
            "47 [D loss: 0.169207] [G loss: 0.189197]\n",
            "200\n",
            "47 [D loss: 0.153411] [G loss: 0.160246]\n",
            "250\n",
            "47 [D loss: 0.077859] [G loss: 0.157126]\n",
            "300\n",
            "47 [D loss: 0.150208] [G loss: 0.153828]\n",
            "350\n",
            "47 [D loss: 0.225487] [G loss: 0.155996]\n",
            "400\n",
            "47 [D loss: 0.301657] [G loss: 0.145883]\n",
            "450\n",
            "47 [D loss: 0.160646] [G loss: 0.152300]\n",
            "500\n",
            "47 [D loss: 0.177490] [G loss: 0.142395]\n",
            "550\n",
            "47 [D loss: 0.154766] [G loss: 0.134816]\n",
            "600\n",
            "47 [D loss: 0.106688] [G loss: 0.119780]\n",
            "650\n",
            "47 [D loss: 0.168395] [G loss: 0.138999]\n",
            "700\n",
            "47 [D loss: 0.181722] [G loss: 0.115431]\n",
            "750\n",
            "47 [D loss: 0.227389] [G loss: 0.127429]\n",
            "800\n",
            "47 [D loss: 0.221726] [G loss: 0.119236]\n",
            "850\n",
            "47 [D loss: 0.270485] [G loss: 0.240178]\n",
            "900\n",
            "47 [D loss: 0.222382] [G loss: 0.196318]\n",
            "950\n",
            "47 [D loss: 0.165945] [G loss: 0.197889]\n",
            "1000\n",
            "47 [D loss: 0.193026] [G loss: 0.187334]\n",
            "1050\n",
            "47 [D loss: 0.144001] [G loss: 0.194893]\n",
            "1100\n",
            "47 [D loss: 0.311924] [G loss: 0.181902]\n",
            "1150\n",
            "47 [D loss: 0.107828] [G loss: 0.171135]\n",
            "0\n",
            "48 [D loss: 2.872454] [G loss: 0.175824]\n",
            "50\n",
            "48 [D loss: 0.248634] [G loss: 0.201720]\n",
            "100\n",
            "48 [D loss: 2.810027] [G loss: 0.230049]\n",
            "150\n",
            "48 [D loss: 0.155218] [G loss: 0.222046]\n",
            "200\n",
            "48 [D loss: 2.825040] [G loss: 0.229911]\n",
            "250\n",
            "48 [D loss: 0.188240] [G loss: 0.222920]\n",
            "300\n",
            "48 [D loss: 2.761060] [G loss: 0.242593]\n",
            "350\n",
            "48 [D loss: 0.190067] [G loss: 0.225155]\n",
            "400\n",
            "48 [D loss: 2.701470] [G loss: 0.223580]\n",
            "450\n",
            "48 [D loss: 0.236562] [G loss: 0.194381]\n",
            "500\n",
            "48 [D loss: 2.715219] [G loss: 0.204021]\n",
            "550\n",
            "48 [D loss: 0.121386] [G loss: 0.182913]\n",
            "600\n",
            "48 [D loss: 2.883313] [G loss: 0.184263]\n",
            "650\n",
            "48 [D loss: 0.147014] [G loss: 0.187328]\n",
            "700\n",
            "48 [D loss: 2.755173] [G loss: 0.203081]\n",
            "750\n",
            "48 [D loss: 0.151322] [G loss: 0.211514]\n",
            "800\n",
            "48 [D loss: 2.798930] [G loss: 0.222961]\n",
            "850\n",
            "48 [D loss: 0.254844] [G loss: 0.190915]\n",
            "900\n",
            "48 [D loss: 2.796317] [G loss: 0.202008]\n",
            "950\n",
            "48 [D loss: 0.181290] [G loss: 0.197207]\n",
            "1000\n",
            "48 [D loss: 2.725148] [G loss: 0.211080]\n",
            "1050\n",
            "48 [D loss: 0.171815] [G loss: 0.217625]\n",
            "1100\n",
            "48 [D loss: 2.683623] [G loss: 0.236245]\n",
            "1150\n",
            "48 [D loss: 0.177264] [G loss: 0.209693]\n",
            "0\n",
            "49 [D loss: 0.174664] [G loss: 0.190236]\n",
            "50\n",
            "49 [D loss: 0.155503] [G loss: 0.203250]\n",
            "100\n",
            "49 [D loss: 0.086892] [G loss: 0.187333]\n",
            "150\n",
            "49 [D loss: 0.213991] [G loss: 0.212863]\n",
            "200\n",
            "49 [D loss: 0.084380] [G loss: 0.191545]\n",
            "250\n",
            "49 [D loss: 0.244277] [G loss: 0.198404]\n",
            "300\n",
            "49 [D loss: 0.115606] [G loss: 0.187820]\n",
            "350\n",
            "49 [D loss: 0.134818] [G loss: 0.201396]\n",
            "400\n",
            "49 [D loss: 0.193485] [G loss: 0.187221]\n",
            "450\n",
            "49 [D loss: 0.148652] [G loss: 0.209156]\n",
            "500\n",
            "49 [D loss: 0.169741] [G loss: 0.192359]\n",
            "550\n",
            "49 [D loss: 0.183737] [G loss: 0.207068]\n",
            "600\n",
            "49 [D loss: 0.227805] [G loss: 0.191040]\n",
            "650\n",
            "49 [D loss: 0.135553] [G loss: 0.177763]\n",
            "700\n",
            "49 [D loss: 0.078034] [G loss: 0.158511]\n",
            "750\n",
            "49 [D loss: 0.212421] [G loss: 0.174116]\n",
            "800\n",
            "49 [D loss: 0.289268] [G loss: 0.151886]\n",
            "850\n",
            "49 [D loss: 0.184694] [G loss: 0.196957]\n",
            "900\n",
            "49 [D loss: 0.232834] [G loss: 0.170882]\n",
            "950\n",
            "49 [D loss: 0.273008] [G loss: 0.185881]\n",
            "1000\n",
            "49 [D loss: 0.191276] [G loss: 0.165855]\n",
            "1050\n",
            "49 [D loss: 0.160104] [G loss: 0.179748]\n",
            "1100\n",
            "49 [D loss: 0.122471] [G loss: 0.165327]\n",
            "1150\n",
            "49 [D loss: 0.208940] [G loss: 0.186444]\n",
            "0\n",
            "50 [D loss: 0.279632] [G loss: 0.172022]\n",
            "50\n",
            "50 [D loss: 0.250849] [G loss: 0.186050]\n",
            "100\n",
            "50 [D loss: 0.284990] [G loss: 0.180212]\n",
            "150\n",
            "50 [D loss: 0.290720] [G loss: 0.186337]\n",
            "200\n",
            "50 [D loss: 0.226068] [G loss: 0.174915]\n",
            "250\n",
            "50 [D loss: 0.183287] [G loss: 0.185175]\n",
            "300\n",
            "50 [D loss: 0.236959] [G loss: 0.172787]\n",
            "350\n",
            "50 [D loss: 0.113176] [G loss: 0.181057]\n",
            "400\n",
            "50 [D loss: 0.322811] [G loss: 0.180812]\n",
            "450\n",
            "50 [D loss: 0.225306] [G loss: 0.201881]\n",
            "500\n",
            "50 [D loss: 0.113041] [G loss: 0.195666]\n",
            "550\n",
            "50 [D loss: 0.246361] [G loss: 0.207175]\n",
            "600\n",
            "50 [D loss: 0.179040] [G loss: 0.175448]\n",
            "650\n",
            "50 [D loss: 0.093308] [G loss: 0.183218]\n",
            "700\n",
            "50 [D loss: 0.155713] [G loss: 0.163901]\n",
            "750\n",
            "50 [D loss: 0.263078] [G loss: 0.189476]\n",
            "800\n",
            "50 [D loss: 0.139206] [G loss: 0.173263]\n",
            "850\n",
            "50 [D loss: 0.207592] [G loss: 0.164732]\n",
            "900\n",
            "50 [D loss: 0.123249] [G loss: 0.148553]\n",
            "950\n",
            "50 [D loss: 0.241052] [G loss: 0.165607]\n",
            "1000\n",
            "50 [D loss: 0.097036] [G loss: 0.152251]\n",
            "1050\n",
            "50 [D loss: 0.215840] [G loss: 0.166400]\n",
            "1100\n",
            "50 [D loss: 0.204825] [G loss: 0.151756]\n",
            "1150\n",
            "50 [D loss: 0.190850] [G loss: 0.167365]\n",
            "0\n",
            "51 [D loss: 0.281283] [G loss: 0.152858]\n",
            "50\n",
            "51 [D loss: 0.273229] [G loss: 0.167185]\n",
            "100\n",
            "51 [D loss: 0.238896] [G loss: 0.153297]\n",
            "150\n",
            "51 [D loss: 0.287937] [G loss: 0.208605]\n",
            "200\n",
            "51 [D loss: 0.205687] [G loss: 0.218092]\n",
            "250\n",
            "51 [D loss: 0.256731] [G loss: 0.228224]\n",
            "300\n",
            "51 [D loss: 0.284291] [G loss: 0.212802]\n",
            "350\n",
            "51 [D loss: 0.195802] [G loss: 0.224808]\n",
            "400\n",
            "51 [D loss: 0.159603] [G loss: 0.198962]\n",
            "450\n",
            "51 [D loss: 0.115822] [G loss: 0.194702]\n",
            "500\n",
            "51 [D loss: 0.200727] [G loss: 0.190706]\n",
            "550\n",
            "51 [D loss: 0.290912] [G loss: 0.220639]\n",
            "600\n",
            "51 [D loss: 0.278355] [G loss: 0.214138]\n",
            "650\n",
            "51 [D loss: 0.170422] [G loss: 0.237186]\n",
            "700\n",
            "51 [D loss: 0.266728] [G loss: 0.226079]\n",
            "750\n",
            "51 [D loss: 0.167294] [G loss: 0.221011]\n",
            "800\n",
            "51 [D loss: 0.317583] [G loss: 0.182064]\n",
            "850\n",
            "51 [D loss: 0.151757] [G loss: 0.203085]\n",
            "900\n",
            "51 [D loss: 0.202904] [G loss: 0.185327]\n",
            "950\n",
            "51 [D loss: 0.172425] [G loss: 0.196372]\n",
            "1000\n",
            "51 [D loss: 0.173551] [G loss: 0.189405]\n",
            "1050\n",
            "51 [D loss: 0.190843] [G loss: 0.207894]\n",
            "1100\n",
            "51 [D loss: 0.294259] [G loss: 0.199996]\n",
            "1150\n",
            "51 [D loss: 0.106019] [G loss: 0.204382]\n",
            "0\n",
            "52 [D loss: 0.262298] [G loss: 0.181396]\n",
            "50\n",
            "52 [D loss: 0.258351] [G loss: 0.206400]\n",
            "100\n",
            "52 [D loss: 0.280658] [G loss: 0.197036]\n",
            "150\n",
            "52 [D loss: 0.267178] [G loss: 0.210056]\n",
            "200\n",
            "52 [D loss: 0.221507] [G loss: 0.198815]\n",
            "250\n",
            "52 [D loss: 0.252936] [G loss: 0.213377]\n",
            "300\n",
            "52 [D loss: 0.141684] [G loss: 0.205655]\n",
            "350\n",
            "52 [D loss: 0.239839] [G loss: 0.211956]\n",
            "400\n",
            "52 [D loss: 0.149497] [G loss: 0.187726]\n",
            "450\n",
            "52 [D loss: 0.071359] [G loss: 0.262138]\n",
            "500\n",
            "52 [D loss: 0.206711] [G loss: 0.233248]\n",
            "550\n",
            "52 [D loss: 0.135821] [G loss: 0.255123]\n",
            "600\n",
            "52 [D loss: 0.119732] [G loss: 0.234652]\n",
            "650\n",
            "52 [D loss: 0.207056] [G loss: 0.255965]\n",
            "700\n",
            "52 [D loss: 0.139027] [G loss: 0.247670]\n",
            "750\n",
            "52 [D loss: 0.198366] [G loss: 0.266937]\n",
            "800\n",
            "52 [D loss: 0.241449] [G loss: 0.260686]\n",
            "850\n",
            "52 [D loss: 0.189458] [G loss: 0.264911]\n",
            "900\n",
            "52 [D loss: 0.199837] [G loss: 0.246085]\n",
            "950\n",
            "52 [D loss: 0.105473] [G loss: 0.244088]\n",
            "1000\n",
            "52 [D loss: 0.191094] [G loss: 0.240172]\n",
            "1050\n",
            "52 [D loss: 0.224376] [G loss: 0.245155]\n",
            "1100\n",
            "52 [D loss: 0.168372] [G loss: 0.237544]\n",
            "1150\n",
            "52 [D loss: 0.228355] [G loss: 0.246032]\n",
            "0\n",
            "53 [D loss: 0.213923] [G loss: 0.236170]\n",
            "50\n",
            "53 [D loss: 0.192814] [G loss: 0.260520]\n",
            "100\n",
            "53 [D loss: 0.180321] [G loss: 0.240238]\n",
            "150\n",
            "53 [D loss: 0.306067] [G loss: 0.270926]\n",
            "200\n",
            "53 [D loss: 0.313328] [G loss: 0.255379]\n",
            "250\n",
            "53 [D loss: 0.168483] [G loss: 0.256084]\n",
            "300\n",
            "53 [D loss: 0.250151] [G loss: 0.240348]\n",
            "350\n",
            "53 [D loss: 0.241932] [G loss: 0.234585]\n",
            "400\n",
            "53 [D loss: 0.277716] [G loss: 0.217031]\n",
            "450\n",
            "53 [D loss: 0.256208] [G loss: 0.213142]\n",
            "500\n",
            "53 [D loss: 0.226213] [G loss: 0.190878]\n",
            "550\n",
            "53 [D loss: 0.223468] [G loss: 0.207713]\n",
            "600\n",
            "53 [D loss: 0.142233] [G loss: 0.177616]\n",
            "650\n",
            "53 [D loss: 0.187717] [G loss: 0.191175]\n",
            "700\n",
            "53 [D loss: 0.150644] [G loss: 0.170298]\n",
            "750\n",
            "53 [D loss: 0.126512] [G loss: 0.223595]\n",
            "800\n",
            "53 [D loss: 0.082868] [G loss: 0.198335]\n",
            "850\n",
            "53 [D loss: 0.155335] [G loss: 0.205104]\n",
            "900\n",
            "53 [D loss: 0.124753] [G loss: 0.187598]\n",
            "950\n",
            "53 [D loss: 0.210289] [G loss: 0.199294]\n",
            "1000\n",
            "53 [D loss: 0.155795] [G loss: 0.194123]\n",
            "1050\n",
            "53 [D loss: 0.178841] [G loss: 0.252656]\n",
            "1100\n",
            "53 [D loss: 0.213916] [G loss: 0.236981]\n",
            "1150\n",
            "53 [D loss: 0.246296] [G loss: 0.272947]\n",
            "0\n",
            "54 [D loss: 0.252173] [G loss: 0.243887]\n",
            "50\n",
            "54 [D loss: 0.263483] [G loss: 0.273618]\n",
            "100\n",
            "54 [D loss: 0.201673] [G loss: 0.249680]\n",
            "150\n",
            "54 [D loss: 0.212257] [G loss: 0.273234]\n",
            "200\n",
            "54 [D loss: 0.218942] [G loss: 0.269805]\n",
            "250\n",
            "54 [D loss: 0.238435] [G loss: 0.279504]\n",
            "300\n",
            "54 [D loss: 0.154520] [G loss: 0.271147]\n",
            "350\n",
            "54 [D loss: 0.174111] [G loss: 0.296620]\n",
            "400\n",
            "54 [D loss: 0.109782] [G loss: 0.279504]\n",
            "450\n",
            "54 [D loss: 0.114209] [G loss: 0.298863]\n",
            "500\n",
            "54 [D loss: 0.165921] [G loss: 0.267806]\n",
            "550\n",
            "54 [D loss: 0.216718] [G loss: 0.274037]\n",
            "600\n",
            "54 [D loss: 0.258192] [G loss: 0.257575]\n",
            "650\n",
            "54 [D loss: 0.168288] [G loss: 0.262897]\n",
            "700\n",
            "54 [D loss: 0.274289] [G loss: 0.254041]\n",
            "750\n",
            "54 [D loss: 0.191289] [G loss: 0.259595]\n",
            "800\n",
            "54 [D loss: 0.230221] [G loss: 0.232554]\n",
            "850\n",
            "54 [D loss: 0.132582] [G loss: 0.252591]\n",
            "900\n",
            "54 [D loss: 0.207693] [G loss: 0.240397]\n",
            "950\n",
            "54 [D loss: 0.158792] [G loss: 0.233506]\n",
            "1000\n",
            "54 [D loss: 0.150319] [G loss: 0.235386]\n",
            "1050\n",
            "54 [D loss: 0.191518] [G loss: 0.244671]\n",
            "1100\n",
            "54 [D loss: 0.135036] [G loss: 0.233052]\n",
            "1150\n",
            "54 [D loss: 0.200109] [G loss: 0.256345]\n",
            "0\n",
            "55 [D loss: 0.246617] [G loss: 0.238697]\n",
            "50\n",
            "55 [D loss: 0.277371] [G loss: 0.253057]\n",
            "100\n",
            "55 [D loss: 0.205987] [G loss: 0.226593]\n",
            "150\n",
            "55 [D loss: 0.243728] [G loss: 0.221439]\n",
            "200\n",
            "55 [D loss: 0.162769] [G loss: 0.217058]\n",
            "250\n",
            "55 [D loss: 0.141448] [G loss: 0.221732]\n",
            "300\n",
            "55 [D loss: 0.162304] [G loss: 0.214273]\n",
            "350\n",
            "55 [D loss: 0.212605] [G loss: 0.216679]\n",
            "400\n",
            "55 [D loss: 0.066256] [G loss: 0.202568]\n",
            "450\n",
            "55 [D loss: 0.264974] [G loss: 0.202167]\n",
            "500\n",
            "55 [D loss: 0.260089] [G loss: 0.201983]\n",
            "550\n",
            "55 [D loss: 0.165571] [G loss: 0.223891]\n",
            "600\n",
            "55 [D loss: 0.204466] [G loss: 0.198008]\n",
            "650\n",
            "55 [D loss: 0.238032] [G loss: 0.212006]\n",
            "700\n",
            "55 [D loss: 0.169769] [G loss: 0.204376]\n",
            "750\n",
            "55 [D loss: 0.145840] [G loss: 0.220089]\n",
            "800\n",
            "55 [D loss: 0.075382] [G loss: 0.209360]\n",
            "850\n",
            "55 [D loss: 0.300936] [G loss: 0.244198]\n",
            "900\n",
            "55 [D loss: 0.211911] [G loss: 0.206114]\n",
            "950\n",
            "55 [D loss: 0.283072] [G loss: 0.247493]\n",
            "1000\n",
            "55 [D loss: 0.061832] [G loss: 0.225786]\n",
            "1050\n",
            "55 [D loss: 0.140859] [G loss: 0.249872]\n",
            "1100\n",
            "55 [D loss: 0.083767] [G loss: 0.252697]\n",
            "1150\n",
            "55 [D loss: 0.210903] [G loss: 0.269098]\n",
            "0\n",
            "56 [D loss: 2.791487] [G loss: 0.255559]\n",
            "50\n",
            "56 [D loss: 0.222005] [G loss: 0.230529]\n",
            "100\n",
            "56 [D loss: 2.746479] [G loss: 0.248608]\n",
            "150\n",
            "56 [D loss: 0.213986] [G loss: 0.209456]\n",
            "200\n",
            "56 [D loss: 2.904890] [G loss: 0.208063]\n",
            "250\n",
            "56 [D loss: 0.107660] [G loss: 0.202121]\n",
            "300\n",
            "56 [D loss: 2.644517] [G loss: 0.205133]\n",
            "350\n",
            "56 [D loss: 0.138358] [G loss: 0.187225]\n",
            "400\n",
            "56 [D loss: 2.724072] [G loss: 0.203090]\n",
            "450\n",
            "56 [D loss: 0.280320] [G loss: 0.185540]\n",
            "500\n",
            "56 [D loss: 2.783347] [G loss: 0.182075]\n",
            "550\n",
            "56 [D loss: 0.290328] [G loss: 0.170654]\n",
            "600\n",
            "56 [D loss: 2.747155] [G loss: 0.186708]\n",
            "650\n",
            "56 [D loss: 0.168397] [G loss: 0.167245]\n",
            "700\n",
            "56 [D loss: 2.751304] [G loss: 0.185375]\n",
            "750\n",
            "56 [D loss: 0.223647] [G loss: 0.192337]\n",
            "800\n",
            "56 [D loss: 2.802330] [G loss: 0.223715]\n",
            "850\n",
            "56 [D loss: 0.219423] [G loss: 0.174413]\n",
            "900\n",
            "56 [D loss: 2.754258] [G loss: 0.186560]\n",
            "950\n",
            "56 [D loss: 0.259644] [G loss: 0.173358]\n",
            "1000\n",
            "56 [D loss: 2.728009] [G loss: 0.162804]\n",
            "1050\n",
            "56 [D loss: 0.104963] [G loss: 0.154369]\n",
            "1100\n",
            "56 [D loss: 2.682898] [G loss: 0.168255]\n",
            "1150\n",
            "56 [D loss: 0.140154] [G loss: 0.183599]\n",
            "0\n",
            "57 [D loss: 0.173158] [G loss: 0.192555]\n",
            "50\n",
            "57 [D loss: 0.302797] [G loss: 0.200158]\n",
            "100\n",
            "57 [D loss: 0.158432] [G loss: 0.192166]\n",
            "150\n",
            "57 [D loss: 0.171352] [G loss: 0.192676]\n",
            "200\n",
            "57 [D loss: 0.277191] [G loss: 0.193766]\n",
            "250\n",
            "57 [D loss: 0.285075] [G loss: 0.207765]\n",
            "300\n",
            "57 [D loss: 0.272380] [G loss: 0.197716]\n",
            "350\n",
            "57 [D loss: 0.183716] [G loss: 0.209043]\n",
            "400\n",
            "57 [D loss: 0.102895] [G loss: 0.188249]\n",
            "450\n",
            "57 [D loss: 0.214649] [G loss: 0.186083]\n",
            "500\n",
            "57 [D loss: 0.189387] [G loss: 0.168971]\n",
            "550\n",
            "57 [D loss: 0.200775] [G loss: 0.189597]\n",
            "600\n",
            "57 [D loss: 0.151351] [G loss: 0.166297]\n",
            "650\n",
            "57 [D loss: 0.211214] [G loss: 0.207865]\n",
            "700\n",
            "57 [D loss: 0.184093] [G loss: 0.186100]\n",
            "750\n",
            "57 [D loss: 0.089073] [G loss: 0.210852]\n",
            "800\n",
            "57 [D loss: 0.149658] [G loss: 0.197164]\n",
            "850\n",
            "57 [D loss: 0.165440] [G loss: 0.201855]\n",
            "900\n",
            "57 [D loss: 0.251725] [G loss: 0.184668]\n",
            "950\n",
            "57 [D loss: 0.264475] [G loss: 0.225697]\n",
            "1000\n",
            "57 [D loss: 0.205504] [G loss: 0.220112]\n",
            "1050\n",
            "57 [D loss: 0.242164] [G loss: 0.235007]\n",
            "1100\n",
            "57 [D loss: 0.193405] [G loss: 0.218158]\n",
            "1150\n",
            "57 [D loss: 0.212268] [G loss: 0.234077]\n",
            "0\n",
            "58 [D loss: 0.313208] [G loss: 0.217625]\n",
            "50\n",
            "58 [D loss: 0.131517] [G loss: 0.229491]\n",
            "100\n",
            "58 [D loss: 0.264435] [G loss: 0.212416]\n",
            "150\n",
            "58 [D loss: 0.267172] [G loss: 0.221847]\n",
            "200\n",
            "58 [D loss: 0.158920] [G loss: 0.198738]\n",
            "250\n",
            "58 [D loss: 0.187406] [G loss: 0.209241]\n",
            "300\n",
            "58 [D loss: 0.279360] [G loss: 0.194884]\n",
            "350\n",
            "58 [D loss: 0.219390] [G loss: 0.196082]\n",
            "400\n",
            "58 [D loss: 0.147241] [G loss: 0.172410]\n",
            "450\n",
            "58 [D loss: 0.186281] [G loss: 0.187155]\n",
            "500\n",
            "58 [D loss: 0.285238] [G loss: 0.175381]\n",
            "550\n",
            "58 [D loss: 0.204182] [G loss: 0.203035]\n",
            "600\n",
            "58 [D loss: 0.094746] [G loss: 0.182134]\n",
            "650\n",
            "58 [D loss: 0.168376] [G loss: 0.201178]\n",
            "700\n",
            "58 [D loss: 0.255886] [G loss: 0.187437]\n",
            "750\n",
            "58 [D loss: 0.248376] [G loss: 0.200259]\n",
            "800\n",
            "58 [D loss: 0.183855] [G loss: 0.173725]\n",
            "850\n",
            "58 [D loss: 0.134005] [G loss: 0.185523]\n",
            "900\n",
            "58 [D loss: 0.177899] [G loss: 0.175692]\n",
            "950\n",
            "58 [D loss: 0.166099] [G loss: 0.172244]\n",
            "1000\n",
            "58 [D loss: 0.295642] [G loss: 0.152472]\n",
            "1050\n",
            "58 [D loss: 0.268835] [G loss: 0.178388]\n",
            "1100\n",
            "58 [D loss: 0.197433] [G loss: 0.171445]\n",
            "1150\n",
            "58 [D loss: 0.206425] [G loss: 0.181068]\n",
            "0\n",
            "59 [D loss: 0.262866] [G loss: 0.154616]\n",
            "50\n",
            "59 [D loss: 0.257431] [G loss: 0.147610]\n",
            "100\n",
            "59 [D loss: 0.158729] [G loss: 0.133482]\n",
            "150\n",
            "59 [D loss: 0.199410] [G loss: 0.151060]\n",
            "200\n",
            "59 [D loss: 0.140078] [G loss: 0.134717]\n",
            "250\n",
            "59 [D loss: 0.109880] [G loss: 0.164349]\n",
            "300\n",
            "59 [D loss: 0.142685] [G loss: 0.160438]\n",
            "350\n",
            "59 [D loss: 0.150489] [G loss: 0.194565]\n",
            "400\n",
            "59 [D loss: 0.170248] [G loss: 0.156489]\n",
            "450\n",
            "59 [D loss: 0.198893] [G loss: 0.168088]\n",
            "500\n",
            "59 [D loss: 0.284717] [G loss: 0.154287]\n",
            "550\n",
            "59 [D loss: 0.192317] [G loss: 0.173329]\n",
            "600\n",
            "59 [D loss: 0.205276] [G loss: 0.167733]\n",
            "650\n",
            "59 [D loss: 0.168852] [G loss: 0.184047]\n",
            "700\n",
            "59 [D loss: 0.266382] [G loss: 0.186906]\n",
            "750\n",
            "59 [D loss: 0.153568] [G loss: 0.215132]\n",
            "800\n",
            "59 [D loss: 0.155760] [G loss: 0.192632]\n",
            "850\n",
            "59 [D loss: 0.302550] [G loss: 0.196820]\n",
            "900\n",
            "59 [D loss: 0.328763] [G loss: 0.173106]\n",
            "950\n",
            "59 [D loss: 0.240740] [G loss: 0.163282]\n",
            "1000\n",
            "59 [D loss: 0.182342] [G loss: 0.159703]\n",
            "1050\n",
            "59 [D loss: 0.157768] [G loss: 0.171003]\n",
            "1100\n",
            "59 [D loss: 0.174094] [G loss: 0.167055]\n",
            "1150\n",
            "59 [D loss: 0.212908] [G loss: 0.172856]\n",
            "0\n",
            "60 [D loss: 0.213582] [G loss: 0.146575]\n",
            "50\n",
            "60 [D loss: 0.229417] [G loss: 0.177052]\n",
            "100\n",
            "60 [D loss: 0.240118] [G loss: 0.168263]\n",
            "150\n",
            "60 [D loss: 0.203974] [G loss: 0.193434]\n",
            "200\n",
            "60 [D loss: 0.206737] [G loss: 0.173094]\n",
            "250\n",
            "60 [D loss: 0.269038] [G loss: 0.207365]\n",
            "300\n",
            "60 [D loss: 0.290808] [G loss: 0.189858]\n",
            "350\n",
            "60 [D loss: 0.219405] [G loss: 0.219068]\n",
            "400\n",
            "60 [D loss: 0.210798] [G loss: 0.213565]\n",
            "450\n",
            "60 [D loss: 0.144291] [G loss: 0.228108]\n",
            "500\n",
            "60 [D loss: 0.185076] [G loss: 0.211642]\n",
            "550\n",
            "60 [D loss: 0.136146] [G loss: 0.249984]\n",
            "600\n",
            "60 [D loss: 0.169558] [G loss: 0.232686]\n",
            "650\n",
            "60 [D loss: 0.180618] [G loss: 0.238271]\n",
            "700\n",
            "60 [D loss: 0.330409] [G loss: 0.220273]\n",
            "750\n",
            "60 [D loss: 0.199641] [G loss: 0.223585]\n",
            "800\n",
            "60 [D loss: 0.244620] [G loss: 0.218214]\n",
            "850\n",
            "60 [D loss: 0.270951] [G loss: 0.235544]\n",
            "900\n",
            "60 [D loss: 0.168140] [G loss: 0.212175]\n",
            "950\n",
            "60 [D loss: 0.221836] [G loss: 0.233924]\n",
            "1000\n",
            "60 [D loss: 0.309500] [G loss: 0.200943]\n",
            "1050\n",
            "60 [D loss: 0.185787] [G loss: 0.231058]\n",
            "1100\n",
            "60 [D loss: 0.276756] [G loss: 0.213000]\n",
            "1150\n",
            "60 [D loss: 0.217463] [G loss: 0.234767]\n",
            "0\n",
            "61 [D loss: 0.219289] [G loss: 0.226639]\n",
            "50\n",
            "61 [D loss: 0.203635] [G loss: 0.209547]\n",
            "100\n",
            "61 [D loss: 0.196790] [G loss: 0.191573]\n",
            "150\n",
            "61 [D loss: 0.251390] [G loss: 0.206056]\n",
            "200\n",
            "61 [D loss: 0.311485] [G loss: 0.203758]\n",
            "250\n",
            "61 [D loss: 0.234555] [G loss: 0.225952]\n",
            "300\n",
            "61 [D loss: 0.291621] [G loss: 0.219478]\n",
            "350\n",
            "61 [D loss: 0.211329] [G loss: 0.229769]\n",
            "400\n",
            "61 [D loss: 0.192939] [G loss: 0.202988]\n",
            "450\n",
            "61 [D loss: 0.182181] [G loss: 0.208985]\n",
            "500\n",
            "61 [D loss: 0.166333] [G loss: 0.185436]\n",
            "550\n",
            "61 [D loss: 0.140917] [G loss: 0.198408]\n",
            "600\n",
            "61 [D loss: 0.207845] [G loss: 0.199145]\n",
            "650\n",
            "61 [D loss: 0.081561] [G loss: 0.194492]\n",
            "700\n",
            "61 [D loss: 0.087441] [G loss: 0.182254]\n",
            "750\n",
            "61 [D loss: 0.298878] [G loss: 0.167819]\n",
            "800\n",
            "61 [D loss: 0.176003] [G loss: 0.149320]\n",
            "850\n",
            "61 [D loss: 0.248844] [G loss: 0.169094]\n",
            "900\n",
            "61 [D loss: 0.153941] [G loss: 0.158176]\n",
            "950\n",
            "61 [D loss: 0.223165] [G loss: 0.157856]\n",
            "1000\n",
            "61 [D loss: 0.135936] [G loss: 0.146736]\n",
            "1050\n",
            "61 [D loss: 0.130861] [G loss: 0.149460]\n",
            "1100\n",
            "61 [D loss: 0.253482] [G loss: 0.134276]\n",
            "1150\n",
            "61 [D loss: 0.298756] [G loss: 0.148197]\n",
            "0\n",
            "62 [D loss: 0.135718] [G loss: 0.137907]\n",
            "50\n",
            "62 [D loss: 0.075910] [G loss: 0.137805]\n",
            "100\n",
            "62 [D loss: 0.188278] [G loss: 0.122810]\n",
            "150\n",
            "62 [D loss: 0.105696] [G loss: 0.129763]\n",
            "200\n",
            "62 [D loss: 0.146039] [G loss: 0.113812]\n",
            "250\n",
            "62 [D loss: 0.141091] [G loss: 0.131551]\n",
            "300\n",
            "62 [D loss: 0.250001] [G loss: 0.138227]\n",
            "350\n",
            "62 [D loss: 0.220985] [G loss: 0.169480]\n",
            "400\n",
            "62 [D loss: 0.237277] [G loss: 0.167568]\n",
            "450\n",
            "62 [D loss: 0.089230] [G loss: 0.215048]\n",
            "500\n",
            "62 [D loss: 0.251518] [G loss: 0.216047]\n",
            "550\n",
            "62 [D loss: 0.250292] [G loss: 0.271236]\n",
            "600\n",
            "62 [D loss: 0.124604] [G loss: 0.266549]\n",
            "650\n",
            "62 [D loss: 0.153559] [G loss: 0.268032]\n",
            "700\n",
            "62 [D loss: 0.117596] [G loss: 0.252200]\n",
            "750\n",
            "62 [D loss: 0.227917] [G loss: 0.245805]\n",
            "800\n",
            "62 [D loss: 0.140466] [G loss: 0.232962]\n",
            "850\n",
            "62 [D loss: 0.236194] [G loss: 0.241146]\n",
            "900\n",
            "62 [D loss: 0.190085] [G loss: 0.223431]\n",
            "950\n",
            "62 [D loss: 0.082164] [G loss: 0.232034]\n",
            "1000\n",
            "62 [D loss: 0.189483] [G loss: 0.209163]\n",
            "1050\n",
            "62 [D loss: 0.217477] [G loss: 0.203278]\n",
            "1100\n",
            "62 [D loss: 0.159299] [G loss: 0.207868]\n",
            "1150\n",
            "62 [D loss: 0.215398] [G loss: 0.178027]\n",
            "0\n",
            "63 [D loss: 0.118380] [G loss: 0.151325]\n",
            "50\n",
            "63 [D loss: 0.235872] [G loss: 0.168317]\n",
            "100\n",
            "63 [D loss: 0.207782] [G loss: 0.150583]\n",
            "150\n",
            "63 [D loss: 0.184500] [G loss: 0.167199]\n",
            "200\n",
            "63 [D loss: 0.232209] [G loss: 0.160850]\n",
            "250\n",
            "63 [D loss: 0.203185] [G loss: 0.158512]\n",
            "300\n",
            "63 [D loss: 0.152119] [G loss: 0.143808]\n",
            "350\n",
            "63 [D loss: 0.209824] [G loss: 0.157276]\n",
            "400\n",
            "63 [D loss: 0.104682] [G loss: 0.136777]\n",
            "450\n",
            "63 [D loss: 0.259840] [G loss: 0.133760]\n",
            "500\n",
            "63 [D loss: 0.163931] [G loss: 0.128227]\n",
            "550\n",
            "63 [D loss: 0.221266] [G loss: 0.144882]\n",
            "600\n",
            "63 [D loss: 0.168405] [G loss: 0.126940]\n",
            "650\n",
            "63 [D loss: 0.150752] [G loss: 0.129043]\n",
            "700\n",
            "63 [D loss: 0.147254] [G loss: 0.128264]\n",
            "750\n",
            "63 [D loss: 0.168517] [G loss: 0.147367]\n",
            "800\n",
            "63 [D loss: 0.276393] [G loss: 0.131977]\n",
            "850\n",
            "63 [D loss: 0.301616] [G loss: 0.149030]\n",
            "900\n",
            "63 [D loss: 0.212279] [G loss: 0.138905]\n",
            "950\n",
            "63 [D loss: 0.082997] [G loss: 0.149944]\n",
            "1000\n",
            "63 [D loss: 0.219057] [G loss: 0.148277]\n",
            "1050\n",
            "63 [D loss: 0.280542] [G loss: 0.167836]\n",
            "1100\n",
            "63 [D loss: 0.087422] [G loss: 0.155959]\n",
            "1150\n",
            "63 [D loss: 0.269093] [G loss: 0.183383]\n",
            "0\n",
            "64 [D loss: 0.091565] [G loss: 0.158280]\n",
            "50\n",
            "64 [D loss: 0.092167] [G loss: 0.168681]\n",
            "100\n",
            "64 [D loss: 0.203247] [G loss: 0.153248]\n",
            "150\n",
            "64 [D loss: 0.238917] [G loss: 0.164011]\n",
            "200\n",
            "64 [D loss: 0.299234] [G loss: 0.141377]\n",
            "250\n",
            "64 [D loss: 0.072904] [G loss: 0.150726]\n",
            "300\n",
            "64 [D loss: 0.239106] [G loss: 0.141886]\n",
            "350\n",
            "64 [D loss: 0.134325] [G loss: 0.149427]\n",
            "400\n",
            "64 [D loss: 0.147173] [G loss: 0.138000]\n",
            "450\n",
            "64 [D loss: 0.283205] [G loss: 0.145855]\n",
            "500\n",
            "64 [D loss: 0.093410] [G loss: 0.136397]\n",
            "550\n",
            "64 [D loss: 0.200139] [G loss: 0.141522]\n",
            "600\n",
            "64 [D loss: 0.206547] [G loss: 0.125949]\n",
            "650\n",
            "64 [D loss: 0.253712] [G loss: 0.135246]\n",
            "700\n",
            "64 [D loss: 0.248348] [G loss: 0.126279]\n",
            "750\n",
            "64 [D loss: 0.167393] [G loss: 0.119966]\n",
            "800\n",
            "64 [D loss: 0.298470] [G loss: 0.117322]\n",
            "850\n",
            "64 [D loss: 0.093470] [G loss: 0.134872]\n",
            "900\n",
            "64 [D loss: 0.152383] [G loss: 0.132406]\n",
            "950\n",
            "64 [D loss: 0.242976] [G loss: 0.144644]\n",
            "1000\n",
            "64 [D loss: 0.174117] [G loss: 0.136846]\n",
            "1050\n",
            "64 [D loss: 0.206544] [G loss: 0.164512]\n",
            "1100\n",
            "64 [D loss: 0.190997] [G loss: 0.152437]\n",
            "1150\n",
            "64 [D loss: 0.152304] [G loss: 0.156326]\n",
            "0\n",
            "65 [D loss: 0.183724] [G loss: 0.151830]\n",
            "50\n",
            "65 [D loss: 0.226937] [G loss: 0.160483]\n",
            "100\n",
            "65 [D loss: 0.135847] [G loss: 0.147351]\n",
            "150\n",
            "65 [D loss: 0.208479] [G loss: 0.149921]\n",
            "200\n",
            "65 [D loss: 0.126516] [G loss: 0.136250]\n",
            "250\n",
            "65 [D loss: 0.322610] [G loss: 0.156684]\n",
            "300\n",
            "65 [D loss: 0.189547] [G loss: 0.146215]\n",
            "350\n",
            "65 [D loss: 0.216974] [G loss: 0.162168]\n",
            "400\n",
            "65 [D loss: 0.215052] [G loss: 0.150871]\n",
            "450\n",
            "65 [D loss: 0.135455] [G loss: 0.161746]\n",
            "500\n",
            "65 [D loss: 0.197162] [G loss: 0.148255]\n",
            "550\n",
            "65 [D loss: 0.213911] [G loss: 0.175665]\n",
            "600\n",
            "65 [D loss: 0.185237] [G loss: 0.150088]\n",
            "650\n",
            "65 [D loss: 0.171991] [G loss: 0.158387]\n",
            "700\n",
            "65 [D loss: 0.156333] [G loss: 0.141368]\n",
            "750\n",
            "65 [D loss: 0.193337] [G loss: 0.170001]\n",
            "800\n",
            "65 [D loss: 0.128267] [G loss: 0.161756]\n",
            "850\n",
            "65 [D loss: 0.254919] [G loss: 0.216184]\n",
            "900\n",
            "65 [D loss: 0.176202] [G loss: 0.206133]\n",
            "950\n",
            "65 [D loss: 0.199357] [G loss: 0.228761]\n",
            "1000\n",
            "65 [D loss: 0.220622] [G loss: 0.211856]\n",
            "1050\n",
            "65 [D loss: 0.285462] [G loss: 0.237771]\n",
            "1100\n",
            "65 [D loss: 0.160122] [G loss: 0.225406]\n",
            "1150\n",
            "65 [D loss: 0.070823] [G loss: 0.231864]\n",
            "0\n",
            "66 [D loss: 0.200505] [G loss: 0.213601]\n",
            "50\n",
            "66 [D loss: 0.129447] [G loss: 0.217254]\n",
            "100\n",
            "66 [D loss: 0.177046] [G loss: 0.214548]\n",
            "150\n",
            "66 [D loss: 0.279146] [G loss: 0.252543]\n",
            "200\n",
            "66 [D loss: 0.164620] [G loss: 0.232004]\n",
            "250\n",
            "66 [D loss: 0.234305] [G loss: 0.272880]\n",
            "300\n",
            "66 [D loss: 0.185597] [G loss: 0.242000]\n",
            "350\n",
            "66 [D loss: 0.215965] [G loss: 0.269823]\n",
            "400\n",
            "66 [D loss: 0.272060] [G loss: 0.250090]\n",
            "450\n",
            "66 [D loss: 0.231075] [G loss: 0.264034]\n",
            "500\n",
            "66 [D loss: 0.171661] [G loss: 0.244823]\n",
            "550\n",
            "66 [D loss: 0.177937] [G loss: 0.276619]\n",
            "600\n",
            "66 [D loss: 0.271486] [G loss: 0.256647]\n",
            "650\n",
            "66 [D loss: 0.151526] [G loss: 0.253273]\n",
            "700\n",
            "66 [D loss: 0.216618] [G loss: 0.244804]\n",
            "750\n",
            "66 [D loss: 0.247730] [G loss: 0.241398]\n",
            "800\n",
            "66 [D loss: 0.140011] [G loss: 0.225505]\n",
            "850\n",
            "66 [D loss: 0.158923] [G loss: 0.252003]\n",
            "900\n",
            "66 [D loss: 0.184119] [G loss: 0.230199]\n",
            "950\n",
            "66 [D loss: 0.209505] [G loss: 0.243766]\n",
            "1000\n",
            "66 [D loss: 0.248734] [G loss: 0.237950]\n",
            "1050\n",
            "66 [D loss: 0.126070] [G loss: 0.253952]\n",
            "1100\n",
            "66 [D loss: 0.248843] [G loss: 0.245836]\n",
            "1150\n",
            "66 [D loss: 0.153317] [G loss: 0.293392]\n",
            "0\n",
            "67 [D loss: 0.296466] [G loss: 0.256221]\n",
            "50\n",
            "67 [D loss: 0.198878] [G loss: 0.274104]\n",
            "100\n",
            "67 [D loss: 0.172126] [G loss: 0.258258]\n",
            "150\n",
            "67 [D loss: 0.194923] [G loss: 0.277113]\n",
            "200\n",
            "67 [D loss: 0.229611] [G loss: 0.253100]\n",
            "250\n",
            "67 [D loss: 0.153985] [G loss: 0.276961]\n",
            "300\n",
            "67 [D loss: 0.165880] [G loss: 0.261931]\n",
            "350\n",
            "67 [D loss: 0.212978] [G loss: 0.280621]\n",
            "400\n",
            "67 [D loss: 0.272858] [G loss: 0.268121]\n",
            "450\n",
            "67 [D loss: 0.220574] [G loss: 0.310339]\n",
            "500\n",
            "67 [D loss: 0.144207] [G loss: 0.289490]\n",
            "550\n",
            "67 [D loss: 0.194433] [G loss: 0.300622]\n",
            "600\n",
            "67 [D loss: 0.197130] [G loss: 0.286747]\n",
            "650\n",
            "67 [D loss: 0.276041] [G loss: 0.310954]\n",
            "700\n",
            "67 [D loss: 0.212486] [G loss: 0.294456]\n",
            "750\n",
            "67 [D loss: 0.282873] [G loss: 0.294526]\n",
            "800\n",
            "67 [D loss: 0.259311] [G loss: 0.284070]\n",
            "850\n",
            "67 [D loss: 0.245187] [G loss: 0.291048]\n",
            "900\n",
            "67 [D loss: 0.196138] [G loss: 0.253256]\n",
            "950\n",
            "67 [D loss: 0.240453] [G loss: 0.248770]\n",
            "1000\n",
            "67 [D loss: 0.225179] [G loss: 0.218635]\n",
            "1050\n",
            "67 [D loss: 0.106834] [G loss: 0.212679]\n",
            "1100\n",
            "67 [D loss: 0.259031] [G loss: 0.198181]\n",
            "1150\n",
            "67 [D loss: 0.212037] [G loss: 0.200738]\n",
            "0\n",
            "68 [D loss: 0.170352] [G loss: 0.204166]\n",
            "50\n",
            "68 [D loss: 0.141779] [G loss: 0.185951]\n",
            "100\n",
            "68 [D loss: 0.095092] [G loss: 0.158659]\n",
            "150\n",
            "68 [D loss: 0.213852] [G loss: 0.182663]\n",
            "200\n",
            "68 [D loss: 0.179050] [G loss: 0.141877]\n",
            "250\n",
            "68 [D loss: 0.243975] [G loss: 0.198279]\n",
            "300\n",
            "68 [D loss: 0.194188] [G loss: 0.174299]\n",
            "350\n",
            "68 [D loss: 0.230500] [G loss: 0.180539]\n",
            "400\n",
            "68 [D loss: 0.206382] [G loss: 0.189172]\n",
            "450\n",
            "68 [D loss: 0.183936] [G loss: 0.191485]\n",
            "500\n",
            "68 [D loss: 0.170147] [G loss: 0.189437]\n",
            "550\n",
            "68 [D loss: 0.150648] [G loss: 0.199716]\n",
            "600\n",
            "68 [D loss: 0.323181] [G loss: 0.183794]\n",
            "650\n",
            "68 [D loss: 0.178645] [G loss: 0.180168]\n",
            "700\n",
            "68 [D loss: 0.148020] [G loss: 0.170987]\n",
            "750\n",
            "68 [D loss: 0.072247] [G loss: 0.186211]\n",
            "800\n",
            "68 [D loss: 0.092410] [G loss: 0.164273]\n",
            "850\n",
            "68 [D loss: 0.182261] [G loss: 0.200893]\n",
            "900\n",
            "68 [D loss: 0.249315] [G loss: 0.186157]\n",
            "950\n",
            "68 [D loss: 0.119462] [G loss: 0.216493]\n",
            "1000\n",
            "68 [D loss: 0.301945] [G loss: 0.174445]\n",
            "1050\n",
            "68 [D loss: 0.171614] [G loss: 0.208537]\n",
            "1100\n",
            "68 [D loss: 0.160078] [G loss: 0.209316]\n",
            "1150\n",
            "68 [D loss: 0.185931] [G loss: 0.217202]\n",
            "0\n",
            "69 [D loss: 0.152214] [G loss: 0.191275]\n",
            "50\n",
            "69 [D loss: 0.229877] [G loss: 0.205082]\n",
            "100\n",
            "69 [D loss: 0.216471] [G loss: 0.200096]\n",
            "150\n",
            "69 [D loss: 0.095263] [G loss: 0.182088]\n",
            "200\n",
            "69 [D loss: 0.268102] [G loss: 0.176244]\n",
            "250\n",
            "69 [D loss: 0.222828] [G loss: 0.195661]\n",
            "300\n",
            "69 [D loss: 0.244000] [G loss: 0.204321]\n",
            "350\n",
            "69 [D loss: 0.146185] [G loss: 0.220176]\n",
            "400\n",
            "69 [D loss: 0.166072] [G loss: 0.205863]\n",
            "450\n",
            "69 [D loss: 0.168806] [G loss: 0.223613]\n",
            "500\n",
            "69 [D loss: 0.263506] [G loss: 0.204082]\n",
            "550\n",
            "69 [D loss: 0.278803] [G loss: 0.228605]\n",
            "600\n",
            "69 [D loss: 0.219171] [G loss: 0.207579]\n",
            "650\n",
            "69 [D loss: 0.190307] [G loss: 0.219046]\n",
            "700\n",
            "69 [D loss: 0.090289] [G loss: 0.196598]\n",
            "750\n",
            "69 [D loss: 0.200891] [G loss: 0.215980]\n",
            "800\n",
            "69 [D loss: 0.208657] [G loss: 0.207716]\n",
            "850\n",
            "69 [D loss: 0.298305] [G loss: 0.221124]\n",
            "900\n",
            "69 [D loss: 0.217404] [G loss: 0.201582]\n",
            "950\n",
            "69 [D loss: 0.149144] [G loss: 0.194828]\n",
            "1000\n",
            "69 [D loss: 0.304336] [G loss: 0.176183]\n",
            "1050\n",
            "69 [D loss: 0.202994] [G loss: 0.157220]\n",
            "1100\n",
            "69 [D loss: 0.126406] [G loss: 0.139824]\n",
            "1150\n",
            "69 [D loss: 0.129541] [G loss: 0.135314]\n",
            "0\n",
            "70 [D loss: 0.234873] [G loss: 0.114617]\n",
            "50\n",
            "70 [D loss: 0.253779] [G loss: 0.136913]\n",
            "100\n",
            "70 [D loss: 0.193339] [G loss: 0.127535]\n",
            "150\n",
            "70 [D loss: 0.141472] [G loss: 0.134023]\n",
            "200\n",
            "70 [D loss: 0.177588] [G loss: 0.111924]\n",
            "250\n",
            "70 [D loss: 0.155217] [G loss: 0.117508]\n",
            "300\n",
            "70 [D loss: 0.099856] [G loss: 0.112690]\n",
            "350\n",
            "70 [D loss: 0.194179] [G loss: 0.113414]\n",
            "400\n",
            "70 [D loss: 0.102243] [G loss: 0.108018]\n",
            "450\n",
            "70 [D loss: 0.250977] [G loss: 0.140148]\n",
            "500\n",
            "70 [D loss: 0.147881] [G loss: 0.130844]\n",
            "550\n",
            "70 [D loss: 0.113468] [G loss: 0.144470]\n",
            "600\n",
            "70 [D loss: 0.261869] [G loss: 0.127047]\n",
            "650\n",
            "70 [D loss: 0.182502] [G loss: 0.152083]\n",
            "700\n",
            "70 [D loss: 0.198973] [G loss: 0.139337]\n",
            "750\n",
            "70 [D loss: 0.161342] [G loss: 0.131101]\n",
            "800\n",
            "70 [D loss: 0.154905] [G loss: 0.099964]\n",
            "850\n",
            "70 [D loss: 0.217806] [G loss: 0.124636]\n",
            "900\n",
            "70 [D loss: 0.215072] [G loss: 0.112073]\n",
            "950\n",
            "70 [D loss: 0.247909] [G loss: 0.145504]\n",
            "1000\n",
            "70 [D loss: 0.111925] [G loss: 0.140033]\n",
            "1050\n",
            "70 [D loss: 0.245102] [G loss: 0.151427]\n",
            "1100\n",
            "70 [D loss: 0.253955] [G loss: 0.124441]\n",
            "1150\n",
            "70 [D loss: 0.167309] [G loss: 0.150993]\n",
            "0\n",
            "71 [D loss: 0.252034] [G loss: 0.136800]\n",
            "50\n",
            "71 [D loss: 0.162321] [G loss: 0.134306]\n",
            "100\n",
            "71 [D loss: 0.234394] [G loss: 0.119364]\n",
            "150\n",
            "71 [D loss: 0.227511] [G loss: 0.133017]\n",
            "200\n",
            "71 [D loss: 0.322530] [G loss: 0.130210]\n",
            "250\n",
            "71 [D loss: 0.201056] [G loss: 0.135280]\n",
            "300\n",
            "71 [D loss: 0.151227] [G loss: 0.126956]\n",
            "350\n",
            "71 [D loss: 0.192840] [G loss: 0.153344]\n",
            "400\n",
            "71 [D loss: 0.261015] [G loss: 0.146609]\n",
            "450\n",
            "71 [D loss: 0.117133] [G loss: 0.145663]\n",
            "500\n",
            "71 [D loss: 0.145958] [G loss: 0.133068]\n",
            "550\n",
            "71 [D loss: 0.174869] [G loss: 0.149240]\n",
            "600\n",
            "71 [D loss: 0.183621] [G loss: 0.139218]\n",
            "650\n",
            "71 [D loss: 0.093257] [G loss: 0.138718]\n",
            "700\n",
            "71 [D loss: 0.209821] [G loss: 0.127426]\n",
            "750\n",
            "71 [D loss: 0.109737] [G loss: 0.145699]\n",
            "800\n",
            "71 [D loss: 0.190493] [G loss: 0.147694]\n",
            "850\n",
            "71 [D loss: 0.137748] [G loss: 0.153619]\n",
            "900\n",
            "71 [D loss: 0.094567] [G loss: 0.142607]\n",
            "950\n",
            "71 [D loss: 0.206436] [G loss: 0.172678]\n",
            "1000\n",
            "71 [D loss: 0.202876] [G loss: 0.164165]\n",
            "1050\n",
            "71 [D loss: 0.178782] [G loss: 0.150448]\n",
            "1100\n",
            "71 [D loss: 0.184724] [G loss: 0.123481]\n",
            "1150\n",
            "71 [D loss: 0.269007] [G loss: 0.155000]\n",
            "0\n",
            "72 [D loss: 0.214564] [G loss: 0.155535]\n",
            "50\n",
            "72 [D loss: 0.260215] [G loss: 0.176422]\n",
            "100\n",
            "72 [D loss: 0.190112] [G loss: 0.166229]\n",
            "150\n",
            "72 [D loss: 0.174347] [G loss: 0.209657]\n",
            "200\n",
            "72 [D loss: 0.285862] [G loss: 0.208099]\n",
            "250\n",
            "72 [D loss: 0.271076] [G loss: 0.212549]\n",
            "300\n",
            "72 [D loss: 0.150066] [G loss: 0.187853]\n",
            "350\n",
            "72 [D loss: 0.229223] [G loss: 0.184867]\n",
            "400\n",
            "72 [D loss: 0.201761] [G loss: 0.174463]\n",
            "450\n",
            "72 [D loss: 0.174709] [G loss: 0.190744]\n",
            "500\n",
            "72 [D loss: 0.262031] [G loss: 0.193613]\n",
            "550\n",
            "72 [D loss: 0.146481] [G loss: 0.217305]\n",
            "600\n",
            "72 [D loss: 0.205454] [G loss: 0.204848]\n",
            "650\n",
            "72 [D loss: 0.140848] [G loss: 0.222884]\n",
            "700\n",
            "72 [D loss: 0.221136] [G loss: 0.215069]\n",
            "750\n",
            "72 [D loss: 0.100324] [G loss: 0.237994]\n",
            "800\n",
            "72 [D loss: 0.320639] [G loss: 0.213511]\n",
            "850\n",
            "72 [D loss: 0.120007] [G loss: 0.238128]\n",
            "900\n",
            "72 [D loss: 0.139571] [G loss: 0.229831]\n",
            "950\n",
            "72 [D loss: 0.188706] [G loss: 0.243556]\n",
            "1000\n",
            "72 [D loss: 0.167273] [G loss: 0.225497]\n",
            "1050\n",
            "72 [D loss: 0.296402] [G loss: 0.246295]\n",
            "1100\n",
            "72 [D loss: 0.165637] [G loss: 0.249815]\n",
            "1150\n",
            "72 [D loss: 0.326257] [G loss: 0.253224]\n",
            "0\n",
            "73 [D loss: 0.252358] [G loss: 0.233908]\n",
            "50\n",
            "73 [D loss: 0.124471] [G loss: 0.251270]\n",
            "100\n",
            "73 [D loss: 0.224427] [G loss: 0.231500]\n",
            "150\n",
            "73 [D loss: 0.318157] [G loss: 0.252668]\n",
            "200\n",
            "73 [D loss: 0.205394] [G loss: 0.231856]\n",
            "250\n",
            "73 [D loss: 0.324396] [G loss: 0.256692]\n",
            "300\n",
            "73 [D loss: 0.132354] [G loss: 0.240193]\n",
            "350\n",
            "73 [D loss: 0.147314] [G loss: 0.256178]\n",
            "400\n",
            "73 [D loss: 0.243368] [G loss: 0.220620]\n",
            "450\n",
            "73 [D loss: 0.304439] [G loss: 0.212106]\n",
            "500\n",
            "73 [D loss: 0.169326] [G loss: 0.206269]\n",
            "550\n",
            "73 [D loss: 0.260364] [G loss: 0.254912]\n",
            "600\n",
            "73 [D loss: 0.268200] [G loss: 0.217779]\n",
            "650\n",
            "73 [D loss: 0.258587] [G loss: 0.214296]\n",
            "700\n",
            "73 [D loss: 0.201833] [G loss: 0.201212]\n",
            "750\n",
            "73 [D loss: 0.261940] [G loss: 0.217634]\n",
            "800\n",
            "73 [D loss: 0.147732] [G loss: 0.203409]\n",
            "850\n",
            "73 [D loss: 0.119170] [G loss: 0.203826]\n",
            "900\n",
            "73 [D loss: 0.091366] [G loss: 0.181676]\n",
            "950\n",
            "73 [D loss: 0.201723] [G loss: 0.205322]\n",
            "1000\n",
            "73 [D loss: 0.157564] [G loss: 0.183770]\n",
            "1050\n",
            "73 [D loss: 0.133987] [G loss: 0.189859]\n",
            "1100\n",
            "73 [D loss: 0.137127] [G loss: 0.173810]\n",
            "1150\n",
            "73 [D loss: 0.272543] [G loss: 0.224756]\n",
            "0\n",
            "74 [D loss: 0.193751] [G loss: 0.220427]\n",
            "50\n",
            "74 [D loss: 0.246007] [G loss: 0.243414]\n",
            "100\n",
            "74 [D loss: 0.142127] [G loss: 0.232824]\n",
            "150\n",
            "74 [D loss: 0.217081] [G loss: 0.255698]\n",
            "200\n",
            "74 [D loss: 0.215972] [G loss: 0.217125]\n",
            "250\n",
            "74 [D loss: 0.230670] [G loss: 0.207253]\n",
            "300\n",
            "74 [D loss: 0.068938] [G loss: 0.198533]\n",
            "350\n",
            "74 [D loss: 0.294456] [G loss: 0.225526]\n",
            "400\n",
            "74 [D loss: 0.242841] [G loss: 0.188854]\n",
            "450\n",
            "74 [D loss: 0.254213] [G loss: 0.186247]\n",
            "500\n",
            "74 [D loss: 0.260081] [G loss: 0.178369]\n",
            "550\n",
            "74 [D loss: 0.221571] [G loss: 0.192843]\n",
            "600\n",
            "74 [D loss: 0.288103] [G loss: 0.206879]\n",
            "650\n",
            "74 [D loss: 0.204312] [G loss: 0.226834]\n",
            "700\n",
            "74 [D loss: 0.196278] [G loss: 0.204841]\n",
            "750\n",
            "74 [D loss: 0.109327] [G loss: 0.215479]\n",
            "800\n",
            "74 [D loss: 0.182466] [G loss: 0.207544]\n",
            "850\n",
            "74 [D loss: 0.211131] [G loss: 0.222211]\n",
            "900\n",
            "74 [D loss: 0.160769] [G loss: 0.213370]\n",
            "950\n",
            "74 [D loss: 0.177315] [G loss: 0.216633]\n",
            "1000\n",
            "74 [D loss: 0.282073] [G loss: 0.189492]\n",
            "1050\n",
            "74 [D loss: 0.251851] [G loss: 0.188025]\n",
            "1100\n",
            "74 [D loss: 0.161807] [G loss: 0.180018]\n",
            "1150\n",
            "74 [D loss: 0.179030] [G loss: 0.183701]\n",
            "0\n",
            "75 [D loss: 0.139139] [G loss: 0.166353]\n",
            "50\n",
            "75 [D loss: 0.212219] [G loss: 0.155575]\n",
            "100\n",
            "75 [D loss: 0.231774] [G loss: 0.140575]\n",
            "150\n",
            "75 [D loss: 0.095142] [G loss: 0.158393]\n",
            "200\n",
            "75 [D loss: 0.173300] [G loss: 0.146207]\n",
            "250\n",
            "75 [D loss: 0.149297] [G loss: 0.156904]\n",
            "300\n",
            "75 [D loss: 0.270384] [G loss: 0.148848]\n",
            "350\n",
            "75 [D loss: 0.222752] [G loss: 0.153827]\n",
            "400\n",
            "75 [D loss: 0.219546] [G loss: 0.140325]\n",
            "450\n",
            "75 [D loss: 0.149969] [G loss: 0.155736]\n",
            "500\n",
            "75 [D loss: 0.142117] [G loss: 0.142411]\n",
            "550\n",
            "75 [D loss: 0.251636] [G loss: 0.146290]\n",
            "600\n",
            "75 [D loss: 0.167258] [G loss: 0.126873]\n",
            "650\n",
            "75 [D loss: 0.176284] [G loss: 0.141629]\n",
            "700\n",
            "75 [D loss: 0.081742] [G loss: 0.107858]\n",
            "750\n",
            "75 [D loss: 0.263921] [G loss: 0.139501]\n",
            "800\n",
            "75 [D loss: 0.315092] [G loss: 0.128225]\n",
            "850\n",
            "75 [D loss: 0.201052] [G loss: 0.131074]\n",
            "900\n",
            "75 [D loss: 0.196191] [G loss: 0.126572]\n",
            "950\n",
            "75 [D loss: 0.166055] [G loss: 0.133034]\n",
            "1000\n",
            "75 [D loss: 0.210104] [G loss: 0.135625]\n",
            "1050\n",
            "75 [D loss: 0.228416] [G loss: 0.152232]\n",
            "1100\n",
            "75 [D loss: 0.160646] [G loss: 0.146445]\n",
            "1150\n",
            "75 [D loss: 0.288629] [G loss: 0.176491]\n",
            "0\n",
            "76 [D loss: 0.178791] [G loss: 0.147932]\n",
            "50\n",
            "76 [D loss: 0.204452] [G loss: 0.161071]\n",
            "100\n",
            "76 [D loss: 0.114726] [G loss: 0.153642]\n",
            "150\n",
            "76 [D loss: 0.181445] [G loss: 0.153000]\n",
            "200\n",
            "76 [D loss: 0.217013] [G loss: 0.136091]\n",
            "250\n",
            "76 [D loss: 0.145202] [G loss: 0.178060]\n",
            "300\n",
            "76 [D loss: 0.239185] [G loss: 0.168143]\n",
            "350\n",
            "76 [D loss: 0.204469] [G loss: 0.207569]\n",
            "400\n",
            "76 [D loss: 0.263222] [G loss: 0.191292]\n",
            "450\n",
            "76 [D loss: 0.231898] [G loss: 0.217266]\n",
            "500\n",
            "76 [D loss: 0.269241] [G loss: 0.182820]\n",
            "550\n",
            "76 [D loss: 0.171381] [G loss: 0.193845]\n",
            "600\n",
            "76 [D loss: 0.154572] [G loss: 0.172296]\n",
            "650\n",
            "76 [D loss: 0.160769] [G loss: 0.171849]\n",
            "700\n",
            "76 [D loss: 0.229233] [G loss: 0.158956]\n",
            "750\n",
            "76 [D loss: 0.194781] [G loss: 0.158362]\n",
            "800\n",
            "76 [D loss: 0.112844] [G loss: 0.140960]\n",
            "850\n",
            "76 [D loss: 0.234618] [G loss: 0.132830]\n",
            "900\n",
            "76 [D loss: 0.135698] [G loss: 0.125502]\n",
            "950\n",
            "76 [D loss: 0.204860] [G loss: 0.133885]\n",
            "1000\n",
            "76 [D loss: 0.193902] [G loss: 0.124662]\n",
            "1050\n",
            "76 [D loss: 0.278739] [G loss: 0.153355]\n",
            "1100\n",
            "76 [D loss: 0.213679] [G loss: 0.143871]\n",
            "1150\n",
            "76 [D loss: 0.134291] [G loss: 0.166693]\n",
            "0\n",
            "77 [D loss: 0.253042] [G loss: 0.150264]\n",
            "50\n",
            "77 [D loss: 0.271274] [G loss: 0.169140]\n",
            "100\n",
            "77 [D loss: 0.080490] [G loss: 0.154541]\n",
            "150\n",
            "77 [D loss: 0.300622] [G loss: 0.169258]\n",
            "200\n",
            "77 [D loss: 0.209198] [G loss: 0.167772]\n",
            "250\n",
            "77 [D loss: 0.324399] [G loss: 0.190536]\n",
            "300\n",
            "77 [D loss: 0.267597] [G loss: 0.174400]\n",
            "350\n",
            "77 [D loss: 0.086819] [G loss: 0.182515]\n",
            "400\n",
            "77 [D loss: 0.207160] [G loss: 0.166943]\n",
            "450\n",
            "77 [D loss: 0.160339] [G loss: 0.213911]\n",
            "500\n",
            "77 [D loss: 0.120495] [G loss: 0.199591]\n",
            "550\n",
            "77 [D loss: 0.231961] [G loss: 0.229892]\n",
            "600\n",
            "77 [D loss: 0.161116] [G loss: 0.203109]\n",
            "650\n",
            "77 [D loss: 0.181601] [G loss: 0.193121]\n",
            "700\n",
            "77 [D loss: 0.101127] [G loss: 0.186085]\n",
            "750\n",
            "77 [D loss: 0.068803] [G loss: 0.199948]\n",
            "800\n",
            "77 [D loss: 0.110074] [G loss: 0.183052]\n",
            "850\n",
            "77 [D loss: 0.169529] [G loss: 0.207281]\n",
            "900\n",
            "77 [D loss: 0.122000] [G loss: 0.170462]\n",
            "950\n",
            "77 [D loss: 0.272342] [G loss: 0.192639]\n",
            "1000\n",
            "77 [D loss: 0.151287] [G loss: 0.181011]\n",
            "1050\n",
            "77 [D loss: 0.314574] [G loss: 0.212291]\n",
            "1100\n",
            "77 [D loss: 0.208788] [G loss: 0.196783]\n",
            "1150\n",
            "77 [D loss: 0.232034] [G loss: 0.205894]\n",
            "0\n",
            "78 [D loss: 0.265600] [G loss: 0.179711]\n",
            "50\n",
            "78 [D loss: 0.246897] [G loss: 0.216402]\n",
            "100\n",
            "78 [D loss: 0.262337] [G loss: 0.179338]\n",
            "150\n",
            "78 [D loss: 0.215165] [G loss: 0.214555]\n",
            "200\n",
            "78 [D loss: 0.280897] [G loss: 0.198152]\n",
            "250\n",
            "78 [D loss: 0.230749] [G loss: 0.203195]\n",
            "300\n",
            "78 [D loss: 0.186463] [G loss: 0.205696]\n",
            "350\n",
            "78 [D loss: 0.166339] [G loss: 0.219429]\n",
            "400\n",
            "78 [D loss: 0.095007] [G loss: 0.195698]\n",
            "450\n",
            "78 [D loss: 0.199978] [G loss: 0.212287]\n",
            "500\n",
            "78 [D loss: 0.246313] [G loss: 0.186638]\n",
            "550\n",
            "78 [D loss: 0.221561] [G loss: 0.203985]\n",
            "600\n",
            "78 [D loss: 0.207798] [G loss: 0.190729]\n",
            "650\n",
            "78 [D loss: 0.201166] [G loss: 0.190625]\n",
            "700\n",
            "78 [D loss: 0.227959] [G loss: 0.183988]\n",
            "750\n",
            "78 [D loss: 0.166137] [G loss: 0.201445]\n",
            "800\n",
            "78 [D loss: 0.260216] [G loss: 0.188379]\n",
            "850\n",
            "78 [D loss: 0.208110] [G loss: 0.225235]\n",
            "900\n",
            "78 [D loss: 0.237096] [G loss: 0.204047]\n",
            "950\n",
            "78 [D loss: 0.159894] [G loss: 0.213294]\n",
            "1000\n",
            "78 [D loss: 0.209260] [G loss: 0.185595]\n",
            "1050\n",
            "78 [D loss: 0.255706] [G loss: 0.194171]\n",
            "1100\n",
            "78 [D loss: 0.322365] [G loss: 0.183293]\n",
            "1150\n",
            "78 [D loss: 0.117591] [G loss: 0.177317]\n",
            "0\n",
            "79 [D loss: 2.680478] [G loss: 0.186016]\n",
            "50\n",
            "79 [D loss: 0.140237] [G loss: 0.164045]\n",
            "100\n",
            "79 [D loss: 2.873290] [G loss: 0.173997]\n",
            "150\n",
            "79 [D loss: 0.219804] [G loss: 0.174596]\n",
            "200\n",
            "79 [D loss: 2.802546] [G loss: 0.177505]\n",
            "250\n",
            "79 [D loss: 0.184412] [G loss: 0.190877]\n",
            "300\n",
            "79 [D loss: 2.851911] [G loss: 0.199342]\n",
            "350\n",
            "79 [D loss: 0.218630] [G loss: 0.207392]\n",
            "400\n",
            "79 [D loss: 2.757025] [G loss: 0.235073]\n",
            "450\n",
            "79 [D loss: 0.191671] [G loss: 0.234021]\n",
            "500\n",
            "79 [D loss: 2.765137] [G loss: 0.236610]\n",
            "550\n",
            "79 [D loss: 0.273387] [G loss: 0.228771]\n",
            "600\n",
            "79 [D loss: 2.664074] [G loss: 0.214526]\n",
            "650\n",
            "79 [D loss: 0.296300] [G loss: 0.219510]\n",
            "700\n",
            "79 [D loss: 2.691594] [G loss: 0.238033]\n",
            "750\n",
            "79 [D loss: 0.196630] [G loss: 0.234980]\n",
            "800\n",
            "79 [D loss: 2.757107] [G loss: 0.226191]\n",
            "850\n",
            "79 [D loss: 0.093837] [G loss: 0.218067]\n",
            "900\n",
            "79 [D loss: 2.720740] [G loss: 0.218419]\n",
            "950\n",
            "79 [D loss: 0.311222] [G loss: 0.197788]\n",
            "1000\n",
            "79 [D loss: 2.712879] [G loss: 0.200813]\n",
            "1050\n",
            "79 [D loss: 0.218416] [G loss: 0.201159]\n",
            "1100\n",
            "79 [D loss: 2.743940] [G loss: 0.234306]\n",
            "1150\n",
            "79 [D loss: 0.220197] [G loss: 0.221356]\n",
            "0\n",
            "80 [D loss: 0.133940] [G loss: 0.206389]\n",
            "50\n",
            "80 [D loss: 0.296229] [G loss: 0.214463]\n",
            "100\n",
            "80 [D loss: 0.271792] [G loss: 0.200776]\n",
            "150\n",
            "80 [D loss: 0.302782] [G loss: 0.206943]\n",
            "200\n",
            "80 [D loss: 0.200987] [G loss: 0.190358]\n",
            "250\n",
            "80 [D loss: 0.289822] [G loss: 0.199058]\n",
            "300\n",
            "80 [D loss: 0.220019] [G loss: 0.196054]\n",
            "350\n",
            "80 [D loss: 0.268734] [G loss: 0.228058]\n",
            "400\n",
            "80 [D loss: 0.157186] [G loss: 0.215968]\n",
            "450\n",
            "80 [D loss: 0.178565] [G loss: 0.241922]\n",
            "500\n",
            "80 [D loss: 0.226854] [G loss: 0.240525]\n",
            "550\n",
            "80 [D loss: 0.213585] [G loss: 0.260775]\n",
            "600\n",
            "80 [D loss: 0.200689] [G loss: 0.266371]\n",
            "650\n",
            "80 [D loss: 0.216417] [G loss: 0.285574]\n",
            "700\n",
            "80 [D loss: 0.249037] [G loss: 0.277870]\n",
            "750\n",
            "80 [D loss: 0.134240] [G loss: 0.274562]\n",
            "800\n",
            "80 [D loss: 0.320854] [G loss: 0.260967]\n",
            "850\n",
            "80 [D loss: 0.167129] [G loss: 0.273829]\n",
            "900\n",
            "80 [D loss: 0.175200] [G loss: 0.264171]\n",
            "950\n",
            "80 [D loss: 0.181311] [G loss: 0.252838]\n",
            "1000\n",
            "80 [D loss: 0.158611] [G loss: 0.244957]\n",
            "1050\n",
            "80 [D loss: 0.232546] [G loss: 0.248932]\n",
            "1100\n",
            "80 [D loss: 0.253488] [G loss: 0.242520]\n",
            "1150\n",
            "80 [D loss: 0.245704] [G loss: 0.253795]\n",
            "0\n",
            "81 [D loss: 0.255001] [G loss: 0.252251]\n",
            "50\n",
            "81 [D loss: 0.238755] [G loss: 0.271500]\n",
            "100\n",
            "81 [D loss: 0.162804] [G loss: 0.234003]\n",
            "150\n",
            "81 [D loss: 0.233238] [G loss: 0.249560]\n",
            "200\n",
            "81 [D loss: 0.274543] [G loss: 0.232950]\n",
            "250\n",
            "81 [D loss: 0.218184] [G loss: 0.241514]\n",
            "300\n",
            "81 [D loss: 0.270076] [G loss: 0.245893]\n",
            "350\n",
            "81 [D loss: 0.246827] [G loss: 0.265033]\n",
            "400\n",
            "81 [D loss: 0.082312] [G loss: 0.256552]\n",
            "450\n",
            "81 [D loss: 0.193911] [G loss: 0.280623]\n",
            "500\n",
            "81 [D loss: 0.105473] [G loss: 0.260342]\n",
            "550\n",
            "81 [D loss: 0.161665] [G loss: 0.264331]\n",
            "600\n",
            "81 [D loss: 0.254635] [G loss: 0.263185]\n",
            "650\n",
            "81 [D loss: 0.179979] [G loss: 0.265979]\n",
            "700\n",
            "81 [D loss: 0.256709] [G loss: 0.262224]\n",
            "750\n",
            "81 [D loss: 0.156265] [G loss: 0.268105]\n",
            "800\n",
            "81 [D loss: 0.204963] [G loss: 0.259638]\n",
            "850\n",
            "81 [D loss: 0.206959] [G loss: 0.285965]\n",
            "900\n",
            "81 [D loss: 0.254835] [G loss: 0.278859]\n",
            "950\n",
            "81 [D loss: 0.098484] [G loss: 0.291937]\n",
            "1000\n",
            "81 [D loss: 0.170903] [G loss: 0.252855]\n",
            "1050\n",
            "81 [D loss: 0.217595] [G loss: 0.282530]\n",
            "1100\n",
            "81 [D loss: 0.117994] [G loss: 0.252041]\n",
            "1150\n",
            "81 [D loss: 0.116099] [G loss: 0.272371]\n",
            "0\n",
            "82 [D loss: 2.876874] [G loss: 0.292129]\n",
            "50\n",
            "82 [D loss: 0.193028] [G loss: 0.246942]\n",
            "100\n",
            "82 [D loss: 2.831771] [G loss: 0.262922]\n",
            "150\n",
            "82 [D loss: 0.171174] [G loss: 0.243771]\n",
            "200\n",
            "82 [D loss: 2.797503] [G loss: 0.256509]\n",
            "250\n",
            "82 [D loss: 0.241245] [G loss: 0.240891]\n",
            "300\n",
            "82 [D loss: 2.896539] [G loss: 0.255224]\n",
            "350\n",
            "82 [D loss: 0.128220] [G loss: 0.228743]\n",
            "400\n",
            "82 [D loss: 2.730561] [G loss: 0.233831]\n",
            "450\n",
            "82 [D loss: 0.196302] [G loss: 0.231550]\n",
            "500\n",
            "82 [D loss: 2.760040] [G loss: 0.235017]\n",
            "550\n",
            "82 [D loss: 0.213374] [G loss: 0.224667]\n",
            "600\n",
            "82 [D loss: 2.810192] [G loss: 0.238521]\n",
            "650\n",
            "82 [D loss: 0.146086] [G loss: 0.206064]\n",
            "700\n",
            "82 [D loss: 2.885363] [G loss: 0.241641]\n",
            "750\n",
            "82 [D loss: 0.195602] [G loss: 0.207081]\n",
            "800\n",
            "82 [D loss: 2.803049] [G loss: 0.211678]\n",
            "850\n",
            "82 [D loss: 0.184618] [G loss: 0.193214]\n",
            "900\n",
            "82 [D loss: 2.751826] [G loss: 0.203703]\n",
            "950\n",
            "82 [D loss: 0.166077] [G loss: 0.171027]\n",
            "1000\n",
            "82 [D loss: 2.775702] [G loss: 0.184494]\n",
            "1050\n",
            "82 [D loss: 0.092473] [G loss: 0.181753]\n",
            "1100\n",
            "82 [D loss: 2.884809] [G loss: 0.190896]\n",
            "1150\n",
            "82 [D loss: 0.118510] [G loss: 0.182921]\n",
            "0\n",
            "83 [D loss: 0.219050] [G loss: 0.182537]\n",
            "50\n",
            "83 [D loss: 0.188453] [G loss: 0.185701]\n",
            "100\n",
            "83 [D loss: 0.165544] [G loss: 0.171155]\n",
            "150\n",
            "83 [D loss: 0.234453] [G loss: 0.190927]\n",
            "200\n",
            "83 [D loss: 0.263895] [G loss: 0.172747]\n",
            "250\n",
            "83 [D loss: 0.247410] [G loss: 0.192331]\n",
            "300\n",
            "83 [D loss: 0.194344] [G loss: 0.179545]\n",
            "350\n",
            "83 [D loss: 0.250863] [G loss: 0.191889]\n",
            "400\n",
            "83 [D loss: 0.264951] [G loss: 0.173762]\n",
            "450\n",
            "83 [D loss: 0.254139] [G loss: 0.191498]\n",
            "500\n",
            "83 [D loss: 0.199926] [G loss: 0.181104]\n",
            "550\n",
            "83 [D loss: 0.102076] [G loss: 0.195493]\n",
            "600\n",
            "83 [D loss: 0.188226] [G loss: 0.186186]\n",
            "650\n",
            "83 [D loss: 0.198879] [G loss: 0.206085]\n",
            "700\n",
            "83 [D loss: 0.206808] [G loss: 0.208980]\n",
            "750\n",
            "83 [D loss: 0.336020] [G loss: 0.216938]\n",
            "800\n",
            "83 [D loss: 0.107162] [G loss: 0.215596]\n",
            "850\n",
            "83 [D loss: 0.174200] [G loss: 0.228454]\n",
            "900\n",
            "83 [D loss: 0.077853] [G loss: 0.214416]\n",
            "950\n",
            "83 [D loss: 0.247912] [G loss: 0.230045]\n",
            "1000\n",
            "83 [D loss: 0.195743] [G loss: 0.224957]\n",
            "1050\n",
            "83 [D loss: 0.271355] [G loss: 0.232019]\n",
            "1100\n",
            "83 [D loss: 0.210962] [G loss: 0.217382]\n",
            "1150\n",
            "83 [D loss: 0.197329] [G loss: 0.228409]\n",
            "0\n",
            "84 [D loss: 0.112138] [G loss: 0.208722]\n",
            "50\n",
            "84 [D loss: 0.289267] [G loss: 0.198940]\n",
            "100\n",
            "84 [D loss: 0.270348] [G loss: 0.182298]\n",
            "150\n",
            "84 [D loss: 0.204585] [G loss: 0.182144]\n",
            "200\n",
            "84 [D loss: 0.184972] [G loss: 0.175009]\n",
            "250\n",
            "84 [D loss: 0.152763] [G loss: 0.183111]\n",
            "300\n",
            "84 [D loss: 0.101303] [G loss: 0.155627]\n",
            "350\n",
            "84 [D loss: 0.162212] [G loss: 0.181538]\n",
            "400\n",
            "84 [D loss: 0.170478] [G loss: 0.179517]\n",
            "450\n",
            "84 [D loss: 0.261190] [G loss: 0.213754]\n",
            "500\n",
            "84 [D loss: 0.328955] [G loss: 0.201313]\n",
            "550\n",
            "84 [D loss: 0.224572] [G loss: 0.220941]\n",
            "600\n",
            "84 [D loss: 0.214090] [G loss: 0.206132]\n",
            "650\n",
            "84 [D loss: 0.176406] [G loss: 0.222780]\n",
            "700\n",
            "84 [D loss: 0.063445] [G loss: 0.204890]\n",
            "750\n",
            "84 [D loss: 0.135449] [G loss: 0.210364]\n",
            "800\n",
            "84 [D loss: 0.293071] [G loss: 0.204584]\n",
            "850\n",
            "84 [D loss: 0.224801] [G loss: 0.224753]\n",
            "900\n",
            "84 [D loss: 0.206484] [G loss: 0.192246]\n",
            "950\n",
            "84 [D loss: 0.166358] [G loss: 0.216947]\n",
            "1000\n",
            "84 [D loss: 0.198342] [G loss: 0.195877]\n",
            "1050\n",
            "84 [D loss: 0.218488] [G loss: 0.201365]\n",
            "1100\n",
            "84 [D loss: 0.278246] [G loss: 0.198798]\n",
            "1150\n",
            "84 [D loss: 0.128736] [G loss: 0.237868]\n",
            "0\n",
            "85 [D loss: 0.098509] [G loss: 0.209326]\n",
            "50\n",
            "85 [D loss: 0.152722] [G loss: 0.215072]\n",
            "100\n",
            "85 [D loss: 0.103831] [G loss: 0.209048]\n",
            "150\n",
            "85 [D loss: 0.128837] [G loss: 0.220696]\n",
            "200\n",
            "85 [D loss: 0.303469] [G loss: 0.218022]\n",
            "250\n",
            "85 [D loss: 0.069503] [G loss: 0.206160]\n",
            "300\n",
            "85 [D loss: 0.234661] [G loss: 0.185336]\n",
            "350\n",
            "85 [D loss: 0.307071] [G loss: 0.179550]\n",
            "400\n",
            "85 [D loss: 0.178261] [G loss: 0.167084]\n",
            "450\n",
            "85 [D loss: 0.101134] [G loss: 0.194275]\n",
            "500\n",
            "85 [D loss: 0.201155] [G loss: 0.162037]\n",
            "550\n",
            "85 [D loss: 0.099772] [G loss: 0.177967]\n",
            "600\n",
            "85 [D loss: 0.266355] [G loss: 0.165898]\n",
            "650\n",
            "85 [D loss: 0.269567] [G loss: 0.173704]\n",
            "700\n",
            "85 [D loss: 0.151104] [G loss: 0.166802]\n",
            "750\n",
            "85 [D loss: 0.217637] [G loss: 0.199438]\n",
            "800\n",
            "85 [D loss: 0.217178] [G loss: 0.192340]\n",
            "850\n",
            "85 [D loss: 0.211880] [G loss: 0.225867]\n",
            "900\n",
            "85 [D loss: 0.282848] [G loss: 0.210829]\n",
            "950\n",
            "85 [D loss: 0.226274] [G loss: 0.243790]\n",
            "1000\n",
            "85 [D loss: 0.218179] [G loss: 0.201548]\n",
            "1050\n",
            "85 [D loss: 0.164414] [G loss: 0.229412]\n",
            "1100\n",
            "85 [D loss: 0.212415] [G loss: 0.206630]\n",
            "1150\n",
            "85 [D loss: 0.126639] [G loss: 0.242273]\n",
            "0\n",
            "86 [D loss: 0.067992] [G loss: 0.227856]\n",
            "50\n",
            "86 [D loss: 0.286679] [G loss: 0.235221]\n",
            "100\n",
            "86 [D loss: 0.251152] [G loss: 0.215958]\n",
            "150\n",
            "86 [D loss: 0.248303] [G loss: 0.224358]\n",
            "200\n",
            "86 [D loss: 0.172061] [G loss: 0.193608]\n",
            "250\n",
            "86 [D loss: 0.162437] [G loss: 0.195381]\n",
            "300\n",
            "86 [D loss: 0.247566] [G loss: 0.193283]\n",
            "350\n",
            "86 [D loss: 0.279377] [G loss: 0.198358]\n",
            "400\n",
            "86 [D loss: 0.241814] [G loss: 0.197329]\n",
            "450\n",
            "86 [D loss: 0.165694] [G loss: 0.208875]\n",
            "500\n",
            "86 [D loss: 0.159863] [G loss: 0.213777]\n",
            "550\n",
            "86 [D loss: 0.228931] [G loss: 0.243677]\n",
            "600\n",
            "86 [D loss: 0.149791] [G loss: 0.215680]\n",
            "650\n",
            "86 [D loss: 0.297571] [G loss: 0.230913]\n",
            "700\n",
            "86 [D loss: 0.297309] [G loss: 0.207478]\n",
            "750\n",
            "86 [D loss: 0.268846] [G loss: 0.200548]\n",
            "800\n",
            "86 [D loss: 0.192650] [G loss: 0.197473]\n",
            "850\n",
            "86 [D loss: 0.206725] [G loss: 0.200362]\n",
            "900\n",
            "86 [D loss: 0.163882] [G loss: 0.191712]\n",
            "950\n",
            "86 [D loss: 0.298168] [G loss: 0.189365]\n",
            "1000\n",
            "86 [D loss: 0.265466] [G loss: 0.175626]\n",
            "1050\n",
            "86 [D loss: 0.309175] [G loss: 0.186612]\n",
            "1100\n",
            "86 [D loss: 0.081106] [G loss: 0.166481]\n",
            "1150\n",
            "86 [D loss: 0.197700] [G loss: 0.175450]\n",
            "0\n",
            "87 [D loss: 0.122536] [G loss: 0.157271]\n",
            "50\n",
            "87 [D loss: 0.153884] [G loss: 0.176685]\n",
            "100\n",
            "87 [D loss: 0.151216] [G loss: 0.166743]\n",
            "150\n",
            "87 [D loss: 0.171935] [G loss: 0.176342]\n",
            "200\n",
            "87 [D loss: 0.212998] [G loss: 0.169548]\n",
            "250\n",
            "87 [D loss: 0.152223] [G loss: 0.164578]\n",
            "300\n",
            "87 [D loss: 0.084371] [G loss: 0.163782]\n",
            "350\n",
            "87 [D loss: 0.224624] [G loss: 0.176269]\n",
            "400\n",
            "87 [D loss: 0.074920] [G loss: 0.167672]\n",
            "450\n",
            "87 [D loss: 0.276945] [G loss: 0.189679]\n",
            "500\n",
            "87 [D loss: 0.208113] [G loss: 0.171062]\n",
            "550\n",
            "87 [D loss: 0.328783] [G loss: 0.175433]\n",
            "600\n",
            "87 [D loss: 0.213166] [G loss: 0.171410]\n",
            "650\n",
            "87 [D loss: 0.318851] [G loss: 0.179670]\n",
            "700\n",
            "87 [D loss: 0.284606] [G loss: 0.182232]\n",
            "750\n",
            "87 [D loss: 0.199973] [G loss: 0.200999]\n",
            "800\n",
            "87 [D loss: 0.198434] [G loss: 0.196165]\n",
            "850\n",
            "87 [D loss: 0.154267] [G loss: 0.197013]\n",
            "900\n",
            "87 [D loss: 0.131338] [G loss: 0.180155]\n",
            "950\n",
            "87 [D loss: 0.089273] [G loss: 0.188437]\n",
            "1000\n",
            "87 [D loss: 0.088613] [G loss: 0.167696]\n",
            "1050\n",
            "87 [D loss: 0.228215] [G loss: 0.189797]\n",
            "1100\n",
            "87 [D loss: 0.333125] [G loss: 0.170394]\n",
            "1150\n",
            "87 [D loss: 0.307410] [G loss: 0.174149]\n",
            "0\n",
            "88 [D loss: 0.182868] [G loss: 0.163625]\n",
            "50\n",
            "88 [D loss: 0.188846] [G loss: 0.167974]\n",
            "100\n",
            "88 [D loss: 0.212489] [G loss: 0.154573]\n",
            "150\n",
            "88 [D loss: 0.153363] [G loss: 0.152424]\n",
            "200\n",
            "88 [D loss: 0.112949] [G loss: 0.151245]\n",
            "250\n",
            "88 [D loss: 0.099362] [G loss: 0.142101]\n",
            "300\n",
            "88 [D loss: 0.196378] [G loss: 0.139255]\n",
            "350\n",
            "88 [D loss: 0.192507] [G loss: 0.151856]\n",
            "400\n",
            "88 [D loss: 0.196900] [G loss: 0.146525]\n",
            "450\n",
            "88 [D loss: 0.106557] [G loss: 0.149199]\n",
            "500\n",
            "88 [D loss: 0.183649] [G loss: 0.127857]\n",
            "550\n",
            "88 [D loss: 0.216425] [G loss: 0.146040]\n",
            "600\n",
            "88 [D loss: 0.122605] [G loss: 0.135071]\n",
            "650\n",
            "88 [D loss: 0.181194] [G loss: 0.159242]\n",
            "700\n",
            "88 [D loss: 0.178135] [G loss: 0.138095]\n",
            "750\n",
            "88 [D loss: 0.221297] [G loss: 0.148230]\n",
            "800\n",
            "88 [D loss: 0.153661] [G loss: 0.135723]\n",
            "850\n",
            "88 [D loss: 0.227020] [G loss: 0.166728]\n",
            "900\n",
            "88 [D loss: 0.250268] [G loss: 0.151118]\n",
            "950\n",
            "88 [D loss: 0.117264] [G loss: 0.157059]\n",
            "1000\n",
            "88 [D loss: 0.236709] [G loss: 0.154813]\n",
            "1050\n",
            "88 [D loss: 0.240935] [G loss: 0.163527]\n",
            "1100\n",
            "88 [D loss: 0.122537] [G loss: 0.154906]\n",
            "1150\n",
            "88 [D loss: 0.162847] [G loss: 0.180740]\n",
            "0\n",
            "89 [D loss: 0.206958] [G loss: 0.164451]\n",
            "50\n",
            "89 [D loss: 0.163088] [G loss: 0.180092]\n",
            "100\n",
            "89 [D loss: 0.264529] [G loss: 0.154940]\n",
            "150\n",
            "89 [D loss: 0.132765] [G loss: 0.148907]\n",
            "200\n",
            "89 [D loss: 0.176975] [G loss: 0.143673]\n",
            "250\n",
            "89 [D loss: 0.196761] [G loss: 0.172380]\n",
            "300\n",
            "89 [D loss: 0.159803] [G loss: 0.157350]\n",
            "350\n",
            "89 [D loss: 0.235796] [G loss: 0.152123]\n",
            "400\n",
            "89 [D loss: 0.103153] [G loss: 0.134466]\n",
            "450\n",
            "89 [D loss: 0.237860] [G loss: 0.150867]\n",
            "500\n",
            "89 [D loss: 0.277105] [G loss: 0.139978]\n",
            "550\n",
            "89 [D loss: 0.238654] [G loss: 0.169767]\n",
            "600\n",
            "89 [D loss: 0.163065] [G loss: 0.164029]\n",
            "650\n",
            "89 [D loss: 0.192166] [G loss: 0.179901]\n",
            "700\n",
            "89 [D loss: 0.233737] [G loss: 0.169457]\n",
            "750\n",
            "89 [D loss: 0.290052] [G loss: 0.184323]\n",
            "800\n",
            "89 [D loss: 0.194845] [G loss: 0.183939]\n",
            "850\n",
            "89 [D loss: 0.301692] [G loss: 0.193612]\n",
            "900\n",
            "89 [D loss: 0.165385] [G loss: 0.179461]\n",
            "950\n",
            "89 [D loss: 0.199761] [G loss: 0.173640]\n",
            "1000\n",
            "89 [D loss: 0.115523] [G loss: 0.156371]\n",
            "1050\n",
            "89 [D loss: 0.193528] [G loss: 0.167444]\n",
            "1100\n",
            "89 [D loss: 0.314690] [G loss: 0.157857]\n",
            "1150\n",
            "89 [D loss: 0.179567] [G loss: 0.161903]\n",
            "0\n",
            "90 [D loss: 0.120850] [G loss: 0.136506]\n",
            "50\n",
            "90 [D loss: 0.201479] [G loss: 0.168659]\n",
            "100\n",
            "90 [D loss: 0.149813] [G loss: 0.155785]\n",
            "150\n",
            "90 [D loss: 0.196771] [G loss: 0.176310]\n",
            "200\n",
            "90 [D loss: 0.280941] [G loss: 0.168298]\n",
            "250\n",
            "90 [D loss: 0.161725] [G loss: 0.189202]\n",
            "300\n",
            "90 [D loss: 0.145774] [G loss: 0.174154]\n",
            "350\n",
            "90 [D loss: 0.148206] [G loss: 0.163687]\n",
            "400\n",
            "90 [D loss: 0.278105] [G loss: 0.151661]\n",
            "450\n",
            "90 [D loss: 0.223445] [G loss: 0.193399]\n",
            "500\n",
            "90 [D loss: 0.302823] [G loss: 0.183423]\n",
            "550\n",
            "90 [D loss: 0.202479] [G loss: 0.227584]\n",
            "600\n",
            "90 [D loss: 0.252019] [G loss: 0.214871]\n",
            "650\n",
            "90 [D loss: 0.260791] [G loss: 0.213022]\n",
            "700\n",
            "90 [D loss: 0.231377] [G loss: 0.216855]\n",
            "750\n",
            "90 [D loss: 0.172611] [G loss: 0.224737]\n",
            "800\n",
            "90 [D loss: 0.250082] [G loss: 0.212375]\n",
            "850\n",
            "90 [D loss: 0.111528] [G loss: 0.262091]\n",
            "900\n",
            "90 [D loss: 0.248849] [G loss: 0.251422]\n",
            "950\n",
            "90 [D loss: 0.290118] [G loss: 0.262560]\n",
            "1000\n",
            "90 [D loss: 0.255573] [G loss: 0.215818]\n",
            "1050\n",
            "90 [D loss: 0.193090] [G loss: 0.235132]\n",
            "1100\n",
            "90 [D loss: 0.181012] [G loss: 0.209099]\n",
            "1150\n",
            "90 [D loss: 0.230727] [G loss: 0.222805]\n",
            "0\n",
            "91 [D loss: 0.222236] [G loss: 0.205674]\n",
            "50\n",
            "91 [D loss: 0.197888] [G loss: 0.231472]\n",
            "100\n",
            "91 [D loss: 0.227938] [G loss: 0.196025]\n",
            "150\n",
            "91 [D loss: 0.257239] [G loss: 0.221446]\n",
            "200\n",
            "91 [D loss: 0.250242] [G loss: 0.203081]\n",
            "250\n",
            "91 [D loss: 0.190904] [G loss: 0.191383]\n",
            "300\n",
            "91 [D loss: 0.315967] [G loss: 0.170109]\n",
            "350\n",
            "91 [D loss: 0.066999] [G loss: 0.182464]\n",
            "400\n",
            "91 [D loss: 0.268158] [G loss: 0.159777]\n",
            "450\n",
            "91 [D loss: 0.173269] [G loss: 0.170448]\n",
            "500\n",
            "91 [D loss: 0.174254] [G loss: 0.153389]\n",
            "550\n",
            "91 [D loss: 0.126829] [G loss: 0.187194]\n",
            "600\n",
            "91 [D loss: 0.228518] [G loss: 0.168157]\n",
            "650\n",
            "91 [D loss: 0.218585] [G loss: 0.187449]\n",
            "700\n",
            "91 [D loss: 0.174803] [G loss: 0.174510]\n",
            "750\n",
            "91 [D loss: 0.172513] [G loss: 0.167424]\n",
            "800\n",
            "91 [D loss: 0.165346] [G loss: 0.155568]\n",
            "850\n",
            "91 [D loss: 0.220371] [G loss: 0.172378]\n",
            "900\n",
            "91 [D loss: 0.251743] [G loss: 0.169361]\n",
            "950\n",
            "91 [D loss: 0.240252] [G loss: 0.167444]\n",
            "1000\n",
            "91 [D loss: 0.088246] [G loss: 0.150421]\n",
            "1050\n",
            "91 [D loss: 0.239589] [G loss: 0.179632]\n",
            "1100\n",
            "91 [D loss: 0.221199] [G loss: 0.174510]\n",
            "1150\n",
            "91 [D loss: 0.210512] [G loss: 0.185788]\n",
            "0\n",
            "92 [D loss: 2.742854] [G loss: 0.171366]\n",
            "50\n",
            "92 [D loss: 0.149071] [G loss: 0.169531]\n",
            "100\n",
            "92 [D loss: 2.664838] [G loss: 0.188615]\n",
            "150\n",
            "92 [D loss: 0.232663] [G loss: 0.171312]\n",
            "200\n",
            "92 [D loss: 2.776751] [G loss: 0.167289]\n",
            "250\n",
            "92 [D loss: 0.225599] [G loss: 0.155658]\n",
            "300\n",
            "92 [D loss: 2.705982] [G loss: 0.161810]\n",
            "350\n",
            "92 [D loss: 0.195140] [G loss: 0.146712]\n",
            "400\n",
            "92 [D loss: 2.513255] [G loss: 0.170175]\n",
            "450\n",
            "92 [D loss: 0.245042] [G loss: 0.165224]\n",
            "500\n",
            "92 [D loss: 2.812618] [G loss: 0.179596]\n",
            "550\n",
            "92 [D loss: 0.323353] [G loss: 0.179041]\n",
            "600\n",
            "92 [D loss: 2.702328] [G loss: 0.173874]\n",
            "650\n",
            "92 [D loss: 0.163520] [G loss: 0.152467]\n",
            "700\n",
            "92 [D loss: 2.834101] [G loss: 0.163003]\n",
            "750\n",
            "92 [D loss: 0.191030] [G loss: 0.153572]\n",
            "800\n",
            "92 [D loss: 2.663854] [G loss: 0.183025]\n",
            "850\n",
            "92 [D loss: 0.183300] [G loss: 0.156612]\n",
            "900\n",
            "92 [D loss: 2.659211] [G loss: 0.160076]\n",
            "950\n",
            "92 [D loss: 0.206256] [G loss: 0.154033]\n",
            "1000\n",
            "92 [D loss: 2.693578] [G loss: 0.173871]\n",
            "1050\n",
            "92 [D loss: 0.244434] [G loss: 0.143573]\n",
            "1100\n",
            "92 [D loss: 2.785422] [G loss: 0.156245]\n",
            "1150\n",
            "92 [D loss: 0.101518] [G loss: 0.143686]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grnnan4Jbepm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_parameter(input, title)\n",
        "  plt.plot(input)\n",
        "  plt.title(title)\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('loss')\n",
        "  plt.show()\n",
        "plt.plot(d_loss_history)\n",
        "plt.plot(g_loss_history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmEuZe4cbepq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}