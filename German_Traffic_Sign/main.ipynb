{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, BatchNormalization, Dropout, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoadSignClassifier:\n",
    "    def createCNN(width, height, depth, classes):\n",
    "        model = Sequential()\n",
    "        inputShape = (height, width, depth)\n",
    "        model.add(Conv2D(filters=8, kernel_size=(5, 5), input_shape=inputShape, activation=\"relu\"))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        \n",
    "        model.add(Conv2D(filters=16, kernel_size=(3, 3), activation=\"relu\"))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Conv2D(filters=16, kernel_size=(3, 3), activation=\"relu\"))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    " \n",
    "        model.add(Conv2D(filters=32, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Conv2D(filters=32, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        model.add(Flatten())\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(512, activation=\"relu\"))\n",
    "        model.add(Dense(classes, activation=\"softmax\"))\n",
    "        return model\n",
    "    \n",
    "    def createCustomActivationCNN(width, height, depth, classes, activation):\n",
    "        model = Sequential()\n",
    "        inputShape = (height, width, depth)\n",
    "        model.add(Conv2D(filters=8, kernel_size=(5, 5), input_shape=inputShape, activation=activation))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        \n",
    "        model.add(Conv2D(filters=16, kernel_size=(3, 3), activation=activation))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Conv2D(filters=16, kernel_size=(3, 3), activation=activation))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    " \n",
    "        model.add(Conv2D(filters=32, kernel_size=(3, 3), padding=\"same\", activation=activation))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Conv2D(filters=32, kernel_size=(3, 3), padding=\"same\", activation=activation))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        model.add(Flatten())\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(512, activation=activation))\n",
    "        model.add(Dense(classes, activation=\"softmax\"))\n",
    "        return model\n",
    "    \n",
    "    def createCNN_without_dropout(width, height, depth, classes):\n",
    "        model = Sequential()\n",
    "        inputShape = (height, width, depth)\n",
    "        model.add(Conv2D(filters=8, kernel_size=(5, 5), input_shape=inputShape, activation=\"relu\"))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        \n",
    "        model.add(Conv2D(filters=16, kernel_size=(3, 3), activation=\"relu\"))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Conv2D(filters=16, kernel_size=(3, 3), activation=\"relu\"))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    " \n",
    "        model.add(Conv2D(filters=32, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Conv2D(filters=32, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(512, activation=\"relu\"))\n",
    "        model.add(Dense(classes, activation=\"softmax\"))\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dataset, csv):\n",
    "    images = []\n",
    "    classes = []\n",
    "    rows = pd.read_csv(dataset + csv)\n",
    "    rows = rows.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    for i, row in rows.iterrows():\n",
    "        img_class = row[\"ClassId\"]\n",
    "        img_path = row[\"Path\"]\n",
    "        image = os.path.join(dataset, img_path)\n",
    "        image = cv2.imread(image)\n",
    "        image_rs = cv2.resize(image, (30, 30), 3)\n",
    "\n",
    "        R, G, B = cv2.split(image_rs)\n",
    "\n",
    "        img_r = cv2.equalizeHist(R)\n",
    "        img_g = cv2.equalizeHist(G)\n",
    "        img_b = cv2.equalizeHist(B)\n",
    "\n",
    "        new_image = cv2.merge((img_r, img_g, img_b))\n",
    "\n",
    "        if i % 500 == 0:\n",
    "            print(f\"loaded: {i}\")\n",
    "        images.append(new_image)\n",
    "        classes.append(img_class)\n",
    "\n",
    "    X = np.array(images)\n",
    "    y = np.array(classes)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded: 0\n",
      "loaded: 500\n",
      "loaded: 1000\n",
      "loaded: 1500\n",
      "loaded: 2000\n",
      "loaded: 2500\n",
      "loaded: 3000\n",
      "loaded: 3500\n",
      "loaded: 4000\n",
      "loaded: 4500\n",
      "loaded: 5000\n",
      "loaded: 5500\n",
      "loaded: 6000\n",
      "loaded: 6500\n",
      "loaded: 7000\n",
      "loaded: 7500\n",
      "loaded: 8000\n",
      "loaded: 8500\n",
      "loaded: 9000\n",
      "loaded: 9500\n",
      "loaded: 10000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-5a00c7704734>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mr\"/home/sspc/Desktop/gtsrb-german-traffic-sign\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mr\"/home/sspc/Desktop/gtsrb-german-traffic-sign\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_Y\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"/Train.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtest_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_Y\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"/Test.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-fb6ec59a93c0>\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(dataset, csv)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mimg_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Path\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mimage_rs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_data = r\"/home/sspc/Desktop/gtsrb-german-traffic-sign\"\n",
    "test_data = r\"/home/sspc/Desktop/gtsrb-german-traffic-sign\"\n",
    "(train_X, train_Y) = load_data(train_data, \"/Train.csv\")\n",
    "(test_X, test_Y) = load_data(test_data, \"/Test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"UPDATE: Normalizing data\")\n",
    "trainX = train_X.astype(\"float64\") / 255.0\n",
    "testX = test_X.astype(\"float64\") / 255.0\n",
    "print(\"UPDATE: One-Hot Encoding data\")\n",
    "num_labels = len(np.unique(train_Y))\n",
    "trainY = to_categorical(train_Y)\n",
    "testY = to_categorical(test_Y)\n",
    "\n",
    "class_totals = trainY.sum(axis=0)\n",
    "class_weight = class_totals.max() / class_totals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_aug = ImageDataGenerator(\n",
    "rotation_range=0,\n",
    "zoom_range=0.15,\n",
    "width_shift_range=0.2,\n",
    "height_shift_range=0.2,\n",
    "shear_range=0.3,\n",
    "horizontal_flip=False,\n",
    "vertical_flip=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "epochs = 15\n",
    "batch_size = 64\n",
    "# model = RoadSignClassifier.createCNN(width=30, height=30, depth=3, classes=43)\n",
    "model_relu = RoadSignClassifier.createCustomActivationCNN(width=30, height=30, depth=3, classes=43,\n",
    "                                                          activation=\"relu\")\n",
    "model_tanh = RoadSignClassifier.createCustomActivationCNN(width=30, height=30, depth=3, classes=43,\n",
    "                                                          activation=\"tanh\")\n",
    "model_sigmoid = RoadSignClassifier.createCustomActivationCNN(width=30, height=30, depth=3, classes=43,\n",
    "                                                             activation=\"sigmoid\")\n",
    "optimizer = Adam(lr=learning_rate, decay=learning_rate / (epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_relu.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "model_relu_fit = model_relu.fit(\n",
    "                x=trainX,\n",
    "                y=trainY,\n",
    "                batch_size=batch_size, \n",
    "                epochs=epochs,\n",
    "                validation_split=0.2,\n",
    "                class_weight=class_weight,\n",
    "                verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tanh.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "model_tanh_fit = model_tanh.fit(\n",
    "                            x=trainX,\n",
    "                            y=trainY,\n",
    "                            batch_size=batch_size, \n",
    "                            epochs=epochs,\n",
    "                            validation_split=0.2,\n",
    "                            class_weight=class_weight,\n",
    "                            verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sigmoid.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "model_sigmoid_fit = model_sigmoid.fit(\n",
    "                                    x=trainX,\n",
    "                                    y=trainY,\n",
    "                                    batch_size=batch_size, \n",
    "                                    epochs=epochs,\n",
    "                                    validation_split=0.2,\n",
    "                                    class_weight=class_weight,\n",
    "                                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_relu = model_relu_fit.history\n",
    "history_tanh = model_sigmoid_fit.history\n",
    "history_sigmoid = model_tanh_fit.history\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Without Augmentation\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('val_accuracy')\n",
    "plt.plot(history_relu['val_accuracy'])\n",
    "plt.plot(history_tanh['val_accuracy'])\n",
    "plt.plot(history_sigmoid['val_accuracy'])\n",
    "plt.legend(['relu', 'tanh', 'sigmoid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss_relu, test_acc_relu = model_relu.evaluate(testX, testY, verbose=1)\n",
    "test_loss_tanh, test_acc_tanh = model_tanh.evaluate(testX, testY, verbose=1)\n",
    "test_loss_sigmoid, test_acc_sigmoid = model_sigmoid.evaluate(testX, testY, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "epochs = 30\n",
    "batch_size = 64\n",
    "model_Adam = RoadSignClassifier.createCNN(width=30, height=30, depth=3, classes=43)\n",
    "model_SGD = RoadSignClassifier.createCNN(width=30, height=30, depth=3, classes=43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(lr=learning_rate, decay=learning_rate / (epochs))\n",
    "model_Adam.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "model_Adam_fit = model_Adam.fit(\n",
    "    x=trainX,\n",
    "    y=trainY,\n",
    "    batch_size=batch_size, \n",
    "    epochs=epochs,\n",
    "    validation_split=0.2,\n",
    "    class_weight=class_weight,\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = SGD(lr=learning_rate, decay=learning_rate / (epochs))\n",
    "model_SGD.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "model_SGD_fit = model_SGD.fit(\n",
    "    x=trainX,\n",
    "    y=trainY,\n",
    "    batch_size=batch_size, \n",
    "    epochs=epochs,\n",
    "    validation_split=0.2,\n",
    "    class_weight=class_weight,\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_adam = model_Adam_fit.history\n",
    "history_sgd = model_SGD_fit.history\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Without Augmentation\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('val_accuracy')\n",
    "plt.plot(history_adam['val_accuracy'])\n",
    "plt.plot(history_sgd['val_accuracy'])\n",
    "plt.legend(['adam', 'sgd'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss_adam, test_acc_adam = model_Adam.evaluate(testX, testY, verbose=1)\n",
    "test_loss_sgd, test_acc_sgd = model_SGD.evaluate(testX, testY, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "epochs = 100\n",
    "batch_size = 64\n",
    "model_with_drop_out = RoadSignClassifier.createCNN(width=30, height=30, depth=3, classes=43)\n",
    "model_without_drop_out = RoadSignClassifier.createCNN_without_dropout(width=30, height=30, depth=3, classes=43)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_with_drop_out.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "model_with_drop_out_fit = model_with_drop_out.fit(\n",
    "    x=trainX,\n",
    "    y=trainY,\n",
    "    batch_size=batch_size, \n",
    "    epochs=epochs,\n",
    "    validation_split=0.2,\n",
    "    class_weight=class_weight,\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_without_drop_out.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "model_without_drop_out_fit = model_without_drop_out.fit(\n",
    "    x=trainX,\n",
    "    y=trainY,\n",
    "    batch_size=batch_size, \n",
    "    epochs=epochs,\n",
    "    validation_split=0.2,\n",
    "    class_weight=class_weight,\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_with_drop_out = model_with_drop_out_fit.history\n",
    "history_without_drop_out = model_without_drop_out_fit.history\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Drop out Efecct\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('accuracy')\n",
    "plt.plot(history_with_drop_out['accuracy'])\n",
    "plt.plot(history_without_drop_out['accuracy'])\n",
    "plt.legend(['withDropOut', 'WithOutDropOut'])\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Drop out Efecct\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('val_accuracy')\n",
    "plt.plot(history_with_drop_out['val_accuracy'])\n",
    "plt.plot(history_without_drop_out['val_accuracy'])\n",
    "plt.legend(['withDropOut', 'WithOutDropOut'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss_with_drop_out, test_acc_with_drop_out = model_with_drop_out.evaluate(testX, testY, verbose=1)\n",
    "test_loss_wihtout_drop_out, test_acc_wihtout_drop_out = model_without_drop_out.evaluate(testX, testY, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "model_fit = model.fit(\n",
    "    x=trainX,\n",
    "    y=trainY,\n",
    "    batch_size=batch_size, \n",
    "    epochs=epochs,\n",
    "    validation_split=0.2,\n",
    "    class_weight=class_weight,\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(testX, testY, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model_fit.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Without Augmentation\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(history['loss'])\n",
    "plt.plot(history['val_loss'])\n",
    "plt.legend(['loss', 'val_loss'])\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Without Augmentation\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('accuracy')\n",
    "plt.plot(history['accuracy'])\n",
    "plt.plot(history['val_accuracy'])\n",
    "plt.legend(['acc', 'val_acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm,\n",
    "                          target_names = ['1', '2', '3', '4'],\n",
    "                          title = 'Confusion matrix',\n",
    "                          cmap = None,\n",
    "                          normalize = False):\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import itertools\n",
    "\n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "\n",
    "    plt.figure(figsize = (14, 12))\n",
    "    plt.imshow(cm, interpolation = 'nearest', cmap = cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation = 0)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis = 1)[:, np.newaxis]\n",
    "\n",
    "\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
    "                     horizontalalignment = \"center\",\n",
    "                     color = \"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment = \"center\",\n",
    "                     color = \"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "test_prediction = model.predict_classes(testX)\n",
    "cm = confusion_matrix(y_true=test_Y, y_pred=test_prediction)\n",
    "print(\"Matrix = \")\n",
    "print(cm)\n",
    "\n",
    "names = []\n",
    "for i in range(43):\n",
    "    names.append(i)\n",
    "plot_confusion_matrix(cm           = cm, \n",
    "                      normalize    = False,\n",
    "                      target_names = names,\n",
    "                      title        = \"Confusion Matrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "epochs = 100\n",
    "batch_size = 64\n",
    "optimizer = Adam(lr=learning_rate, decay=learning_rate / (epochs))\n",
    "model_without_augment = RoadSignClassifier.createCNN(width=30, height=30, depth=3, classes=43)\n",
    "model_with_augment = RoadSignClassifier.createCNN(width=30, height=30, depth=3, classes=43)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_without_augment.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "model_without_augment_fit = model_without_augment.fit(\n",
    "    x=trainX,\n",
    "    y=trainY,\n",
    "    batch_size=batch_size, \n",
    "    epochs=epochs,\n",
    "    validation_split=0.2,\n",
    "    class_weight=class_weight,\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_with_augment.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "model_with_augment_fit = model_without_augment.fit(\n",
    "    data_aug.flow(trainX, trainY, batch_size=batch_size), \n",
    "    epochs=epochs,\n",
    "    validation_data=(testX, testY),\n",
    "    class_weight=class_weight,\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss_without_augment, test_acc_without_augment = model_without_augment.evaluate(testX, testY, verbose=1)\n",
    "test_loss_with_augment, test_acc_with_augment = model_with_augment.evaluate(testX, testY, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_without_augment = model_without_augment_fit.history\n",
    "history_with_augment = model_with_augment_fit.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Augmentation Effect\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('accuracy')\n",
    "plt.plot(history_without_augment['accuracy'])\n",
    "plt.plot(history_with_augment['accuracy'])\n",
    "plt.legend(['wihtout_augment', 'with_augmet'])\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Augmentation Effect\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('val_accuracy')\n",
    "plt.plot(history_without_augment['val_accuracy'])\n",
    "plt.plot(history_with_augment['val_accuracy'])\n",
    "plt.legend(['wihtout_augment', 'with_augmet'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
